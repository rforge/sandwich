\documentclass[nojss]{jss}
\usepackage{thumbpdf,lmodern}
\usepackage{amsmath,amssymb,bm}
\usepackage{multicol}
%% need no \usepackage{Sweave}

\newcommand{\newoperator}[3]{\newcommand*{#1}{\mathop{#2}#3}}
\newcommand{\renewoperator}[3]{\renewcommand*{#1}{\mathop{#2}#3}}
\newcommand{\vI}{\bm I}
\newcommand{\vH}{\bm H}

\author{Susanne Berger\\University of Innsbruck \And 
        Nathaniel Graham\\Trinity University Texas \And
        Achim Zeileis\\University of Innsbruck}
\title{Various Versatile Variances: An Object-Oriented Implementation of Clustered Covariances in \proglang{R}}

\Plainauthor{Susanne Berger, Nathaniel Graham, Achim Zeileis}
\Plaintitle{Various Versatile Variances: An Object-Oriented Implementation of Clustered Covariances in R}
\Shorttitle{Various Versatile Variances}

\Keywords{clustered data, clustered covariance matrix estimators, object-orientation, simulation, \proglang{R}}
\Plainkeywords{clustered data, clustered covariance matrix estimators, object-orientation, simulation, R}

\Abstract{
  Clustered covariances or clustered standard errors are very widely used to
  account for correlated or clustered data, especially in economics, political
  sciences, or other social sciences. They are employed to adjust the inference
  following estimation of a standard least-squares regression or generalized
  linear model estimated by maximum likelihood. Although many publications just
  refer to ``the'' clustered standard errors, there is a surprisingly wide
  variety of clustered covariances, particularly due to different flavors of
  bias corrections. Furthermore, while the linear regression model is certainly
  the most important application case, the same strategies can be employed in
  more general models (e.g. for zero-inflated, censored, or limited responses).

  In \proglang{R}, functions for covariances in clustered or panel models have been
  somewhat scattered or available only for certain modeling functions, notably
  the (generalized) linear regression model. In contrast, an object-oriented
  approach to ``robust'' covariance matrix estimation -- applicable beyond
  \code{lm()} and \code{glm()} -- is available in the \pkg{sandwich} package but
  has been limited to the case of cross-section or time series data. However,
  this shortcoming has been corrected in \pkg{sandwich} (starting from
  version~2.4.0): Based on methods for two generic functions (\code{estfun()} and
  \code{bread()}), clustered and panel covariances are now provided in
  \code{vcovCL()} and \code{vcovPL()}, respectively. These are directly
  applicable to models from packages \pkg{MASS}, \pkg{pscl}, \pkg{countreg},
  \pkg{betareg}, among many others. Some empirical illustrations are provided
  as well as an assessment of the methods' performance in a simulation study.
}

\Address{
  Susanne Berger, Achim Zeileis\\
  Department of Statistics\\
  Faculty of Economics and Statistics\\
  Universit\"at Innsbruck\\
  Universit\"atsstr.~15\\
  6020 Innsbruck, Austria\\
  E-mail: \email{Susanne.Berger@uibk.ac.at}, \email{Achim.Zeileis@R-project.org}\\
  URL: \url{https://www.uibk.ac.at/statistics/personal/berger/}, \url{https://eeecon.uibk.ac.at/~zeileis/}\\
  
  Nathaniel Graham\\
  Department of Finance and Decision Sciences\\
  Trinity University Texas\\
  One Trinity Place\\
  San Antonio, Texas 78212, United States of America\\
  E-mail: \email{npgraham1@npgraham1.com}\\
  URL: \url{https://sites.google.com/site/npgraham1/}
}

\begin{document}

\SweaveOpts{engine=R,eps=FALSE}
%\VignetteIndexEntry{Various Versatile Variances: An Object-Oriented Implementation of Clustered Covariances in R}
%\VignetteDepends{sandwich,pscl,lattice,geepack,multiwayvcov,plm,lme4,countreg}
%\VignetteKeywords{clustered data, clustered covariance matrix estimators, object-orientation, simulation, R}
%\VignettePackage{sandwich}

<<preliminaries,echo=FALSE,results=hide>>=
library("sandwich")
library("pscl")
library("lattice")
library("countreg")
library("lme4")
source("sim-CL.R")
panel.xyref <- function(x, y, ...) {
  panel.abline(h = 0.95, col = 2)
  panel.xyplot(x, y, ...)  
}
options(prompt = "R> ", continue = "+   ")
@ 

% simulations

<<sim-01,echo=FALSE,results=hide>>=
if(file.exists("sim-01.rda")) load("sim-01.rda") else {
set.seed(1)
s01 <- sim(nrep = 10000, nid = 100, nround = 5,
           dist = "gaussian", rho = seq(0, 0.9, by = 0.1), xrho = 0.25,
           coef = c(0, 0.85, 0.5, 0.7), formula = response ~ x1 + x2 + x3,
           vcov = c("without", "HC0", "HC0-id", "random", "gee", "bk", "dk"),
           type = "copula")
save(s01, file = "sim-01.rda")
}
@

<<sim-02,echo=FALSE,results=hide>>=
if(file.exists("sim-02.rda")) load("sim-02.rda") else {
set.seed(2)
s02 <- sim(nrep = 10000, nid = 100, nround = 5,
           dist = c("gaussian", "logit", "poisson"), rho = seq(0, 0.9, by = 0.1), xrho = 0.25,
           coef = c(0, 0.85, 0, 0), formula = response ~ x1,
           vcov = c("without", "HC0", "HC0-id", "random", "gee", "bk", "dk"),
           type = "copula")
save(s02, file = "sim-02.rda")
}
@

<<sim-03,echo=FALSE,results=hide>>=
if(file.exists("sim-03.rda")) load("sim-03.rda") else {
set.seed(3)
s03 <- sim(nrep = 10000, nid = 100, nround = 5,
           dist = c("zerotrunc", "zip", "beta"), rho = seq(0, 0.9, by = 0.1), xrho = 0.25,
           coef = c(0, 0.85, 0, 0), formula = response ~ x1,
           vcov = c("without", "HC0", "HC0-id"),
           type = "copula")
save(s03, file = "sim-03.rda")
}
@

<<sim-04,echo=FALSE,results=hide>>=
if(file.exists("sim-04.rda")) load("sim-04.rda") else {
set.seed(4)
s04 <- sim(nrep = 10000, nid = c(10, seq(50, 250, by = 50)), nround = 5,
           dist = c("gaussian","poisson", "logit"), rho = 0.25, xrho = 0.25,
           coef = c(0, 0.85, 0, 0), formula = response ~ x1,
           vcov = c("HC0-id","HC1-id","HC2-id","HC3-id"),
           type = "copula")
save(s04, file = "sim-04.rda")
}
@

<<sim-05,echo=FALSE,results=hide>>=
if(file.exists("sim-05.rda")) load("sim-05.rda") else {
set.seed(5)
s05 <- sim(nrep = 10000, nid = 100, nround = 5,
           dist = c("gaussian", "logit", "poisson"), rho = seq(0, 0.9, by = 0.1), xrho = 0.25,
           coef = c(0, 0.85, 0, 0), formula = response ~ x1,
           vcov = c("without", "HC0", "HC0-id"),
           type = "ranef")
save(s05, file = "sim-05.rda")
}
@

\section{Introduction} \label{sec:intro}

Observations with correlations between objects of the same group/cluster are often referred to as ``cluster-correlated'' observations. Each cluster comprises multiple objects that are correlated within, but not acress, clusters, leading to a nested or hierarchical structure \citep{hac:Galbraith:Daniel:Vissel:2010}.
Ignoring this dependency and pretending observations are independent not only
across but also within the clusters, still leads to parameter estimates that
are consistent (albeit not efficient) in many situations. However, the
observations' information will typically be overestimated and hence lead to
overstated precision of the parameter estimates and inflated type~I errors
in the corresponding tests \citep{hac:Moulton:1986, hac:Moulton:1990}.
Therefore, clustered covariances are widely used to account for clustered
correlations in the data.

Such clustering effects can emerge both in cross-section and in panel (or
longitudinal) data. Typical examples for clusting cross-section data include
firms within the same industry or students within the same school or call.
In panel data, a common source of clustering is that observations for the
same individual at different time points are correlated while the individuals
may be independent \citep{hac:Cameron+Miller:2015}.

This paper contributes to the literature particularly in two respects:
%
Most importantly, we discuss a set of computational tools for the \proglang{R} system for
statistical computing \citep{hac:R:2017}, providing an object-oriented
implementation of clustered covariances/standard errors in the \proglang{R}
package \pkg{sandwich} \citep{hac:Zeileis:2004a,hac:Zeileis:2006}. Using this
infrastructure, sandwich covariances for cross-section or time series data have
been available for models beyond \code{lm()} or \code{glm()}, e.g., for packages
\pkg{MASS} \citep{hac:Venables+Ripley:2002}, \pkg{pscl}/\pkg{countreg}
\citep{hac:Zeileis+Kleiber+Jackman:2008}, \pkg{betareg}
\citep{hac:Cribari-Neto+Zeileis:2010,hac:Gruen+Kosmidis+Zeileis:2012}, among
many others. However, corresponding functions for clustered or panel data had
not been available in \pkg{sandwich} but have been somewhat scattered or
available only for certain modeling functions.

Moreover, we perform a Monte Carlo simulation study for various response
distributions with the aim to assess the performance of clustered standard errors
beyond \code{lm()} and \code{glm()}. This also includes special cases for which
such a finite-sample assessment has not yet been carried out in the literature
(to the best of our knowledge).

The rest of this manuscript is structured as follows: Section~\ref{sec:overview}
discusses the idea of clustered covariances and reviews existing \proglang{R}
packages for sandwich as well as clustered covariances.
Section~\ref{sec:methods} deals with the theory behind sandwich covariances,
especially with respect to clustered covariances for cross-sectional and
longitudinal data, clustered data, as well as panel data.
Section~\ref{sec:software} then takes a look behind the scenes of the new
object-oriented \proglang{R} implementation for clustered covariances,
Section~\ref{sec:illu} gives an empirical illustration based on data provided
from \cite{hac:Petersen:2009} and \cite{hac:Aghion+VanReenen+Zingales:2013}.
The simulation setup and results are discussed in Section~\ref{sec:simulation}.

\section{Overview} \label{sec:overview}

There is a range of popular strategies for dealing with clustered dependencies
in regression models. In the statistics literature, random effects (especially
random intercepts) are often introduced to capture unobserved cluster
correlations (e.g., using the \pkg{lme4} package in \proglang{R},
\citealp{hac:Bates+Machler+Bolker:2015}). Alternatively, generalized estimating
equations (GEE) can account for such correlations by adjusting the model's
scores in the estimation, also leading naturally to a clustered covariance
(e.g., available in the \pkg{geepack} package for \proglang{R},
\citealp{hac:Halekoh+Hojsgaard+Yan:2002}). Another approach, widely used
in econometrics and the social sciences, is to assume that the model's score
function was correctly specified but that only the remaining likelihood was
potentially misspecified, e.g., due to a lack of independence as in the case
of clustered correlations (see \citealp{hac:White:1994}, for a classic textbook,
and \citealp{hac:Freedman:2006}, for a criticial review). This approach leaves
the parameter estimator unchanged -- then also known as quasi-maximum likelihood
(QML) estimator or, in GEE jargon, as independence working model -- but adjust
the covariance matrix by using a sandwich estimator, especially in Wald tests
and corresponding confidence intervals.

Important special cases of this QML approach combined with sandwich covariances
include: (1) independent but heteroscedastic observations necessitating
heteroscedasticity-consistent (HC) covariances
\citep[see e.g.,][]{hac:Long+Ervin:2000}, (2) autocorrelated time series of
observations requiring heteroscedasticity- and autocorrelation-consistent
(HAC) covariances \citep[such as][]{hac:Newey+West:1987,hac:Andrews:1991},
(3) and clustered sandwich covariances for clustered or panel data
\citep[see e.g.,][]{hac:Cameron+Miller:2015}.

Various kinds of sandwich covariances have already been implemented in several
\proglang{R} packages, with the linear regression case receiving most attention.
But some packages also cover more general models.


\subsection[R packages for sandwich covariances]{\proglang{R} packages for sandwich covariances}

The standard \proglang{R} package to model sandwich covariance estimators is the
\pkg{sandwich} package \citep{hac:Zeileis:2004a,hac:Zeileis:2006}, which
provides an object-oriented implementation for the building blocks of the
sandwich that rely only on a small set of extractor functions (\code{estfun()}
and \code{bread()}) for fitted model objects. The function \code{sandwich()}
computes a plain sandwich estimate \citep{hac:Eicker:1963,hac:Huber:1967,hac:White:1980}
from a fitted model object, defaulting to what is known as HC0 or HC1 in linear
regresion models. \code{vcovHC()} is a wrapper to \code{sandwich()} combined
with \code{meatHC()} and \code{bread()} to compute general HC covariances
ranging from HC0 to HC5. \code{vcovHAC()}, based on \code{sandwich()} with
\code{meatHAC()} and \code{bread()}, computes HAC covariance matrix estimates.
Further convenience interfaces \code{kernHAC()} for Andrews' kernel HAC
\citep{hac:Andrews:1991} and \code{NeweyWest()} for Newey-West-style HAC
\citep{hac:Newey+West:1987,hac:Newey+West:1994} are available. However,
in versions prior to 2.4.0 of \pkg{sandwich} no similarly object-oriented
approach to clustered sandwich covariances was available.

Another \proglang{R} package that includes heteroscedasticity consistent
covariance estimators (HC0--HC4), for models produced by \code{lm()} only,
is the \pkg{car} package \citep{hac:Fox+Weisberg:2011} in function
\code{hccm()}. Like \code{vcovHC()} from \pkg{sandwich} this is limited
to the cross-section case without clustering, though.

\subsection[R packages for clustered covariances]{\proglang{R} packages for clustered covariances}

The lack of support for clustered sandwich covariances in standard packages
like \pkg{sandwich} or \pkg{car} has led to a number of different
implementations scattered over various packages. Typically, these are tied
to either objects from \code{lm()} or dedicated model objects fitting certain
(generalized) linear models for clustered or panel data. The list of packages
includes: \pkg{multiwayvcov} \citep{hac:Graham+Arai+Hagstroemer:2016},
\pkg{plm} \citep{hac:Croissant+Millo:2008}, \pkg{geepack}
\citep{hac:Halekoh+Hojsgaard+Yan:2002}, \pkg{lfe} \citep{hac:Gaure:2016},
\pkg{clubSandwich} \citep{hac:Pustejovsky:2016}, and \pkg{clusterSEs}
\citep{hac:Esarey:2017}, among others.

In \pkg{multiwayvcov}, the implementation was object-oriented in many aspects
building on \pkg{sandwich} infrastructure. However, certain details assumed
\code{lm} or \code{glm}-like objects. In \pkg{plm} and \pkg{lfe} several
types of sandwich covariances are available for the packages' \code{plm}
(panel linear models) and \code{felm} (fixed-effect linear models), respectively.
The \pkg{geepack} package can estimate independence working models for
\code{glm}-type models, also supporting clustered covariances for the
resulting \code{geeglm} objects. Finally, \pkg{clusterSEs} and
\pkg{clubSandwich} focus on the case of ordinary or weighted least squares
regression models.

In a nutshell, there is good coverage of clustered covariances for (generalized)
linear regression objects albeit potentially necessitating reestimating a
certain model using a different model-fitting function/packages. However, there
was no object-oriented implementation for clustered covariances in \proglang{R},
that enabled plugging in different model objects from in principle any class.
Therefore, starting from the implementation in \pkg{multiwayvcov} a new and
object-oriented implementation was established and integrated in \pkg{sandwich},
allowing application to more general models, including zero-inflated, censored,
or limited responses. 

\section{Methods} \label{sec:methods}

In the next section, some theoretical background of the underlying methods for
clustered as well as panel data is introduced. As a starting point, the
structure of sandwich covariances is established, because all types of
covariances considered here are based on sandwich covariances.

Let $(y_{i},x_{i})$ for $i = 1, \ldots, n$ be data with some distribution controlled by a parameter vector $\theta$ with k dimensions.
To compute sandwich covariances, take a bread matrix $B(\theta)$ and a meat matrix $M(\theta)$ and multiply them to a sandwich with meat between two slices of bread \citep{hac:Zeileis:2006}
\begin{eqnarray} \label{eq:sandwich}
  S(\theta) & = & B(\theta) \cdot M(\theta) \cdot B(\theta) \\  \label{eq:bread}
  B(\theta) & = & \left( \E[ - \psi'(y, x, \theta) ] \right)^{-1} \\  \label{obj}
  M(\theta) & = & \VAR[ \psi(y, x, \theta) ].
\end{eqnarray}
The building blocks for the calculation of the sandwich are provided by the \pkg{sandwich} package.
A natural idea for an object-oriented implementation of such estimators is to provide for various functions that compute different estimators for the meat $M(\theta)$ based on an \code{estfun()} extractor function that extracts the empirical estimating functions $\psi(y, x, \theta)$ from a fitted model object.
An estimating function
\begin{eqnarray}
  \psi(y, x, \theta) \quad = \quad \frac{\partial \Psi(y, x, \theta)}{\partial \theta},
\end{eqnarray}
is defined as the derivative of an objective function $\Psi(y, x, \theta)$, typically the log-likelihood, with respect to a parameter vector $\theta$.
Thus, an empirical estimating (or score) function evaluates an estimating function at the observed data and the estimated parameters \citep{hac:Zeileis:2006}.
\code{estfun()} returns an $n \times k$ matrix of empirical estimating functions from a fitted model object \citep{hac:Zeileis:2006}:
 \[ \left( \begin{array}{c} \psi(y_1, x_1, \hat \theta) \\ \vdots \\ \psi(y_n, x_n, \hat \theta)
           \end{array} \right). \]

\code{bread()} extracts the empirical version of the inverse Hessian
\begin{equation} \label{eq:Bhat}
  \hat B \quad = \quad \left( \frac{1}{n} \sum_{i = 1}^n - \psi'(y_i, x_i, \hat \theta) \right)^{-1}.
\end{equation}
\code{meat()}, \code{meatHAC()} and \code{meatHC()} compute outer product, HAC and HC estimators for the meat, respectively, given the existence of an \code{estfun()} method:
\begin{eqnarray} \label{eq:meat-op}
  \hat M & = & \frac{1}{n} \sum_{i = 1}^n\psi(y_i, x_i, \hat \theta) \psi(y_i, x_i, \hat \theta)^\top \\ \label{eq:meat-hac}
  \hat M_\mathrm{HAC} & = & \frac{1}{n} \sum_{i, j = 1}^n w_{|i-j|} \, \psi(y_i, x_i, \hat \theta) \psi(y_j, x_j, \hat \theta)^\top \\ \label{eq:meat-hc}
  \hat M_\mathrm{HC} & = & \frac{1}{n} X^\top \left( \begin{array}{ccc} 
  \omega(r(y_1, x_1^\top \theta)) & \cdots & 0 \\ 
  \vdots & \ddots & \vdots \\
  0 & \cdots & \omega(r(y_n, x_n^\top \theta))
  \end{array} \right) X.
\end{eqnarray}
The outer product estimator in Equation~\ref{eq:meat-op} corresponds to the White estimator \citep{hac:Eicker:1963,hac:Huber:1967,hac:White:1980}.
$w_{|i-j|}$ in Equation~\ref{eq:meat-hac} is a vector of weights \citep{hac:Zeileis:2004a}.
In Equation~\ref{eq:meat-hc}, functions $\omega(\cdot)$ derive estimates of the variance of the observed working residuals $r(y_1, x_1^\top \theta), \ldots, r(y_n, x_n^\top \theta)$ \citep{hac:Zeileis:2006}.

\subsection{Clustered covariances} 

A simple and natural estimator for the meat matrix is the outer product of the empirical estimating functions $\hat M$.
For (one-way) clustered covariances, the level of aggregation changes from individuals to clusters/groups, which leads to changes in the meat matrix. 
The empirical estimating functions are first summed groupwise (e.g., for $g = 1, \ldots, G$ cluster, with $n_{g}$ observations per cluster), returning a $G \times k$ matrix of empirical estimating functions
 \[ \left( \begin{array}{c} \psi(y_1, x_1, \hat \theta) \\ \vdots \\ \psi(y_G, x_G, \hat \theta)
           \end{array} \right), \]
before the outer product is calculated
\begin{equation} \label{eq:meatCL}
  \hat M_\mathrm{CL} \quad = \quad \frac{1}{n} \sum_{g = 1}^G\sum_{i = 1}^{n_{g}}\psi(y_{ig}, x_{ig}, \hat \theta) \psi(y_{ig}, x_{ig}, \hat \theta)^\top.
\end{equation}
The clustered covariance estimator generalizes White's famous covariance estimator \citep{hac:White:1980}.
If it is the case that each observation is its own cluster, clustered covariances correspond to the White estimator.

\subsubsection{Types of bias correction}
The clustered covariance estimator controls for both heteroscedasticity across as well as within clusters, but this comes at the cost that the number of clusters approaches infinity, not the number of observations \citep{hac:Cameron+Gelbach+Miller:2008,hac:Cameron+Miller:2015}.
Although many publications just refer to ``the'' clustered standard errors, there is a surprisingly wide variation in clustered covariances, particularly due to different flavors of bias corrections.
The bias correction factor can be split in two parts, a ``cluster bias correction'' and an ``HC bias correction''.
The cluster bias correction is defined as 
\begin{equation} \label{eq:biasadj0}
 \frac{G}{G - 1},
\end{equation} 
where $G$ is again the number of clusters.
The HC bias correction can additionally to the cluster bias correction be applied to the clustered covariance estimator. This is an easy task for HC0 and HC1 types of bias correction, but more demanding for cluster generalizations of HC2 and HC3, which have been developed by \cite{hac:Kauermann+Carroll:2001} and \cite{hac:Bell+McCaffrey:2002}.
HC0 to HC3 bias corrections are defined as
\begin{eqnarray} \label{eq:biasadj1}
  \mathrm{HC0:} & = & 1 \\  \label{eq:biasadj2}
  \mathrm{HC1:} & = & \frac{n}{n - k} \\  \label{eq:biasadj3}
  \mathrm{HC2:} & = & [\vI_{n_{g}} - \vH_{gg}]^{-0.5} \\  \label{eq:biasadj4}
  \mathrm{HC3:} & = & [\vI_{n_{g}} - \vH_{gg}]^{-1}, 
\end{eqnarray}  
where $n$ is the number of observations and $k$ is the number of estimated parameters.
$\vI_{n_{g}}$ is an identity matrix of size $n_{g}$, and $\vH_{gg}$ is a block-diagonal hat matrix.

\subsubsection{Two- and multi-way clustered covariances}
Certainly, there can be more than one cluster dimension, as for example observations that are characterized by housholds within states or companies within industries. It can therefore sometimes be helpful that one-way clustered covariances can be extended to so-called multi-way clustering as shown by \cite{hac:Miglioretti+Heagerty:2007}, \cite{hac:Thompson:2011} and \cite{hac:Cameron+Gelbach+Miller:2011}.

Multi-way clustered covariances comprise clustering on $2^{D} - 1$ dimensional combinations.
Clustering in two dimensions, for example in $id$ and $time$, gives $D = 2$, such that the clustered covariance matrix is composed of $2^2 - 1 = 3$ one-way clustered covariance matrices that have to be added up or substracted off, respectively.
For two-way clustered covariances with cluster dimensions $id$ and $time$, the one-way clustered covariance matrices on $id$ and on $time$ are added up, and the two-way clustered covariance matrix with clusters formed by the intersection of $id$ and $time$ is substacted off
\begin{equation} \label{eq:twoway}
S(\theta)_{id,time} \quad = \quad S(\theta)_{id} + S(\theta)_{time} - S(\theta)_{id \cap time}.
\end{equation}
Clustered covariance matrices like $S(\theta)_{id}$ and $S(\theta)_{time}$ with an odd number of cluster dimensions are added up, whereas those with an even number of cluster dimensions like $S(\theta)_{id \cap time}$ are substracted off to get the two-way clustered covariance estimator. The same heuristic applies to clustered covariances with more than two dimensions of clusters.

\cite{hac:Petersen:2009}, \cite{hac:Thompson:2011} and \cite{hac:Ma:2014} suggest to substract the White covariance matrix as the last substracted matrix in case that the clusters formed by the intersection of $id$ and $time$ do only contain a single observation\footnote{If one wants to cluster by dimensions $id$ and $year$ in a sample of $id-month$ observations, each cluster is composed of 12 observations \citep{hac:Ma:2014}. In such a setting, the last substracted matrix should be a one-way clustered covariance matrix with clusters formed by the intersections of $id$ and $year$.}. 

As \cite{hac:Ma:2014} argues, the White estimator does not take into account any bias adjustment, such that the cluster bias correction $\frac{G}{G-1}$ is larger. Thus, if always the clustered covariance matrix is substracted as the last substracted matrix instead of White's covariance, clustered covariances are underestimated and can thus lead to erroneous conclusions.

\subsection{Clustered covariances for panel data}
The information of panel data sets is often overstated, as cross-sectional as well as temporal dependencies may occur \citep{hac:Hoechle:2007}. \cite[][p.~702]{hac:Cameron+Trivedi:2005} noticed that ``NT correlated observations have less information than NT independent observations''. 
For panel data, the source of dependence in the data is crucial to find out what kind of covariance is optimal \citep{hac:Petersen:2009}.
In the following, panel Newey-West standard errors as well as Driscoll and Kraay standard errors are examined \citep[see also][]{hac:Millo:2014}.

\subsubsection{Panel Newey-West}
\cite{hac:Newey+West:1987} proposed a heteroscedasticity and autocorrelation consistent standard error estimator that is traditionally used for time-series data, but can be modified for use in panel data by estimating correlations between lagged residuals in the same cluster \citep{hac:Petersen:2009}.

A panel Newey-West estimator can be obtained by setting the cross-sectional as well as the cross-serial correlation to zero \citep{hac:Millo:2014}. The meat is composed of
\begin{equation} \label{eq:newey}
  \hat M_\mathrm{PL}^ {NW} \quad = \quad \hat M + \sum_{l = 1}^L w_l[\hat \Omega_l + \hat \Omega_l^\top],
\end{equation}  
with
\begin{equation}
  \hat \Omega_l = \sum_{t = 1}^T\sum_{i = 1}^n \psi(y_{i,t}, x_{i,t}, \hat \theta) \psi(y_{i,t-l}, x_{i,t-l}, \hat \theta)^\top,
\end{equation}
where $w_l$ is a distance-decreasing kernel function, and $L$ the maximal lag length. As \cite{hac:Petersen:2009} noticed, the maximal lag length in a panel data set is the maximum number of years per firm minus one.

Though, an underlying assumption is that although residuals are correlated within groups, they are uncorrelated between groups, which is often artificial and does often not reflect the nature of panel data \citep{hac:Hoechle:2007}. 
To impose fewer restrictive assumptions, correlation within and between groups should be possible.

\subsubsection{Driscoll and Kraay}
\cite{hac:Driscoll+Kraay:1998} have adapted the Newey-West approach and introduce a technique yielding standard error estimates that are robust to spatial and temporal dependence of general form, but with the caveat that a long enough time dimension exists.

Driscoll and Kraay standard errors are a generalization of Newey-West standard errors, applied to the time series of cross-section averages of $\psi(y_{i,t}, x_{i,t}, \hat \theta)$ \citep{hac:Hoechle:2007}. Thus, independent of the cross-section dimension, Driscoll and Kraay standard errors are consistent.

They are obtained as the positive square root of the diagonal elements of the sandwich $S(\theta)$ with a meat
\begin{equation} \label{eq:driscoll}
  \hat M_\mathrm{PL}^ {DC} \quad = \quad \hat M_\mathrm{CL, t} + \sum_{l = 1}^{L} w_l[\hat \Omega_l + \hat \Omega_l^\top],
\end{equation}  
where $i$ denotes the cross section and $t$ the time units with 
\begin{equation}
  \hat \Omega_l = \sum_{t = 1}^T \psi(y_{t}, x_{t}, \hat \theta) \psi(y_{t-l}, x_{t-l}, \hat \theta)^\top \quad \mathrm{and} \quad \psi(y_{t}, x_{t}, \hat \theta) = \sum_{i = 1}^{n_{t}}\psi(y_{i,t}, x_{i,t}, \hat \theta)
\end{equation}
\citep{hac:Hoechle:2007}. $w_l = 1 - \frac{l}{L + 1}$ are the modified Barttlett weights, and $L$ is the maximal lag length used \citep{hac:Hoechle:2007}.  For lag $l = 0$, $\hat \Omega_0 = \hat M_{CL, t}$, where $\hat M_{CL, t}$ is the meat of one-way clustered covariances clustered by time. 

\subsection{Panel-corrected standard errors}

For time-series-cross-section (hereinafter abbreviated to TSCS) data, panel-corrected standard errors (hereinafter abbreviated to PCSE) have been introduced by \cite{hac:Beck+Katz:1995} as an attempt to weaken the problems of the Parks-Kmenta method\footnote{For details, see \cite{hac:Hoechle:2007}.}. 
According to \cite{hac:Beck+Katz:1995}, TSCS data are characterized by having repeated observations on fixed units (for example states), where the number of units normally is between 10 and 100, and the number of time periods ranges between 20 and 50 years. 

Thus, TSCS data do often allow not only for heterocedasticity, but for spatial and temporal correlation as well.
Furthermore, \cite{hac:Johnson:2004} points out that a crucial assumption of PCSE is that the contemporaneous correlation accross cluster follows a fixed pattern.
The authors suggest to keep the OLS parameter estimates, but use PCSE instead of ordinary OLS standard errors.

To calculate PCSE \citep{hac:Johnson:2004}, the empirical working residuals are separated by cluster dimension $g$, such that they are of form $r(y_1, x_1^\top \hat \theta), r(y_2, x_2^\top \hat \theta), \ldots, r(y_G, x_G^\top \hat \theta)$.

The working residuals can be recovered from the empirical estimating function divided by $x_i$, as $\psi(y_i, x_i, \hat \theta) = r(y_i, x_i^\top \hat \theta) \cdot x_i$ \citep{hac:Zeileis:2006}.
 Each empirical working residual is a vector with $t$ elements. Grouped together into a $t \times G$ matrix,
 \begin{equation} \label{eq:workres}
  E \quad = \quad [r(y_1, x_1^\top \hat \theta) \quad r(y_2, x_2^\top \hat \theta) \quad \ldots \quad r(y_G, x_G^\top \hat \theta)],
 \end{equation}  
the meat of the panel-corrected covariance matrix is defined as \citep{hac:Beck+Katz:1995}
\begin{equation} \label{eq:pcse}
 \hat M_\mathrm{PC} \quad = \quad \frac{1}{n} X^\top \bigg[ \frac{(E^\top E)}{t} \otimes \vI_t \bigg] X.
\end{equation}
Compared to White's estimator, where squared residuals are along the diagonal of the estimator, the covariance across clusters is estimated by the cross product of their residuals \citep{hac:Johnson:2004}. While Whites approach does not account for clustering and handles each observations separately, panel-corrected covariances utilize the clustering structure.

With regard to the finite-sample propertiers of the PCSE estimator, \cite{hac:Hoechle:2007} argues that they are rather poor if the cross-sectional dimension $g$ is large compared to the time dimension $t$. The PCSE estimator will be quite imprecise for a small $t/g$ ratio, as the whole $g \times g$ cross-sectional covariance matrix is estimated.

In the next section, the software implementation of the various types of clustered covariances is introduced.

\section{Software} \label{sec:software}

\subsection{Clustered covariances}

One-, two-, and multi-way clustered standard errors with the different types of bias correction introduced in Equation~\ref{eq:biasadj1}--Equation~\ref{eq:biasadj4} are implemented with the following arguments in the function
\begin{verbatim}
vcovCL(x, cluster = NULL, type = NULL, sandwich = TRUE, fix = FALSE, ...)
\end{verbatim}
as a part of the \pkg{sandwich} package.
As compared to classical sandwich estimators, the meat for clustered covariances changes and can be separately calculated by the function
\begin{verbatim}
meatCL(x, cluster = NULL, type = NULL, cadjust = TRUE, white = FALSE, ...).
\end{verbatim}

The first argument \code{x} is a fitted model object of class \code{lm}, \code{glm}, \code{zerotrunc}, \code{betareg} or \code{hurdle}, among others.

The \code{cluster} argument allows to supply one or more cluster variables. If no cluster variable is supplied, each observation is its own cluster per default. In such a case, the clustered covariance estimator collapses to Whites robust covariance estimator.

The bias correction is composed of two parts, the cluster bias correction defined in Equation~\ref{eq:biasadj0} and the HC bias correction defined in Equation~\ref{eq:biasadj1}--Equation~\ref{eq:biasadj4}. Both types of bias correction can be separately switched on and off.
With the \code{cadjust} argument, the cluster bias adjustment can be turned on and off, the \code{type} argument allows to specify the HC type of bias correction.

HC2 and HC3 types of bias correction are geared towards the linear model, but are also applicable for \code{glm} \citep{hac:Bell+McCaffrey:2002, hac:Kauermann+Carroll:2001}. Nevertheless, a precondition for the HC2 and HC3 corrections is the existence of a hat matrix or a weighted version of the hat matrix for GLMs, respectively.
Hence, this is the only part of \code{vcovCL()} that is not fully object-oriented, but just works for \code{lm} and \code{glm} model objects.
However, beyond the \code{glm} case it's often not clear what the hat matrix should be.

The \code{white} argument is executed only for two- and multi-way clustered covariances and specifies whether to substract White's covariance matrix as the last substracted matrix in Equation~\ref{eq:twoway} or the covariance matrix formed by the intersection of groups \citep{hac:Petersen:2009,hac:Thompson:2011,hac:Ma:2014}.

The \code{sandwich} argument returns either the full sandwich estimator or only the meat \citep[for details, see][]{hac:Zeileis:2004a}.

The \code{fix} argument specifies whether any negative eigenvalues from the eigendecomposition of the estimated covariance matrix should be converted to zero.
In some applications with fixed effects, covariance matrix of the estimator may not be positive-definite, which is the why this argument is sometimes required. Additionally, \cite{hac:Cameron+Gelbach+Miller:2011} observe that this is most likely to arise when clustering is done over the same groups as the fixed effects.

\subsection{Clustered covariances for panel data}

For panel data, 
\begin{verbatim}
vcovPL(x, cluster = NULL, order.by = NULL, kernel = "Bartlett",
 sandwich = TRUE, fix = FALSE, ...)
\end{verbatim}
as well as 
\begin{verbatim}
meatPL(x, cluster = NULL, order.by = NULL, kernel = "Bartlett",
 lag = "max", bw = NULL, adjust = TRUE, ...)
\end{verbatim}
allow for estimation of Driscoll and Kraay type covariances \citep{hac:Driscoll+Kraay:1998} as well as Newey-West type covariances \citep{hac:Newey+West:1987}. Again, \code{vcovPL()} returns the full sandwich if the argument \code{sandwich = TRUE}, and \code{fix = TRUE} converts any negative eigenvalues to zero.

The \code{cluster} argument allows to specify a variable indicating the cluster dimension, and resorts to cross-section if no cluster is supplied. A cluster can either be supplied explicitly or be an attribute of the model, or can be a list with both indexes \code{cluster} and \code{order.by}.
The argument \code{order.by} allows to specify a variable indicating the aggregation within time periods.

The \code{kernel} argument is a character specifying the kernel used. All kernels described in \cite{hac:Andrews:1991} are supported. See \cite{hac:Zeileis:2004} for details.

The \code{lag} argument indicates the lag length used, and can be either numeric or a character (one of ``\code{max}'', ``\code{NW1987}'' or ``\code{NW1994}''). ``\code{max}'' (or equivalently, ``\code{P2009}'' for \cite{hac:Petersen:2009}) indicates the maximum lag length $T - 1$, that is the number of time periods minus one.
``\code{NW1987}'' corresponds to \cite{hac:Newey+West:1987}, who have shown that their estimator is consistent if the number of lags increases with time periods $T$, but with speed less that $T^{1/4}$ \citep[see also][]{hac:Hoechle:2007}.
``\code{NW1994}'' sets the lag length to $\mathrm{floor}[4 \cdot (\frac{T}{100})^{2/9}]$ \citep{hac:Newey+West:1994}.
The function argument \code{bw} specifies the bandwidth of the kernel, which by default corresponds to $\textrm{lag} + 1$.

The \code{adjust} argument allows to make a finite sample adjustment, which amounts to multiplication with $n/(n - k)$, where $n$ is the number of observations, and $k$ is the number of estimated parameters.

%\cite{hac:Driscoll+Kraay:1998} apply a Newey-West type correction to the sequence of cross-sectional averages of the moment conditions %\citep{hac:Hoechle:2007}.
%For large length of the time dimesion (and regardless of the length of the cross-sectional dimension), the \cite{hac:Driscoll+Kraay:1998} %standard errors are robust to general forms of cross-sectional and serial correlation \citep{hac:Hoechle:2007}.
%The \cite{hac:Newey+West:1987} covariance matrix restricts the \cite{hac:Driscoll+Kraay:1998} covariance matrix to non cross-sectional %correlation.

\subsection{Panel-corrected standard errors}

Panel-corrected standard errors (PCSE) a la \cite{hac:Beck+Katz:1995} are implemented in functions
\begin{verbatim}
vcovPC(x, cluster = NULL, order.by = NULL, subsample = TRUE,
 sandwich = TRUE, fix = FALSE, ...)
\end{verbatim}
and
\begin{verbatim}
meatPC(x,  cluster = NULL, order.by = NULL, subsample = TRUE,
 kronecker = TRUE, ...)
\end{verbatim}
and are usually used for time-series-cross-section (TSCS) data with a large enough time dimension. 
The arguments \code{sandwich} and \code{fix} do the same things as in \code{vcovCL()} and \code{vcovPL()}. The same holds true for \code{cluster} and \code{order.by}.

While an easy task for balanced panel, there are two alternatives to estimate the meat for unbalanced panel \citep{hac:Bailey+Katz:2011}. For \code{subsample = TRUE}, a balanced subset of the panel is employed, whereas for \code{subsample = FALSE}, a pairwise balaced sample is used. For details, see \cite{hac:Bailey+Katz:2011}.

The argument \code{kronecker} relates to estimation of the meat and determines whether calculations are executed with the Kronecker product or elementwise, which is computationally faster for a large number of observations.

\section{Illustrations} \label{sec:illu}
The classical area of application for clustered standard errors is the linear model, with model errors that are correlated within clusters. In this section, replication and comparison of clustered standard errors available in a couple of \proglang{R} packages is shown on the basis of benchmark \cite{hac:Petersen:2009} data in Example \ref{ex-petersen}. Due to the nice feature of object-orientation, clustered covariances in \code{vcovCL()} are now available for models beyond \code{lm()} or \code{glm()}. Example \ref{ex-aghion} applies \code{vcovCL()} to a hurdle model, using data provided by \cite{hac:Aghion+VanReenen+Zingales:2013}.

\subsection[Petersen (2009)]{\cite{hac:Petersen:2009}} \label{ex-petersen}
Benchmark data for testing the clustered standard error estimates in the linear model is a simulated data set\footnote{\url{http://www.kellogg.northwestern.edu/faculty/petersen/htm/papers/se/test_data.txt}} provided by \cite{hac:Petersen:2009}, containing 500 firms over 10 years. The data include 4 variables, \code{firmid}, \code{year}, dependent variable \code{y} and explanatory variable \code{x} \citep[see also][]{hac:Graham+Arai+Hagstroemer:2016}.

The data are available in \pkg{sandwich} as well as in \pkg{multiwayvcov}.
<<petersen-data>>= 
data("PetersenCL", package = "sandwich")
@ 
A linear model is fitted with \code{lm()}, 
<<petersen-model>>=
p_lm <- lm(y ~ x, data = PetersenCL)
@ 
and testing the estimated coefficients with \code{coeftest()} from package \pkg{lmtest} gives
<<petersen-coeftest>>=
library("lmtest")
coeftest(p_lm)
@ 
However, the data are clustered by firms (\code{firmid}), with a total of 500 clusters. But ignoring dependency by using ``classical'' standard errors results in an overstate of estimator precision.

\subsubsection{One-way clustered standard errors}
Thus, one-way clustered standard errors clustered by \code{firmid} are employed.
It can be observed that clustered standard errors are larger than ``classical'' standard errors.
<<petersen-onecl-firm>>=
library("sandwich")
coeftest(p_lm, vcov = vcovCL(p_lm, cluster = PetersenCL$firmid))
@ 
Next, a comparison of clustered standard errors resulting from packages \pkg{sandwich}, \pkg{multiwayvcov}, \pkg{plm} and \pkg{geepack} is shown. 
First, \code{vcovCL()} from \pkg{sandwich} and \code{cluster.vcov()} from \pkg{multiwayvcov} are compared. 
<<petersen-packages1>>=
library("multiwayvcov")
@ 
Both functions are closely related, as \code{vcovCL()} is partly based on \code{cluster.vcov()}.
<<petersen-comparison1>>=
vc <- list(
  "sandwich" = vcovCL(p_lm, cluster = PetersenCL$firmid),
  "multiwayvcov" = cluster.vcov(p_lm, cluster = PetersenCL$firmid)
)
sapply(vc, function(x) sqrt(diag(x)))
@ 
The bias correction in \code{vcovCL()} is set to \code{cadjust = TRUE} and \code{type = "HC1"} by default. The same type of bias correction is achieved for \code{cluster.vcov()} by default.

Second, \code{vcovCL()} is compared to packages \pkg{plm} and \pkg{geepack}, who are also able to calculate clustered covariances.
<<petersen-packages2>>=
library("plm")
library("geepack")
@ 
A pooling model is estimated with \code{plm()}, \code{geeglm()} fits a generalized estimating equation (GEE) with independence correlation structure.
<<petersen-models>>=
p_plm <- plm(y ~ x, data = PetersenCL, model = "pooling")
p_gee <- geeglm(y ~ x, data = PetersenCL, id = PetersenCL$firmid,
  corstr = "independence", family = gaussian)
@ 
As there is no \code{vcov()}-method specified for \code{geeglm()}, the convenience function \code{vcov.geeglm()} is provided to make this contribution.
<<petersen-convenience>>=
vcov.geeglm <- function(object) {
  vc <- object$geese$vbeta
  rownames(vc) <- colnames(vc) <- names(coef(object))
  return(vc)
}
@ 
Nevertheless, in order to accomplish exactly the same value for clustered standard errors using \code{plm} and \code{geepack}, the bias correction $\frac{G}{G - 1}\frac{n - 1}{n - k}$ has to be omitted from the clustered covariance matrix estimated by \code{vcovCL()}. This is achieved by setting the function arguments \code{cadjust = FALSE} and \code{type = "HC0"}.
<<petersen-comparison2>>=
vc <- list(
  "sandwich" = vcovCL(p_lm, cluster = PetersenCL$firmid,
                      cadjust = FALSE, type = "HC0"),
  "plm" = vcovHC(p_plm, type = "HC0", cluster = "group"),
  "geepack" = vcov(p_gee)
)
sapply(vc, function(x) sqrt(diag(x)))
@
All packages examined produce the same clustered standard errors for \code{lm} model objects. For packages \pkg{plm} and \pkg{geepack}, one has to omit the HC bias correction factor as well as the cluster bias correction. In general, differences in clustered standard errors often coincide with different types of bias corrections.

\subsubsection{Two-way clustered standard errors}
Two-way clustered standard errors with cluster dimensions \code{firmid} as well as \code{year} are, at least for the explanatory variable \code{x}, a bit larger than one-way clustered standard errors clustered by \code{firmid}.
<<petersen-twocl>>=
coeftest(p_lm, vcov. = vcovCL(p_lm, 
cluster = cbind(PetersenCL$firmid, PetersenCL$year)))
@ 
 However, as cluster dimension \code{year} has a total of only 10 cluster, the results should be regarded with caution, as it is required by theory that each cluster dimension has many clusters \citep{hac:Petersen:2009,hac:Cameron+Gelbach+Miller:2011,hac:Cameron+Miller:2015}.
 
\subsection[Aghion et al. (2013) and Berger et al. (2017)]{\cite{hac:Aghion+VanReenen+Zingales:2013} and \cite{hac:Berger+Stocker+Zeileis:2017}} \label{ex-aghion}
In this section, in a further example we will make use of the object-orientation of \code{vcovCL()}, and estimate clustered standard errors for a count data hurdle model.

\cite{hac:Aghion+VanReenen+Zingales:2013} investigate the effect of institutional owners (these are, for example, pension funds, insurance companies, etc.) on innovation.
The authors use firm-level panel data on innovation and institutional ownership from 1991 to 1999 over 803 firms, with the data clustered at company as well as industry level. 
To capture the differing value of patents, citation-weighted patent counts are used as a proxy for innovation, whereby the authors weight the patents by the number of future citations.
This motivates the use of count data models.

\cite{hac:Aghion+VanReenen+Zingales:2013} mostly employ Poisson and negative binomial models in a quasi-maximum likelihood approach and cluster standard errors by either companies or industries.
Still, one limitation of standard count data models is that the zeros and the nonzeros (positives) are assumed to come from the same data-generating process.

From an economic perspective, there is a difference in determinants of ``first innovation'' and ``continuing innovation''. The rationale behind this is the notion of nonlinearities in the innovation process.
In case that the first innovation is especially hard to obtain in comparison to succeeding innovations, hurdle models offer a useful way that allows for a distinction to be made between these two processes \citep{hac:Berger+Stocker+Zeileis:2017}.
Therefore, \cite{hac:Berger+Stocker+Zeileis:2017} employ two-part hurdle models with a binary part that models the decision to innovate at all, and a count part that models ongoing innovation, respectively.
The \cite{hac:Aghion+VanReenen+Zingales:2013} data are available in the \pkg{sandwich} package.
<<innovation-data>>= 
data("InstInnovation", package = "sandwich")
@ 
Hurdle models are fitted with the \code{hurdle} function from the \pkg{pscl} package \citep{hac:Zeileis+Kleiber+Jackman:2008}. Here, the count model family chosen is a negative binomial, and the zero hurdle model family is a binomial with logit link.
<<innovation-model>>=
library("pscl")
h.innov <- hurdle(
  cites ~ institutions + log(capital/employment) + log(sales),
  data = InstInnovation, dist = "negbin")
@
Below, a comparison of "classical", HC0 and clustered standard errors for an exemplary hurdle model is shown. Standard errors are clustered by companies, with a total of 803 clusters.
<<innovation-se>>= 
library("sandwich")
vc <- list(
  "classical" = vcov(h.innov),
  "HC0" = sandwich(h.innov),
  "cluster" = vcovCL(h.innov, cluster = InstInnovation$company)
)
sapply(vc, function(x) sqrt(diag(x)))
@
What can be observed is that when the data are clustered, ``classical'' standard errors can greatly overstate estimator precision.
Then, for the exemplary hurdle model, clustered standard errors are scaled up by factors between $1.48$ and $1.86$ compared to HC0  standard errors.

\section{Simulation} \label{sec:simulation}

Next, we run a Monte Carlo simulation to assess the methods' performance in a simulation study.
The aim is to test clustered standard errors beyond linear and generalized linear models. For the linear model, there are a couple of simulation studies in the literature \citep{hac:Cameron+Gelbach+Miller:2008,hac:Arceneaux+Nickerson:2009,hac:Petersen:2009,hac:Cameron+Gelbach+Miller:2011,hac:Harden:2011,hac:Thompson:2011,hac:Cameron+Miller:2015,hac:Jin:2015}, far less for generalized linear models \citep[see for example][]{hac:Miglioretti+Heagerty:2007} and, up to our knowledge, none for models beyond \code{lm()} and \code{glm()}.

\subsection{Simulation design}
The simulations are each composed of $10,000$ replications.
We systematically vary parameters $\rho$ and $G$. $\rho$ determines the strength of cluster correlation and varies from $0$ to $0.9$. The number of clusters $G$ ranges from $10$ to $50, 100, 150, 200$ to $250$. Numerous studies \citep{hac:Green+Vavreck:2008,hac:Arceneaux+Nickerson:2009,hac:Harden:2011} confirmed that the higher the number of clusters, the lower the bias of the standard errors (in linear regression models). Furthermore, we only have a look at balanced cluster, observations per cluster are fixed to $5$.

\subsubsection{Linear predictor}
The linear predictor is
\begin{eqnarray} \label{eq:predictor}
h(\mu_{ig}) = \beta_{0} + \beta_{1} \cdot x_{1,ig} + \beta_{2} \cdot x_{2,g} + \beta_{3} \cdot x_{3,ig}
\end{eqnarray}
with a link function $h$ and expected value $\mu_{ig}$. 
In order to generate within cluster error correlation, there are two possible options. The first option is to take the marginal model Equation~\ref{eq:predictor} and introduce correlation via a copula. The second option is to add a random effect in Equation~\ref{eq:predictor}. 

It has to be mentioned that for the linear model with an identity link function $h(\cdot)$, both options amount to the same thing. Though for models other than \code{lm()}, the two options are different. In the simulation exercises, the copula option is favoured, because for models other than \code{lm()}, including a random effect simultaneously includes a bias, too.

We analyze three regressor variables
\begin{eqnarray} \label{eq:regressors}
x_{1,ig} & \sim & \rho_{x} \cdot \mathcal{N}_{g}(0, 1) + (1 - \rho_{x}) \cdot \mathcal{N}_{ig}(0, 1) \label{x1} \\ 
x_{2,g} & \sim & \mathcal{N}_{g}(0, 1) \label{x2} \\
x_{3,ig} & \sim & \mathcal{N}_{ig}(0, 1) \label{x3}
\end{eqnarray}
Regressor $x_{1,ig}$ in Equation~\ref{x1} is composed of a linear combination of a random draw at cluster level ($\mathcal{N}_{g}$) and a random draw at individual level ($\mathcal{N}_{ig}$), regressor $x_{2,g}$ in Equation~\ref{x2} is composed of a random draw at cluster level, and regressor $x_{3,ig}$ in Equation~\ref{x3} consists of a random draw at individual level. In most of the simulations, only a single regressor $x_{1,ig}$ is used.

In line with \cite{hac:Harden:2011}, this is done to produce variation at cluster level, individual level and at a combination of both levels. The cluster correlation of $x_{1,ig}$ is controlled by parameter $\rho_{x}$, and is set to 0.25 per default. This implies at least some within cluster correlation of regressor $x_{1,ig}$. If $\rho_{x} = 1$, regressor $x_{1,ig}$ is equivalent to regressor $x_{2,g}$. Furthermore, if $\rho_{x} = 0$, regressor $x_{1,ig}$ corresponds to regressor $x_{3,ig}$.

The vector of coefficients is fixed to
\begin{eqnarray} \label{eq:coefs}
\beta_{1} & = & (0, 0.85, 0.5, 0.7)^\top \label{beta1} \\ 
\beta_{2} & = & (0, 0.85, 0, 0)^\top \label{beta2}
\end{eqnarray}
even though these values can be interchanged without influencing the results\footnote{Values are equivalent to \cite{hac:Harden:2011}.}.

Response distributions examined include Gaussian (\code{gaussian}), binomial with a logit link (\code{binomial(logit)}), Poisson (\code{poisson}), zero-truncated Poisson (\code{zerotrunc}), Beta (\code{beta}) and zero-inflated Poisson (\code{zip}). \code{vcovCL()} allows estimation of clustered standard errors for all the abovementioned responses (and many more).

\subsubsection{Sandwich covariances}
Standard errors being compared to each other include iid standard errors without heteroscedasticity and without autocorrelation (\code{classical}), HC0 standard errors \citep{hac:White:1980}, Driscoll and Kraay standard errors (\code{DK}) \citep{hac:Driscoll+Kraay:1998}, panel-corrected standard errors (\code{PC}) a la \cite{hac:Beck+Katz:1995} and clustered standard errors with HC0 to HC3 adjustment (\code{HC0-cluster}, \code{HC1-cluster}, \code{HC2-cluster}, \code{HC3-cluster}). In addition, standard errors from a random effects model (\code{random}) and from a GEE with exchangeable correlation structure (\code{gee}) are compared.

\subsubsection{Outcome measure}
In order to assess the validity of statistical inference, the coverage rate is the outcome measure of interest.
If standard errors are estimated accurately, the coverage rate of the $95$\% confidence interval should be close to 0.95. 
Values less than 0.925 will be considered to have underestimated standard errors towards a Type I error, while values greater than 0.975 will be considered to have overestimated standard errors towards a Type II error.

\subsubsection{Simulation code}
The \proglang{R} script \code{sim-CL.R} comprises the simulation code that simulates the data generating process that has been described above and includes three functions: \code{dgp()}, \code{fit()} and \code{sim()}.
While \code{dgp()} specifies the data generating process and generates a data frame with (at most) three regressors \code{x1}, \code{x2}, \code{x3} as well as cluster dimensions \code{id} and \code{time}.
\code{fit()} is responsible for the model fitting, the covariances as well as further outcomes (bias, mad, power and coverage). \code{sim()} conducts all simulations and provides for parallelization support.

%\begin{table}[t!]
%  \centering
%\begin{tabular}{| l | p{6cm} | p{7cm} |}
%  \hline
%  model & formula & model fit \\ \hline
%  \code{m} & \code{formula <- response ~ x1 + x2 + x3} & \code{lm(formula, data = data)} \\ \hline
%  \code{m_fe} & \code{formula_fe <- update(formula, . ~ . + id)} &\code{lm(formula_fe, data = data)} \\ \hline
%  \code{m_re} & \code{formula_re <- update(formula, . ~ . + (1 | id))} & \code{lme4::lmer(formula_re, data = %data, REML = FALSE)} \\ \hline
%  \code{m_gee} & \code{formula <- response ~ x1 + x2 + x3} & \code{geepack::geeglm(formula, data = data, id = %id, corstr = "exchangeable", family = gaussian)} \\ \hline
%\end{tabular}
%\caption{\code{sim-CL.R}: model fitting for a Gaussian response}
%  \label{tab:models}
%\end{table}

Four models are fitted with a Gaussian response: A pooled model (\code{m}), a fixed effects model (\code{m_fe}), a random effects model (\code{m_re}) and a GEE with an exchangeable correlation structure (\code{m_gee}). The fixed effects model includes fixed \code{id} effects, the random effects model is specified as a model with random intercepts. Model fittig for random effects models and GEEs requires the packages \pkg{lme4} and \pkg{geepack}, respectively.

Table~\ref{tab:vcov} shows how the different types of covariances are calculated for a Gaussian response. It has to be noticed, however, that it is not possible to fit all model types for all mentioned response distributions. There are some limitations for random effects models as well as for GEEs.

Furthermore, to calculate clustered standard errors, the square root of the diagonal elements of the covariance matrices in Table~\ref{tab:vcov} is calculated.

\begin{table}[t!]
  \centering
\begin{tabular}{ | l | l | p{9cm} |}
  \hline
  vcov label & model & covariance matrix \\ \hline
  \code{classical} & \code{m} & \code{vcov(m)} \\ \hline
  \code{HC0} & \code{m} & \code{sandwich(m)} \\ \hline
%  \code{HC1} & \code{m} & \code{vcovHC(m, type = "HC1")} \\ \hline
%  \code{HC2} &\code{m} & \code{vcovHC(m, type = "HC2")} \\ \hline
%  \code{HC3} & \code{m} & \code{vcovHC(m, type = "HC3")} \\ \hline
  \code{HC0-cluster} & \code{m} & \code{vcovCL(m, cluster = data\$id, type = "HC0")} \\ \hline
  \code{HC1-cluster} & \code{m} & \code{vcovCL(m, cluster = data\$id, type = "HC1")} \\ \hline
  \code{HC2-cluster} & \code{m} & \code{vcovCL(m, cluster = data\$id, type = "HC2")} \\ \hline
  \code{HC3-cluster} & \code{m} & \code{vcovCL(m, cluster = data\$id, type = "HC3")} \\ \hline
  \code{fixed} & \code{m_fe} & \code{vcov(m_fe)[1L:length(coef(m)), 1L:length(coef(m))]} \\ \hline
  \code{random} & \code{m_re} & \code{vcov(m_re)} \\ \hline
  \code{gee} & \code{m_gee}  & \code{m_gee\$geese\$vbeta} \\ \hline
  \code{DK} & \code{m} & \code{vcovPL(m, cluster = data\$id, adjust = FALSE)} \\ \hline
  \code{BK} & \code{m} & \code{vcovPC(m, cluster = data\$id,
                             order.by = data\$round, kronecker = TRUE)} \\ \hline
\end{tabular}
\caption{\code{sim-CL.R}: covariance matrices for a Gaussian response}
\label{tab:vcov}
\end{table}

\subsection{Results}
The questions of interest in our simulation study can be summarized in the following four experiments:
\begin{itemize}
   \item Experiment I: How do the different model/covariance estimators perform for different types of regressors in the linear regression model?
   \item Experiment II: How do clustered covariances perform for glm type of responses?
   \item Experiment III: How do clustered covariances perform for beyond glm responses?
   \item Experiment IV: How do clustered covariances with HC0--HC3 bias correction perform for different type of responses?
\end{itemize}
Figure~\ref{fig:sim-01} shows the results from Experiment I and plots the coverage probabilities for the coefficients \code{x1} in Equation~\ref{x1}, \code{x2} in Equation~\ref{x2} and \code{x3} in Equation~\ref{x3} on the y-axis. Regressor \code{x1} is composed of a combination of a random draw at cluster level and a random draw at individual level, with a cluster correlation of 0.25. Regressor \code{x2} consists of a random draw at cluster level, and regressor \code{x3} is composed of a random draw at individual level. 

On the x-axis the within cluster correlation $\rho$ is shown, running from 0 (no correlation) to 0.9 (high correlation).
Depending on the regressors, the implications of running the cluster correlation $\rho$ from 0 to 0.9 on different covariance estimators are substantial.

Starting with regressor \code{x1}, at $\rho = 0$, all standard errors except the Driscoll and Kraay standard errors perform well. It can be observed that when the number of observations within each cluster is small with only $N_g = 5$, Driscoll and Kraay standard errors are substantially underestimated, even for a $\rho$ of 0.
Panel-corrected standard errors for regressor \code{x1} underestimate the ``true'' standard errors, that applies all the more the higher $\rho$. To a smaller extent, the same holds true for ``classical'' standard errors.
As noted by \cite{hac:Hoechle:2007}, PCSE can be quite imprecise if the crosss-sectional dimension is large compared to the time dimension.

It can be observed that for regressor \code{x2}, Driscoll and Kraay standard errors, PCSE, and to a smaller extent also ``classical'' and HC0 standard errors are getting worse the larger the within cluster correlation. ``classical'' standard errors are biased downwards because of the wrong assumption of independent observations \citep[see also][]{hac:Harden:2011}.

For regressor \code{x3}, independent of the value of $\rho$, all methods perform well (except again the Driscoll and Kraay estimator).

In summary, it holds for the linear model that if a regressor exhibits within cluster correlation even to a small extent, HC0 and ``classical'' standard errors that do not take the cluster dimension into account, deteriorate with an increasing $\rho$. In addition, PCSE and Driscoll and Kraay standard errors do not perform sufficiently well because of a too small $N_g$.

As the effects of regressor \code{x1} are in between the effects of regressors \code{x2} (varies at cluster level) and \code{x3} (varies at individual level), we limit ourselves to a single regressor \code{x1} in the following simulation experiments.

\setkeys{Gin}{width=\textwidth} 
\begin{figure}[t!] 
\centering
<<sim-01-figure,echo=FALSE,fig=TRUE,height=5,width=10>>=
my.settings <- canonical.theme(color = TRUE)
my.settings[["strip.background"]]$col <- "gray"
my.settings[["strip.border"]]$col <- "black"
my.settings[["superpose.line"]]$lwd <- 1
levels(s01$vcov) <- c("PC", "DK", "gee", "HC0", "HC0-cluster", "random", "classical")
my.settings[["superpose.line"]]$col <- c("#006400", "#00868B", "#e41a1c", "#f781bf", "#377eb8", "#984ea3", "#ff7f00")
my.settings[["superpose.symbol"]]$col <- c("#006400", "#00868B", "#e41a1c", "#f781bf", "#377eb8", "#984ea3", "#ff7f00")
xyplot(coverage ~ rho | par, groups = ~ factor(vcov),
  data = s01, subset = par != "(Intercept)",
  ylim = c(0, 1),
  type = "b", xlab = expression(rho), ylab = "coverage",
  auto.key = list(columns = 3),
  par.strip.text = list(col = "black"), par.settings = my.settings,
  panel = panel.xyref)
@ 
\caption{Fixed: $N_{g} = 5$, $G = 100$, $\rho_{x} = 0.25$, $F = \mathcal{N}$. Copula.}
\label{fig:sim-01}
\end{figure}

Figure~\ref{fig:sim-02} illustrates the results from Experiment II. The y-axis represents again the coverage, and $\rho$ is plotted on the x-axis. Three response distributions are compared: Gaussian, binomial (with a logit link) and Poisson. Unlike in Experiment I, only a single regressor \code{x1} is included, with a $\rho_x$ of 0.25.
The left panel depicts the coverage for the Gaussian response, and thus corresponds to the left panel in Figure~\ref{fig:sim-01}. 

Results are qualitatively similar for the binomial and Poisson responses. The only exception are the standard errors from a random effects model, where the coverage is moving faster downwards than for the other standard errors. 
One can thus conclude that clustered standard errors will also work for glm's.

\begin{figure}[t!]
\centering
<<sim-02-figure,echo=FALSE,fig=TRUE,height=5,width=10>>=
my.settings <- canonical.theme(color = TRUE)
my.settings[["strip.background"]]$col <- "gray"
my.settings[["strip.border"]]$col <- "black"
my.settings[["superpose.line"]]$lwd <- 1
levels(s02$vcov) <- c("PC", "DK", "gee", "HC0", "HC0-cluster", "random", "classical")
levels(s02$dist) <- c("gaussian", "binomial(logit)", "poisson")
my.settings[["superpose.line"]]$col <- c("#006400", "#00868B", "#e41a1c", "#f781bf", "#377eb8", "#984ea3", "#ff7f00")
my.settings[["superpose.symbol"]]$col <- c("#006400", "#00868B" , "#e41a1c", "#f781bf", "#377eb8", "#984ea3", "#ff7f00")
xyplot(coverage ~ rho | dist, groups = ~ factor(vcov),
  data = s02, subset = par != "(Intercept)",
  ylim = c(0.5, 1),
  type = "b", xlab = expression(rho), ylab = "coverage",
  auto.key = list(columns = 3),
  par.strip.text = list(col = "black"), par.settings = my.settings,
  panel = panel.xyref)
@ 
\caption{Fixed: $N_{g} = 5$, $G = 100$, $\rho_{x} = 0.25$, $F = \mathcal{N}$, Binomial, Poisson. Copula.}
\label{fig:sim-02}
\end{figure}

Figure~\ref{fig:sim-03} shows the outcome of Experiment III and reveals the strength of the new function \code{vcovCL()} that comes with the feature that allows to estimate clustered covariances for models beyond glms. The y-axis represents again the coverage, and $\rho$ is plotted on the x-axis. 
Comparisons are made between beta regression, zerotruncated Poisson and zero-inflated Poisson (ZIP). For these response distributions, clustered standard errors perform well and remain nearly constant even for an increasing $\rho$. Standard errors that do not take into account the cluster correlation, like ``classical'' and HC0 standard errors underestimate the true standard errors more and more the larger $\rho$.

\begin{figure}[t!] 
\centering
<<sim-03-figure,echo=FALSE,fig=TRUE,height=5,width=10>>=
s03 <- na.omit(s03)
my.settings <- canonical.theme(color = TRUE)
my.settings[["strip.background"]]$col <- "gray"
my.settings[["strip.border"]]$col <- "black"
my.settings[["superpose.line"]]$lwd <- 1
levels(s03$vcov) <- c("HC0", "HC0-cluster", "classical")
levels(s03$dist) <- c("betareg", "zerotrunc", "zeroinfl")
my.settings[["superpose.line"]]$col <- c("#f781bf", "#377eb8", "#ff7f00")
my.settings[["superpose.symbol"]]$col <- c("#f781bf", "#377eb8", "#ff7f00")
xyplot(coverage ~ rho | dist, groups = ~ factor(vcov),
  data = s03, subset = par != "(Intercept)",
  ylim = c(0.8, 1),
  type = "b", xlab = expression(rho), ylab = "coverage",
  auto.key = list(columns = 2),
  par.strip.text = list(col = "black"), par.settings = my.settings,
  panel = panel.xyref)
@ 
\caption{Fixed: $N_{g} = 5$, $G = 100$, $\rho_{x} = 0.25$, $F =$ Beta, zerotrunc. Poisson, ZIP. Copula.}
\label{fig:sim-03}
\end{figure}

Figure~\ref{fig:sim-04} depicts the findings of Experiment IV. The y-axis represents again the coverage, but in contrast to the other simulation experiments, the number of clusters $G$ is plotted on the x-axis, ranging from 10 to 50 clusters, and in further steps of 50 up to 250 clusters. Gaussian, binomial and Poisson responses are compared with each other, with the focus on clustered standard errors with HC0--HC3 types of bias correction.

In most cases, all of the standard errors are underestimated for $G = 10$ clusters (except clustered standard errors with HC3 bias correction for the binomial response). The larger the number of clusters $G$, the better the coverage and the less standard errors are underestimated. It can be observed that about 50 clusters is often enough for accurate inference. This result is well known in the literature \citep[][among others]{hac:Arceneaux+Nickerson:2009,hac:Petersen:2009,hac:Harden:2011,hac:Cameron+Miller:2015}.

Additionally, it can be observed that the higher the number of clusters, the less the different types of HC bias correction make the difference. However, for a small number of clusters, the HC3 correction works best, followed by HC2, HC1 and HC0. For the linear model, \cite{hac:Long+Ervin:2000} suggest to use HC3 for small samples (with less than 250 observations).

\begin{figure}[t!] 
<<sim-04-figure,echo=FALSE,fig=TRUE,height=5,width=10>>=
my.settings <- canonical.theme(color = TRUE)
my.settings[["strip.background"]]$col <- "gray"
my.settings[["strip.border"]]$col <- "black"
my.settings[["superpose.line"]]$lwd <- 1
levels(s04$vcov) <- c("HC0-cluster", "HC1-cluster", "HC2-cluster", "HC3-cluster")
levels(s04$dist) <- c("gaussian", "binomial(logit)", "poisson")
my.settings[["superpose.line"]]$col <- c("#377eb8", "#00E5EE", "#e41a1c", "#4daf4a")
my.settings[["superpose.symbol"]]$col <- c("#377eb8", "#00E5EE","#e41a1c", "#4daf4a")
xyplot(coverage ~ nid | dist, groups = ~ factor(vcov),
  data = na.omit(s04), subset = par != "(Intercept)",
  type = "b", xlab = "G", ylab = "coverage",
  auto.key = list(columns = 2),
  par.strip.text = list(col = "black"), par.settings = my.settings,
  panel = panel.xyref)
@ 
\caption{Fixed: $N_{g} = 5$, $\rho = 0.25$, $\rho_{x} = 0.25$, $F = \mathcal{N}$, Binomial, Poisson. Copula.}
\label{fig:sim-04}
\end{figure}

\section*{Computation details}

The packages \pkg{sandwich}, \pkg{countreg}, \pkg{geepack}, \pkg{lattice}, \pkg{lme4}, \pkg{lmtest},\pkg{multiwayvcov}, \pkg{plm} and \pkg{pscl} are
required for the applications in this paper.
For replication of the simulation exercises, the \proglang{R} script \code{sim-CL.R} is required.

\proglang{R} version \Sexpr{paste(R.Version()[6:7], collapse = ".")} has been used for computations.
Package versions that have been employed are \pkg{sandwich}
\Sexpr{gsub("-", "--", packageDescription("sandwich")$Version)},
\pkg{countreg} \Sexpr{gsub("-", "--", packageDescription("countreg")$Version)},
\pkg{geepack} \Sexpr{gsub("-", "--", packageDescription("geepack")$Version)},
\pkg{lattice} \Sexpr{gsub("-", "--", packageDescription("lattice")$Version)},
\pkg{lme4} \Sexpr{gsub("-", "--", packageDescription("lme4")$Version)},
\pkg{lmtest} \Sexpr{gsub("-", "--", packageDescription("lmtest")$Version)},
\pkg{multiwayvcov} \Sexpr{gsub("-", "--", packageDescription("multiwayvcov")$Version)},
\pkg{plm} \Sexpr{gsub("-", "--", packageDescription("plm")$Version)},
and \pkg{pscl} \Sexpr{gsub("-", "--", packageDescription("pscl")$Version)}
have been used.

\proglang{R} itself and all packages (except \pkg{countreg}) used are available from
CRAN at \url{https://CRAN.R-project.org/}. 
\pkg{countreg} is accessible from \url{https://R-Forge.R-project.org/projects/countreg/}.


\bibliography{hac}

\newpage

\begin{appendix}

%% for "plain pretty" printing
\DefineVerbatimEnvironment{Sinput}{Verbatim}{}
<<echo=FALSE>>=
options(prompt = "  ")
@

\section{Simulation of correlated data with a random cluster effect} \label{sec:rceffect}

Introducing within cluster error correlation via a random cluster effect instead of a copula comes along with introducing a bias, which also affects the coverage.
The linear predictor in Equation~\ref{eq:predictor} with a random cluster effect $v_g$ is
\begin{eqnarray} \label{eq:predictorv}
h(\mu_{ig}) = \beta_{0} + \beta_{1} \cdot x_{1,ig} + \beta_{2} \cdot x_{2,g} + \beta_{3} \cdot x_{3,ig} + v_g,
\end{eqnarray}
where $v_g$ is a random draw at cluster level $v_{g} \sim \mathcal{N}_{g}(0, \frac{\rho}{1 - \rho})$.
$\rho$ ranges from 0 to 0.9 and determines again the importance of the random cluster effect.

Contrasting coverage and bias plots for models except the linear model depicts the problem caused by this approach.

\begin{figure}[t!] 
\centering
<<sim-05-figure,echo=FALSE,fig=TRUE,height=5,width=10>>=
my.settings <- canonical.theme(color = TRUE)
my.settings[["strip.background"]]$col <- "gray"
my.settings[["strip.border"]]$col <- "black"
my.settings[["superpose.line"]]$lwd <- 1
levels(s05$vcov) <- c("HC0", "HC0-cluster", "classical")
levels(s05$dist) <- c("gaussian", "binomial(logit)", "poisson")
my.settings[["superpose.line"]]$col <- c("#f781bf", "#377eb8", "#ff7f00")
my.settings[["superpose.symbol"]]$col <- c("#f781bf", "#377eb8", "#ff7f00")
xyplot(coverage ~ rho | dist, groups = ~ factor(vcov),
  data = s05, subset = par != "(Intercept)",
  ylim = c(0, 1),
  type = "b", xlab = expression(rho), ylab = "coverage",
  auto.key = list(columns = 2),
  par.strip.text = list(col = "black"), par.settings = my.settings,
  panel = panel.xyref)
@  
<<sim-05-bias-figure,echo=FALSE,fig=TRUE,height=5,width=10>>=
panel.xyref <- function(x, y, ...) {
  panel.abline(h = 0, col = 2)
  panel.xyplot(x, y, ...)  
}
my.settings <- canonical.theme(color = TRUE)
my.settings[["strip.background"]]$col <- "gray"
my.settings[["strip.border"]]$col <- "black"
my.settings[["superpose.line"]]$lwd <- 1
levels(s05$vcov) <- c("HC0", "HC0-cluster", "classical")
levels(s05$dist) <- c("gaussian", "binomial(logit)", "poisson")
my.settings[["superpose.line"]]$col <- c("slategray", "slategray", "slategray")
my.settings[["superpose.symbol"]]$col <- c("slategray", "slategray", "slategray")
xyplot(bias ~ rho | dist, groups = ~ factor(vcov),
  data = s05, subset = par != "(Intercept)",
  ylim = c(-0.5, 0.1),
  type = "b", xlab = expression(rho), ylab = "bias",
  par.strip.text = list(col = "black"), par.settings = my.settings,
  panel = panel.xyref)
@ 
\caption{Fixed: $N_{g} = 5$, $G = 100$, $\rho_{x} = 0.25$, $F = \mathcal{N}$, Binomial, Poisson. Random effect.}
\label{fig:sim-05-01}
\end{figure}

Figure~\ref{fig:sim-05-01} constrasts coverage (in the upper panel) and bias (in the lower panel) for Gaussian, binomial (with a logit link) and Poisson response distributions with a single regressor \code{x1}. Regressor \code{x1} exhibits a cluster correlation $\rho_x$ of 0.25. Each of the 10,000 simulated datasets consists of 100 clusters with 5 observations per cluster.

For the Gaussian response, the random cluster effect does not introduce a bias, but for the binomial and Poisson responses, the random cluster effect comes along with a negative bias. It can be observed that the bias is rising with the random cluster effect $\rho$.

\begin{figure}[t!] 
\centering
<<sim-02-1figure,echo=FALSE,fig=TRUE,height=5,width=10>>=
panel.xyref <- function(x, y, ...) {
  panel.abline(h = 0.95, col = 2)
  panel.xyplot(x, y, ...)  
}
my.settings <- canonical.theme(color = TRUE)
my.settings[["strip.background"]]$col <- "gray"
my.settings[["strip.border"]]$col <- "black"
my.settings[["superpose.line"]]$lwd <- 1
levels(s02$vcov) <- c("PC", "DK", "gee", "HC0", "HC0-cluster", "random", "classical")
levels(s02$dist) <- c("gaussian", "binomial(logit)", "poisson")
my.settings[["superpose.line"]]$col <- c("#f781bf", "#377eb8", "#ff7f00")
my.settings[["superpose.symbol"]]$col <- c("#f781bf", "#377eb8", "#ff7f00")
xyplot(coverage ~ rho | dist, groups = ~ factor(vcov),
  data = s02[which(s02$vcov %in% c("classical", "HC0", "HC0-cluster")), ], subset = par != "(Intercept)",
  ylim = c(0, 1),
  type = "b", xlab = expression(rho), ylab = "coverage",
  auto.key = list(columns = 2),
  par.strip.text = list(col = "black"), par.settings = my.settings,
  panel = panel.xyref)
@ 
<<sim-02-1-bias-figure,echo=FALSE,fig=TRUE,height=5,width=10>>=
panel.xyref <- function(x, y, ...) {
  panel.abline(h = 0, col = 2)
  panel.xyplot(x, y, ...)  
}
my.settings <- canonical.theme(color = TRUE)
my.settings[["strip.background"]]$col <- "gray"
my.settings[["strip.border"]]$col <- "black"
my.settings[["superpose.line"]]$lwd <- 1
levels(s02$vcov) <- c("PC", "DK", "gee", "HC0", "HC0-cluster", "random", "classical")
levels(s02$dist) <- c("gaussian", "binomial(logit)", "poisson")
my.settings[["superpose.line"]]$col <- c("slategray", "slategray", "slategray")
my.settings[["superpose.symbol"]]$col <- c("slategray", "slategray", "slategray")
xyplot(bias ~ rho | dist, groups = ~ factor(vcov),
  data = s02[which(s02$vcov %in% c("classical", "HC0", "HC0-cluster")), ], subset = par != "(Intercept)",
  ylim = c(-0.5, 0.1),
  type = "b", xlab = expression(rho), ylab = "bias",
  par.strip.text = list(col = "black"), par.settings = my.settings,
  panel = panel.xyref)
@ 
\caption{Fixed: $N_{g} = 5$, $G = 100$, $\rho_{x} = 0.25$, $F = \mathcal{N}$, Binomial, Poisson. Copula.}
\label{fig:sim-02-1}
\end{figure}

Introducing within cluster correlation via a copula as is done for all simulation exercises in Section~\ref{sec:simulation} does not introduce a bias. This can be observed in Figure~\ref{fig:sim-02-1}, where estimators are unbiased for the response distributions and thus no further deteriorate the coverage.

\end{appendix}

\end{document}
