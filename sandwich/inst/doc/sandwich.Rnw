\documentclass{Z}
%% need no \usepackage{Sweave}

\author{Achim Zeileis\\Wirtschaftsuniversit\"at Wien}
\Plainauthor{Achim Zeileis}

\title{Econometric Computing with HC and HAC Covariance Matrix Estimators}

\Keywords{covariance matrix estimator, heteroskedasticity, autocorrelation,
  estimating functions, econometric computing, \proglang{R}}
\Plainkeywords{covariance matrix estimator, heteroskedasticity, autocorrelation,
  estimating functions, econometric computing, R}

\Abstract{
  sandwich
}

\begin{document}

\SweaveOpts{engine=R,eps=FALSE}
%\VignetteIndexEntry{Econometric Computing with HC and HAC Covariance Matrix Estimators}
%\VignetteDepends{sandwich,zoo,lmtest,strucchange,scatterplot3d}
%\VignetteKeywords{covariance matrix estimator, heteroskedasticity, autocorrelation, estimating functions, econometric computing, R}
%\VignettePackage{sandwich}


<<preliminaries,echo=FALSE,results=hide>>=
library(zoo)
library(sandwich)
library(strucchange)
library(lmtest)
@

\section{Introduction} \label{sec:intro}

\cite{hac:R:2004}
\cite{hac:Cribari-Neto+Zarkos:2003}


stress econometric computing

reusable components

covariance matrices not only as options to certain test but 
as stand-alone functions which can be plugged into various 
inference procedures

\section{The linear regression model} \label{sec:model}

To fix notations, we consider the linear regression model
\begin{equation} \label{eq:lm}
  y_i \quad = \quad x_i^\top \beta + u_i \qquad (i = 1, \dots, n),
\end{equation}
with dependent variable $y_i$, $k$-dimensional regressor
$x_i$ with coefficient vector $\beta$ and error term $u_i$.
In the usual matrix notation comprising all $n$ observations 
this can be formulated as $y = X \beta + u$.

In the general linear model, it is typically assumed that the
errors have zero mean and variance $\VAR[u] = \Omega$. Under
suitable regularity conditions \citep[see e.g.,][]{hac:Greene:1997},
the coefficients $\beta$ can be consistently estimated by 
ordinary least squares (OLS) giving the well-known OLS
estimator $\hat \beta$ with corresponding OLS residuals $\hat u_i$:
\begin{eqnarray}
  \hat \beta & = & \left( X^\top X \right)^{-1} X^\top y \\
  \hat u & = & (I_n - H) \, u \; = \; (I_n - X \left( X^\top X \right)^{-1} X^\top) \, u
\end{eqnarray}
where $I_n$ is the $n$-dimensional identity matrix and $H$ is usually 
called hat matrix. The estimates $\hat \beta$ are unbiased and are
asymptotically normal with covariance matrix $\Psi$ which is usally
denoted in one of the two forms given below.

\begin{eqnarray}
  \Psi \; = \; \VAR[\hat \beta] & = & 
    \left( X^\top X \right)^{-1} X^\top \Omega X \left( X^\top X \right)^{-1} \label{eq:PsiHC} \\
    & = & \frac{1}{n} \left( \frac{1}{n} X^\top X \right)^{-1} \Phi
          \left( \frac{1}{n} X^\top X \right)^{-1} \label{eq:PsiHAC}
\end{eqnarray}

where $\Phi = n^{-1} X^\top \Omega X^\top$ is essiantlly the covariance
matrix of the estimating functions $V_i(\beta) = x_i (y_i - x_i^\top \beta)$.
The estimating functions evaluated at the parameter estimates
$\hat V_i = V_i(\hat \beta)$ have then sum zero.

For doing inference in the linear regression model, it is essential
to have a consistent estimator for $\Psi$---the most common inferential
task being to test whether one of the coefficients $\beta_j$ is zero which is is usually
assessed using the $t$ ratio $\beta_j/\sqrt{\hat \Psi_{jj}}$. What kind of 
estimator is used for $\Psi$ depends on the assumptions about $\Omega$ that
are used: In the classical linear model independent and homoskedastic
errors with variance $\sigma^2$ are assumed yielding $\Omega = \sigma^2 I_n$
and $\Psi = \sigma^2 (X^\top X)^{-1}$ which can be easily estimated by
plugging in the usual OLS estimate
${\hat \sigma}^2 = (n-k)^{-1} \sum_{i = 1}^n {\hat u_i}^2$.
But if the independence and/or homoskedasticity assumption is violated,
inference based on the estimator for such
squerical errors $\hat \Psi_{\mathrm{const}} = \hat \sigma (X^\top X)^{-1}$
will be biased. HC and HAC estimators tackle
this problem by plugging an estimate $\hat \Omega$ or $\hat \Phi$ into
(\ref{eq:PsiHC}) or (\ref{eq:PsiHAC}) respectively which are consistent
in the presence of heteroskedasticity and autocorrelation respectively.
Such estimators and their implementation are described in the following
section.


\section{Estimating covariance matrices}

\subsection{Dealing with heteroskedasticity}

$\hat \Psi_{\mathrm{HC}}$ plugging in a 
$\hat \Omega = \mathrm{diag}(\omega_1, \dots, \omega_n)$

\begin{eqnarray*}
  \mathrm{const:} \quad \omega_i & = & \hat \sigma^2 \\
    \mathrm{HC0:} \quad \omega_i & = & {\hat u_i}^2 \\
    \mathrm{HC1:} \quad \omega_i & = & \frac{n}{n-k} \, {\hat u_i}^2 \\
    \mathrm{HC2:} \quad \omega_i & = & \frac{{\hat u_i}^2}{1 - h_i} \\
    \mathrm{HC3:} \quad \omega_i & = & \frac{{\hat u_i}^2}{(1 - h_i)^2} \\
    \mathrm{HC4:} \quad \omega_i & = & \frac{{\hat u_i}^2}{(1 - h_i)^{\delta_i}}
\end{eqnarray*}

where $h_i = H_{ii}$ are the diagonal elements of the hat matrix
and $\delta_i = \min\{4, h_i/\bar h\}$.

\begin{verbatim}
vcovHC(lmobject, omega = NULL, type = "HC3",
  order.by = NULL)
\end{verbatim}

\code{"HC3", "const", "HC", "HC0", "HC1", "HC2", "HC4"}

\code{omega(residuals, diaghat, df)}


\cite{hac:White:1980}
\cite{hac:MacKinnon+White:1985}
\cite{hac:Long+Ervin:2000}
\cite{hac:Cribari-Neto:2004}


\subsection{Dealing with autocorrelation}

$\hat \Psi_{\mathrm{HAC}}$ plugging in a $\hat \Phi$ with

\begin{equation} \label{eq:HAC}
  \hat \Phi \quad = \quad \frac{1}{n}
  \sum_{i,j = 1}^n w_{|i-j|} \hat V_i \hat V_j^\top
\end{equation}

weights $w_k$ ($k = 0, \dots, n-1$)

finite sample correction $n/(n-k)$

Newey-West or Bartlett kernel
\begin{equation}
  w_i \quad = \quad 1 - \frac{i}{L + 1}
\end{equation}
where $L$ is the maximum lag, other weights are zero. In terms of
a generic bandwidth $B$ usually formulated as $B = L + 1$.

Quadratic spectral kernel
\begin{equation}
  w_i \quad = \quad \frac{3}{x^2} \, \left(\frac{\sin(x)}{x} - \cos (x) \right)
\end{equation}

where $x = 6/5 \pi \cdot i/B$ and $B$ is again a bandwidth parameter.

\begin{verbatim}
vcovHAC(lmobject, weights, 
  order.by = NULL, prewhite = FALSE, adjust = TRUE, sandwich = TRUE)
\end{verbatim}

\code{weights(x, order.by, prewhite, ar.method, data)}

\cite{hac:Newey+West:1987}
\cite{hac:Andrews:1991}
\cite{hac:Andrews+Monahan:1992}
\cite{hac:Lumley+Heagerty:1999}


\section{Applications and illustrations}

$t$ ratio $\beta_j/\sqrt{\hat \Psi_{jj}}$

For computing
$p$ values the asymptotic normal distribution or the $t$ distribution with
$n-k$ degrees of freedom are used.

\subsection{Testing coefficients in cross-sectional data}

\cite{hac:Greene:1997}
\cite{hac:Cribari-Neto:2004}
\cite{hac:Zeileis+Hothorn:2002}
\cite{hac:Fox:2002}

<<hc-data>>=
data(PublicSchools)
ps <- na.omit(PublicSchools)
ps$Income <- ps$Income * 0.0001
@

<<hc-model>>=
fm.ps <- lm(Expenditure ~ Income + I(Income^2), data = ps)
@

<<hc-test>>=
coeftest(fm.ps, df = Inf, vcov = vcovHC(fm.ps, type = "HC0"))
coeftest(fm.ps, df = Inf, vcov = vcovHC(fm.ps, type = "HC4"))
@

\code{vcovHC(fm.ps, type = "HC0")}

\setkeys{Gin}{width=.6\textwidth} 
\begin{figure}[tbh]
\begin{center}
<<hc-plot,fig=TRUE,height=5,width=6,echo=FALSE>>=
plot(Expenditure ~ Income, data = ps,
  xlab = "per capita income",
  ylab = "per capita spending on public schools")
inc <- seq(0.5, 1.2, by = 0.001)
lines(inc, predict(fm.ps, data.frame(Income = inc)), col = 4, lty = 2)
fm.ps2 <- lm(Expenditure ~ Income, data = ps)
abline(fm.ps2, col = 4)
text(ps[2,2], ps[2,1], rownames(ps)[2], pos = 2)
@
\caption{\label{fig:hc} Expenditure on public schools and income with fitted models}
\end{center}
\end{figure}


\subsection{Testing coefficients in time-series data}

\cite{hac:Greene:1997}

<<hac-data>>=
data(Investment)
@

<<hac-model>>=
fm.inv <- lm(RealInv ~ RealGNP + RealInt, data = Investment)
@

<<hac-test>>=
coeftest(fm.inv, df = Inf, vcov = NeweyWest(fm.inv, lag = 4))
@

\setkeys{Gin}{width=.6\textwidth} 
\begin{figure}[tbh]
\begin{center}
<<hac-plot,fig=TRUE,height=4,width=6,echo=FALSE>>=
plot(Investment[, "RealInv"], type = "b", pch = 19, ylab = "Real investment")
lines(ts(fitted(fm.inv), start = 1964), col = 4)
@
\caption{\label{fig:hac} Investment equation data with fitted model}
\end{center}
\end{figure}


\setkeys{Gin}{width=.6\textwidth} 
\begin{figure}[tbh]
\begin{center}
<<hac-plot2,fig=TRUE,height=5,width=6,echo=FALSE>>=
library(scatterplot3d)
s3d <- scatterplot3d(Investment[,c(5,7,6)],
  type = "b", angle = 65, scale.y = 1, pch = 16)
s3d$plane3d(fm.inv, lty.box = "solid", col = 4)
@
\caption{\label{fig:hac2} Investment equation data with fitted model (3D)}
\end{center}
\end{figure}


\subsection{Testing and dating structural changes in the presence of heteroskedasticity and autocorrelation}

If the first component if $x_i$ is equal to unity
\begin{equation}
\sup_{j = 1, \dots, n} \left| \frac{1}{\sqrt{n \, \hat \Phi_{11}}} \; \sum_{i = 1}^{j} \hat u_i  \right|.
\end{equation}

\cite{sc:Bai+Perron:2003}
\cite{sc:Andrews:1993}
\cite{sc:Ploberger+Kraemer:1992}

\cite{hac:Zeileis+Leisch+Hornik:2002}
\cite{hac:Zeileis:2004}


<<sc-data>>=
data(RealInt)
@

<<sc-ocus>>=
ocus <- gefp(RealInt ~ 1, fit = lm, vcov = kernHAC)
@

\code{plot(ocus)}, \code{sctest(ocus)}


<<sc-Fstats>>=
fs <- Fstats(RealInt ~ 1, vcov = kernHAC)
@

\code{sctest(fs)}, \code{plot(fs)}

<<sc-bp>>=
bp <- breakpoints(RealInt ~ 1)
confint(bp, vcov = kernHAC)
@



\setkeys{Gin}{width=\textwidth} 
\begin{figure}[tbh]
\begin{center}
<<sc-plot,fig=TRUE,height=4,width=10,echo=FALSE>>=
par(mfrow = c(1, 2))
plot(ocus, aggregate = FALSE, main = "")
plot(RealInt, ylab = "Real interest rate")
lines(ts(fitted(bp), start = start(RealInt), freq = 4), col = 4)
lines(confint(bp, vcov = kernHAC))
@
\caption{\label{fig:sc} OLS-based CUSUM test (left) and fitted model (right) for real interest data}
\end{center}
\end{figure}


\section{Summary} \label{sec:summary}


\clearpage

\begin{appendix}

\section[R code]{\proglang{R} code}

\subsection{Testing coefficients in cross-sectional data}

Load public schools data,
omit \code{NA} in Wisconsin and scale income:
\begin{verbatim}
data(PublicSchools)
ps <- na.omit(PublicSchools)
ps$Income <- ps$Income * 0.0001
\end{verbatim}

Fit quadratic regression model:
\begin{verbatim}
fm.ps <- lm(Expenditure ~ Income + I(Income^2), data = ps)
\end{verbatim}

Compare standard errors:
\begin{verbatim}
sqrt(diag(vcov(fm.ps)))
sqrt(diag(vcovHC(fm.ps, type = "const")))
sqrt(diag(vcovHC(fm.ps, type = "HC0")))
sqrt(diag(vcovHC(fm.ps, type = "HC3")))
sqrt(diag(vcovHC(fm.ps, type = "HC4")))
\end{verbatim}

Test coefficient of quadratic term:
\begin{verbatim}
coeftest(fm.ps, df = Inf, vcov = vcovHC(fm.ps, type = "HC0"))
coeftest(fm.ps, df = Inf, vcov = vcovHC(fm.ps, type = "HC4"))
\end{verbatim}

Visualization:
\begin{verbatim}
plot(Expenditure ~ Income, data = ps,
  xlab = "per capita income",
  ylab = "per capita spending on public schools")
inc <- seq(0.5, 1.2, by = 0.001)
lines(inc, predict(fm.ps, data.frame(Income = inc)), col = 4, lty = 2)
fm.ps2 <- lm(Expenditure ~ Income, data = ps)
abline(fm.ps2, col = 4)
text(ps[2,2], ps[2,1], rownames(ps)[2], pos = 2)
\end{verbatim}

\subsection{Testing coefficients in time-series data}

Load investment equation data:
\begin{verbatim}
data(Investment)
\end{verbatim}

Fit regression model:
\begin{verbatim}
fm.inv <- lm(RealInv ~ RealGNP + RealInt, data = Investment)
\end{verbatim}

Test coefficients using Newey-West HAC estimator:
\begin{verbatim}
coeftest(fm.inv, df = Inf, vcov = NeweyWest(fm.inv, lag = 4))
\end{verbatim}

Visualization:
\begin{verbatim}
plot(Investment[, "RealInv"], type = "b", pch = 19, ylab = "Real investment")
lines(ts(fitted(fm.inv), start = 1964), col = 4)
\end{verbatim}

3-d visualization:
\begin{verbatim}
library(scatterplot3d)
s3d <- scatterplot3d(Investment[,c(5,7,6)],
  type = "b", angle = 65, scale.y = 1, pch = 16)
s3d$plane3d(fm.inv, lty.box = "solid", col = 4)
\end{verbatim}

\subsection{Testing and dating structural changes in the presence of heteroskedasticity and autocorrelation}

Load real interest series:
\begin{verbatim}
data(RealInt)
\end{verbatim}

OLS-based CUSUM test with quadratic spectral kernel HAC estimate:
\begin{verbatim}
ocus <- gefp(RealInt ~ 1, fit = lm, vcov = kernHAC)
plot(ocus, aggregate = FALSE)
sctest(ocus)
\end{verbatim}

sup$F$ test with quadratic spectral kernel HAC estimate:
\begin{verbatim}
fs <- Fstats(RealInt ~ 1, vcov = kernHAC)
plot(fs)
sctest(fs)
\end{verbatim}

Breakpoint estimation and confidence intervals
with quadratic spectral kernel HAC estimate:
\begin{verbatim}
bp <- breakpoints(RealInt ~ 1)
confint(bp, vcov = kernHAC)
\end{verbatim}

Visualization:
\begin{verbatim}
plot(RealInt, ylab = "Real interest rate")
lines(ts(fitted(bp), start = start(RealInt), freq = 4), col = 4)
lines(confint(bp, vcov = kernHAC))
\end{verbatim}

\end{appendix}

\clearpage

\bibliography{hac}

\end{document}
