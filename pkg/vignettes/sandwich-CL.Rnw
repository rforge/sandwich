\documentclass[nojss]{jss}
\usepackage{thumbpdf,lmodern}
\usepackage{amsmath,amssymb,bm}
\usepackage{multicol}
\usepackage{mathtools}
%% need no \usepackage{Sweave}

\newcommand{\newoperator}[3]{\newcommand*{#1}{\mathop{#2}#3}}
\newcommand{\renewoperator}[3]{\renewcommand*{#1}{\mathop{#2}#3}}
\newcommand{\vI}{\bm I}
\newcommand{\vH}{\bm H}

%% for internal use
\newcommand{\fixme}[1]{\emph{\marginpar{FIXME} (#1)}}
\newcommand{\readme}[1]{\emph{\marginpar{README} (#1)}}

\author{Susanne Berger\\University of Innsbruck \And 
        Nathaniel Graham\\Trinity University Texas \And
        Achim Zeileis\\University of Innsbruck}
\title{Various Versatile Variances: An Object-Oriented Implementation of Clustered Covariances in \proglang{R}}

\Plainauthor{Susanne Berger, Nathaniel Graham, Achim Zeileis}
\Plaintitle{Various Versatile Variances: An Object-Oriented Implementation of Clustered Covariances in R}
\Shorttitle{Various Versatile Variances}

\Keywords{clustered data, clustered covariance matrix estimators, object orientation, simulation, \proglang{R}}
\Plainkeywords{clustered data, clustered covariance matrix estimators, object orientation, simulation, R}

\Abstract{
  Clustered covariances or clustered standard errors are very widely used to
  account for correlated or clustered data, especially in economics, political
  sciences, or other social sciences. They are employed to adjust the inference
  following estimation of a standard least-squares regression or generalized
  linear model estimated by maximum likelihood. Although many publications just
  refer to ``the'' clustered standard errors, there is a surprisingly wide
  variety of clustered covariances, particularly due to different flavors of
  bias corrections. Furthermore, while the linear regression model is certainly
  the most important application case, the same strategies can be employed in
  more general models (e.g. for zero-inflated, censored, or limited responses).

  In \proglang{R}, functions for covariances in clustered or panel models have been
  somewhat scattered or available only for certain modeling functions, notably
  the (generalized) linear regression model. In contrast, an object-oriented
  approach to ``robust'' covariance matrix estimation -- applicable beyond
  \code{lm()} and \code{glm()} -- is available in the \pkg{sandwich} package but
  has been limited to the case of cross-section or time series data. Now,
  this shortcoming has been corrected in \pkg{sandwich} (starting from
  version~2.4.0): Based on methods for two generic functions (\code{estfun()} and
  \code{bread()}), clustered and panel covariances are now provided in
  \code{vcovCL()} and \code{vcovPL()}, respectively. These are directly
  applicable to models from many packages, e.g., including \pkg{MASS}, \pkg{pscl}, \pkg{countreg},
  \pkg{betareg}, among others. Some empirical illustrations are provided
  as well as an assessment of the methods' performance in a simulation study.
}

\Address{
  Susanne Berger, Achim Zeileis\\
  Department of Statistics\\
  Faculty of Economics and Statistics\\
  Universit\"at Innsbruck\\
  Universit\"atsstr.~15\\
  6020 Innsbruck, Austria\\
  E-mail: \email{Susanne.Berger@uibk.ac.at}, \email{Achim.Zeileis@R-project.org}\\
  URL: \url{https://www.uibk.ac.at/statistics/personal/berger/}, \url{https://eeecon.uibk.ac.at/~zeileis/}\\
  
  Nathaniel Graham\\
  Department of Finance and Decision Sciences\\
  Trinity University Texas\\
  One Trinity Place\\
  San Antonio, Texas 78212, United States of America\\
  E-mail: \email{npgraham1@npgraham1.com}\\
  URL: \url{https://sites.google.com/site/npgraham1/}
}

\begin{document}

\SweaveOpts{engine=R,eps=FALSE}
%\VignetteIndexEntry{Various Versatile Variances: An Object-Oriented Implementation of Clustered Covariances in R}
%\VignetteDepends{sandwich,pscl,lattice,geepack,multiwayvcov,plm,lme4,countreg}
%\VignetteKeywords{clustered data, clustered covariance matrix estimators, object orientation, simulation, R}
%\VignettePackage{sandwich}

<<preliminaries,echo=FALSE,results=hide>>=
library("sandwich")
library("pscl")
library("lattice")
library("countreg")
library("lme4")
source("sim-CL.R")
panel.xyref <- function(x, y, ...) {
  panel.abline(h = 0.95, col = 2)
  panel.xyplot(x, y, ...)  
}
options(prompt = "R> ", continue = "+   ")
@ 

% simulations

<<sim-01,echo=FALSE,results=hide>>=
if(file.exists("sim-01.rda")) load("sim-01.rda") else {
set.seed(1)
s01 <- sim(nrep = 10000, nid = 100, nround = 5,
           dist = "gaussian", rho = seq(0, 0.9, by = 0.1), xrho = 0.25,
           coef = c(0, 0.85, 0.5, 0.7), formula = response ~ x1 + x2 + x3,
           vcov = c("without", "HC0", "HC0-id", "random", "gee", "bk", "dk"),
           type = "copula")
save(s01, file = "sim-01.rda")
}
@

<<sim-02,echo=FALSE,results=hide>>=
if(file.exists("sim-02.rda")) load("sim-02.rda") else {
set.seed(2)
s02 <- sim(nrep = 10000, nid = 100, nround = 5,
           dist = c("gaussian", "logit", "poisson"), rho = seq(0, 0.9, by = 0.1), xrho = 0.25,
           coef = c(0, 0.85, 0, 0), formula = response ~ x1,
           vcov = c("without", "HC0", "HC0-id", "random", "gee", "bk", "dk"),
           type = "copula")
save(s02, file = "sim-02.rda")
}
@

<<sim-03,echo=FALSE,results=hide>>=
if(file.exists("sim-03.rda")) load("sim-03.rda") else {
set.seed(3)
s03 <- sim(nrep = 10000, nid = 100, nround = 5,
           dist = c("zerotrunc", "zip", "beta"), rho = seq(0, 0.9, by = 0.1), xrho = 0.25,
           coef = c(0, 0.85, 0, 0), formula = response ~ x1,
           vcov = c("without", "HC0", "HC0-id"),
           type = "copula")
save(s03, file = "sim-03.rda")
}
@

<<sim-04,echo=FALSE,results=hide>>=
if(file.exists("sim-04.rda")) load("sim-04.rda") else {
set.seed(4)
s04 <- sim(nrep = 10000, nid = c(10, seq(50, 250, by = 50)), nround = 5,
           dist = c("gaussian","poisson", "logit"), rho = 0.25, xrho = 0.25,
           coef = c(0, 0.85, 0, 0), formula = response ~ x1,
           vcov = c("HC0-id","HC1-id","HC2-id","HC3-id"),
           type = "copula")
save(s04, file = "sim-04.rda")
}
@

<<sim-05,echo=FALSE,results=hide>>=
if(file.exists("sim-05.rda")) load("sim-05.rda") else {
set.seed(5)
s05 <- sim(nrep = 10000, nid = 100, nround = 5,
           dist = c("gaussian", "logit", "poisson"), rho = seq(0, 0.9, by = 0.1), xrho = 0.25,
           coef = c(0, 0.85, 0, 0), formula = response ~ x1,
           vcov = c("without", "HC0", "HC0-id"),
           type = "ranef")
save(s05, file = "sim-05.rda")
}
@

\section{Introduction} \label{sec:intro}

Observations with correlations between objects of the same group/cluster are
often referred to as ``cluster-correlated'' observations. Each cluster comprises
multiple objects that are correlated within, but not across, clusters, leading
to a nested or hierarchical structure \citep{hac:Galbraith+Daniel+Vissel:2010}.
Ignoring this dependency and pretending observations are independent not only
across but also within the clusters, still leads to parameter estimates that
are consistent (albeit not efficient) in many situations. However, the
observations' information will typically be overestimated and hence lead to
overstated precision of the parameter estimates and inflated type~I errors
in the corresponding tests \citep{hac:Moulton:1986, hac:Moulton:1990}.
Therefore, clustered covariances are widely used to account for clustered
correlations in the data.

Such clustering effects can emerge both in cross-section and in panel (or
longitudinal) data. Typical examples for clustered cross-section data include
firms within the same industry or students within the same school or class.
In panel data, a common source of clustering is that observations for the
same individual at different time points are correlated while the individuals
may be independent \citep{hac:Cameron+Miller:2015}.

This paper contributes to the literature particularly in two respects:
%
(1)~Most importantly, we discuss a set of computational tools for the \proglang{R} system for
statistical computing \citep{hac:R:2017}, providing an object-oriented
implementation of clustered covariances/standard errors in the \proglang{R}
package \pkg{sandwich} \citep{hac:Zeileis:2004a,hac:Zeileis:2006}. Using this
infrastructure, sandwich covariances for cross-section or time series data have
been available for models beyond \code{lm()} or \code{glm()}, e.g., for packages
\pkg{MASS} \citep{hac:Venables+Ripley:2002}, \pkg{pscl}/\pkg{countreg}
\citep{hac:Zeileis+Kleiber+Jackman:2008}, \pkg{betareg}
\citep{hac:Cribari-Neto+Zeileis:2010,hac:Gruen+Kosmidis+Zeileis:2012}, among
many others. However, corresponding functions for clustered or panel data had
not been available in \pkg{sandwich} but have been somewhat scattered or
available only for certain modeling functions.

(2)~Moreover, we perform a Monte Carlo simulation study for various response
distributions with the aim to assess the performance of clustered standard errors
beyond \code{lm()} and \code{glm()}. This also includes special cases for which
such a finite-sample assessment has not yet been carried out in the literature
(to the best of our knowledge).

The rest of this manuscript is structured as follows: Section~\ref{sec:overview}
discusses the idea of clustered covariances and reviews existing \proglang{R}
packages for sandwich as well as clustered covariances.
Section~\ref{sec:methods} deals with the theory behind sandwich covariances,
especially with respect to clustered covariances for cross-sectional and
longitudinal data, clustered data, as well as panel data.
Section~\ref{sec:software} then takes a look behind the scenes of the new
object-oriented \proglang{R} implementation for clustered covariances,
Section~\ref{sec:illu} gives an empirical illustration based on data provided
from \cite{hac:Petersen:2009} and \cite{hac:Aghion+VanReenen+Zingales:2013}.
The simulation setup and results are discussed in Section~\ref{sec:simulation}.

\section{Overview} \label{sec:overview}

There is a range of popular strategies for dealing with clustered dependencies
in regression models. In the statistics literature, random effects (especially
random intercepts) are often introduced to capture unobserved cluster
correlations (e.g., using the \pkg{lme4} package in \proglang{R},
\citealp{hac:Bates+Machler+Bolker:2015}). Alternatively, generalized estimating
equations (GEE) can account for such correlations by adjusting the model's
scores in the estimation, also leading naturally to a clustered covariance
(e.g., available in the \pkg{geepack} package for \proglang{R},
\citealp{hac:Halekoh+Hojsgaard+Yan:2002}). Another approach, widely used
in econometrics and the social sciences, is to assume that the model's score
function was correctly specified but that only the remaining likelihood was
potentially misspecified, e.g., due to a lack of independence as in the case
of clustered correlations (see \citealp{hac:White:1994}, for a classic textbook,
and \citealp{hac:Freedman:2006}, for a criticial review). This approach leaves
the parameter estimator unchanged -- then also known as quasi-maximum likelihood
(QML) estimator or, in GEE jargon, as independence working model -- but adjusts
the covariance matrix by using a sandwich estimator, especially in Wald tests
and corresponding confidence intervals.

Important special cases of this QML approach combined with sandwich covariances
include: (1) independent but heteroscedastic observations necessitating
heteroscedasticity-consistent (HC) covariances
\citep[see e.g.,][]{hac:Long+Ervin:2000}, (2) autocorrelated time series of
observations requiring heteroscedasticity- and autocorrelation-consistent
(HAC) covariances \citep[such as][]{hac:Newey+West:1987,hac:Andrews:1991},
(3) and clustered sandwich covariances for clustered or panel data
\citep[see e.g.,][]{hac:Cameron+Miller:2015}.

Various kinds of sandwich covariances have already been implemented in several
\proglang{R} packages, with the linear regression case receiving most attention.
But some packages also cover more general models.


\subsection[R packages for sandwich covariances]{\proglang{R} packages for sandwich covariances}

The standard \proglang{R} package for sandwich covariance estimators is the
\pkg{sandwich} package \citep{hac:Zeileis:2004a,hac:Zeileis:2006}, which
provides an object-oriented implementation for the building blocks of the
sandwich that rely only on a small set of extractor functions (\code{estfun()}
and \code{bread()}) for fitted model objects. The function \code{sandwich()}
computes a plain sandwich estimate \citep{hac:Eicker:1963,hac:Huber:1967,hac:White:1980}
from a fitted model object, defaulting to what is known as HC0 or HC1 in linear
regresion models. \code{vcovHC()} is a wrapper to \code{sandwich()} combined
with \code{meatHC()} and \code{bread()} to compute general HC covariances
ranging from HC0 to HC5. \code{vcovHAC()}, based on \code{sandwich()} with
\code{meatHAC()} and \code{bread()}, computes HAC covariance matrix estimates.
Further convenience interfaces \code{kernHAC()} for Andrews' kernel HAC
\citep{hac:Andrews:1991} and \code{NeweyWest()} for Newey-West-style HAC
\citep{hac:Newey+West:1987,hac:Newey+West:1994} are available. However,
in versions prior to 2.4.0 of \pkg{sandwich} no similarly object-oriented
approach to clustered sandwich covariances was available.

Another \proglang{R} package that includes heteroscedasticity-consistent
covariance estimators (HC0--HC4), for models produced by \code{lm()} only,
is the \pkg{car} package \citep{hac:Fox+Weisberg:2011} in function
\code{hccm()}. Like \code{vcovHC()} from \pkg{sandwich} this is limited
to the cross-section case without clustering, though.

\subsection[R packages for clustered covariances]{\proglang{R} packages for clustered covariances}

The lack of support for clustered sandwich covariances in standard packages
like \pkg{sandwich} or \pkg{car} has led to a number of different
implementations scattered over various packages. Typically, these are tied
to either objects from \code{lm()} or dedicated model objects fitting certain
(generalized) linear models for clustered or panel data. The list of packages
includes: \pkg{multiwayvcov} \citep{hac:Graham+Arai+Hagstroemer:2016},
\pkg{plm} \citep{hac:Croissant+Millo:2008}, \pkg{geepack}
\citep{hac:Halekoh+Hojsgaard+Yan:2002}, \pkg{lfe} \citep{hac:Gaure:2016},
\pkg{clubSandwich} \citep{hac:Pustejovsky:2016}, and \pkg{clusterSEs}
\citep{hac:Esarey:2017}, among others.

In \pkg{multiwayvcov}, the implementation was object-oriented in many aspects
building on \pkg{sandwich} infrastructure. However, certain details assumed
\code{lm} or \code{glm}-like objects. In \pkg{plm} and \pkg{lfe} several
types of sandwich covariances are available for the packages' \code{plm}
(panel linear models) and \code{felm} (fixed-effect linear models), respectively.
The \pkg{geepack} package can estimate independence working models for
\code{glm}-type models, also supporting clustered covariances for the
resulting \code{geeglm} objects. Finally, \pkg{clusterSEs} and
\pkg{clubSandwich} focus on the case of ordinary or weighted least squares
regression models.

In a nutshell, there is good coverage of clustered covariances for (generalized)
linear regression objects albeit potentially necessitating reestimating a
certain model using a different model-fitting function/packages. However, there
was no object-oriented implementation for clustered covariances in \proglang{R},
that enabled plugging in different model objects from in principle any class.
Therefore, starting from the implementation in \pkg{multiwayvcov} a new and
object-oriented implementation was established and integrated in \pkg{sandwich},
allowing application to more general models, including zero-inflated, censored,
or limited responses. 

\section{Methods} \label{sec:methods}

To establish the theoretical background of sandwich covariances for
clustered as well as panel data the notation of \cite{hac:Zeileis:2006}
is adopted. Here, the conceptual building blocks from \cite{hac:Zeileis:2006}
are briefly repeated and then carried further for clustered covariances.

\subsection{Sandwich covariances}

Let $(y_{i},x_{i})$ for $i = 1, \ldots, n$ be data with some distribution
controlled by a parameter vector $\theta$ with $k$ dimensions. For a wide
range of models the (quasi-)maximum likelihood estimator $\hat \theta$ is
governed by a central limit theorem \citep{hac:White:1994} so that
$\hat \theta \approx \mathcal{N}(\theta, n^{-1} S(\theta))$. Moreover, the covariance
matrix is of sandwich type with a meat matrix $M(\theta)$ between two slices
of bread $B(\theta)$:
\begin{eqnarray} \label{eq:sandwich}
  S(\theta) & = & B(\theta) \cdot M(\theta) \cdot B(\theta) \\  \label{eq:bread}
  B(\theta) & = & \left( \E[ - \psi'(y, x, \theta) ] \right)^{-1} \\  \label{eq:meat}
  M(\theta) & = & \VAR[ \psi(y, x, \theta) ].
\end{eqnarray}
An estimating function
\begin{eqnarray}
  \psi(y, x, \theta) \quad = \quad \frac{\partial \Psi(y, x, \theta)}{\partial \theta}
\end{eqnarray}
is defined as the derivative of an objective function $\Psi(y, x, \theta)$, typically the log-likelihood, with respect to the parameter vector $\theta$.
Thus, an empirical estimating (or score) function evaluates an estimating function at the observed data and the estimated parameters
such that an $n \times k$ matrix is obtained \citep{hac:Zeileis:2006}:
\begin{eqnarray} \label{eq:estfun}
\left( \begin{array}{c} \psi(y_1, x_1, \hat \theta) \\ \vdots \\ \psi(y_n, x_n, \hat \theta)
           \end{array} \right).
\end{eqnarray}
The estimate for $\hat B$ is based on second derivatives, i.e., the empirical version of the inverse Hessian
\begin{equation} \label{eq:Bhat}
  \hat B \quad = \quad \left( \frac{1}{n} \sum_{i = 1}^n - \psi'(y_i, x_i, \hat \theta) \right)^{-1},
\end{equation}
whereas $\hat M, \hat M_\mathrm{HAC}, \hat M_\mathrm{HC}$ compute outer product, HAC and HC estimators for the meat, respectively, 
\begin{eqnarray} \label{eq:meat-op}
  \hat M & = & \frac{1}{n} \sum_{i = 1}^n\psi(y_i, x_i, \hat \theta) \psi(y_i, x_i, \hat \theta)^\top \\ \label{eq:meat-hac}
  \hat M_\mathrm{HAC} & = & \frac{1}{n} \sum_{i, j = 1}^n w_{|i-j|} \, \psi(y_i, x_i, \hat \theta) \psi(y_j, x_j, \hat \theta)^\top \\ \label{eq:meat-hc}
  \hat M_\mathrm{HC} & = & \frac{1}{n} X^\top \left( \begin{array}{ccc} 
  \omega(r(y_1, x_1^\top \theta)) & \cdots & 0 \\ 
  \vdots & \ddots & \vdots \\
  0 & \cdots & \omega(r(y_n, x_n^\top \theta))
  \end{array} \right) X.
\end{eqnarray}
The outer product estimator in Equation~\ref{eq:meat-op} corresponds to the basic sandwich estimator \citep{hac:Eicker:1963,hac:Huber:1967,hac:White:1980}.
$w_{|i-j|}$ in Equation~\ref{eq:meat-hac} is a vector of weights \citep{hac:Zeileis:2004a}.
In Equation~\ref{eq:meat-hc}, functions $\omega(\cdot)$ derive estimates of the variance of the empirical working residuals $r(y_1, x_1^\top \hat \theta), \ldots, r(y_n, x_n^\top \hat \theta)$ and may also depend on hat values as well as degrees of freedom \citep{hac:Zeileis:2006}.
The HC type of the model in Equation~\ref{eq:meat-hc} has to be of a form that allows factorization of the scores
\begin{equation} \label{eq:fact}
\psi(y_i, x_i, \hat\theta)  =  r(y_i, x_i^\top \hat \theta) \cdot x_i
\end{equation}  
into empirical working residuals times the regressor vector. This is, however, only possible in situations where the parameter
of the response distribution depends on a single linear predictor (possibly through a link function).

The building blocks for the calculation of the sandwich are provided by the
\pkg{sandwich} package, where the \code{sandwich()} function calculates an
estimator of the sandwich $S(\theta)$ (see Equation~\ref{eq:sandwich}) by
multiplying estimators for the meat (from Equation~\ref{eq:meat}) between
two slices of bread (from Equation~\ref{eq:bread}). A natural idea for an
object-oriented implementation of these estimators is to provide common
building blocks, namely a simple \code{bread()} extractor that computes
$\hat B$ from Equation~\ref{eq:Bhat} and an \code{estfun()} extractor that
returns the empirical estimating functions from Equation~\ref{eq:estfun}.
On top of these extractors a number of meat estimators can be defined:
\code{meat()} for $\hat M$ from Equation~\ref{eq:meat-op},
\code{meatHAC()} for $\hat M_\mathrm{HAC}$ from Equation~\ref{eq:meat-hac},
and \code{meatHC()} for $\hat M_\mathrm{HC}$ from Equation~\ref{eq:meat-hc},
respectively. In addition to the \code{estfun()} method a \code{model.matrix()}
method is needed in \code{meatHC()} for the decomposition of the scores into
empirical working residuals and regressor matrix.

\subsection{Clustered covariances} 

For clustered observations, similar ideas as above can be employed but the data
has more structure that needs to be incorporated into the meat estimators.
Specifically, for one-way clustering there is not simply an observation
$i$ from $1, \dots, n$ observations but an observation $(i,g)$ from
$1, \dots, n_g$ observations within cluster/group $g$ (with $g = 1, \dots, G$
and $n = n_1 + \dots + n_G$). As only the $G$ groups can be assumed to be
independent while there might be correlation withing the cluster/group,
the empirical estimation function is summed up within each group prior to
computing meat estimators. Thus, the core idea of many clustered covariances is
to replace Equation~\ref{eq:estfun} with the following equation and then
proceeding ``as usual'' in the computation of meat estimators afterwards:
\begin{eqnarray} \label{eq:estfun-cl}
\left( \begin{array}{c} \psi(y_{11}, x_{11}, \hat \theta) + \dots + \psi(y_{n_1 1}, x_{n_1 1}, \hat \theta)\\
           \vdots \\
	   \psi(y_{1G}, x_{1G}, \hat \theta) + \dots + \psi(y_{n_G G}, x_{n_G G}, \hat \theta)
           \end{array} \right).
\end{eqnarray}  
The basic meat estimator based on the outer product then becomes:
\begin{equation} \label{eq:meatCL}
  \hat M_\mathrm{CL} \quad = \quad \frac{1}{n} \sum_{g = 1}^G\sum_{i = 1}^{n_{g}}\psi(y_{ig}, x_{ig}, \hat \theta) \psi(y_{ig}, x_{ig}, \hat \theta)^\top.
\end{equation}
In the case where observation is its own cluster, the clustered $\hat M_\mathrm{CL}$
corresponds to the basic~$\hat M$.

The new function \code{meatCL()} in the \pkg{sandwich} package implements this
basic trick along with several types of bias correction and the possibility for
multi-way instead of one-way clustering.


\subsubsection{Types of bias correction}

The clustered covariance estimator controls for both heteroscedasticity across
as well as within clusters, but this comes at the cost that the number of
clusters $G$ must approach infinity, not just the number of observations $n$
\citep{hac:Cameron+Gelbach+Miller:2008,hac:Cameron+Miller:2015}. Although many
publications just refer to ``the'' clustered standard errors, there is a
surprisingly wide variation in clustered covariances, particularly due to
different flavors of bias corrections. The bias correction factor can be split
in two parts, a ``cluster bias correction'' and an ``HC bias correction''. The
cluster bias correction captures the effect of having just a finite number of
clusters $G$ and it is defined as 
\begin{equation} \label{eq:biasadj0}
 \frac{G}{G - 1}.
\end{equation} 
The HC bias correction can be applied additionally similar to the corresponding
cross-section data estimators. HC0 to HC3 bias corrections for cluster $g$ are
defined as
\begin{eqnarray} \label{eq:biasadj1}
  \mathrm{HC0:} &  & 1 \\  \label{eq:biasadj2}
  \mathrm{HC1:} &  & \frac{n}{n - k} \\  \label{eq:biasadj3}
  \mathrm{HC2:} &  & (I_{n_{g}} - H_{gg})^{-0.5} \\  \label{eq:biasadj4}
  \mathrm{HC3:} &  & (I_{n_{g}} - H_{gg})^{-1}, 
\end{eqnarray}  
where $n$ is the number of observations and $k$ is the number of estimated parameters,
$I_{n_{g}}$ is an identity matrix of size $n_{g}$, $H_{gg}$ is the block from the hat
matrix $H$ that pertains to cluster~$g$.

Thus, it is completely straightforward to add the factors for HC0 and HC1
to $\hat M_\mathrm{CL}$ (Equation~\ref{eq:meatCL}). However, but is more demanding
to apply the cluster generalizations of HC2 and HC3
\citep[due to][]{hac:Kauermann+Carroll:2001,hac:Bell+McCaffrey:2002} for which
the correction factors from Equations~\ref{eq:biasadj3} and~\ref{eq:biasadj4} have
to be applied to the (working) residuals prior to computing the clustered meat matrix.
More precisely, the empirical working residuals $r(y_{g}, x_{g}^\top \hat \theta)$
in group $g$ are adjusted via
\begin{equation} \label{eq:fact-adj}
  \tilde r(y_{g}, x_{g}^\top \hat \theta) = (I_{n_{g}} - H_{gg})^\alpha \cdot r(y_{g}, x_{g}^\top \hat \theta)
\end{equation}  
with $\alpha = -0.5$ for HC2 and $\alpha = -1$ for HC3, before obtaining the adjusted
empirical estimating functions based on Equation~\ref{eq:fact} as
\begin{equation} \label{eq:fact-reg}
\tilde \psi(y_i, x_i, \hat\theta) = \tilde r(y_{i}, x_{i}^\top \hat \theta) \cdot x_i.
\end{equation}  
Then these adjusted estimating functions can be employed ``as usual'' to obtain the
$\hat M_\mathrm{CL}$.

Note that in terms of methods in \proglang{R}, it is not sufficient to have just \code{estfun()}
and\linebreak
\code{model.matrix()} extractors but an extractor for (blocks of) the full hat matrix
are required as well. Currently, no such extractor method is available in base \proglang{R}
(as \code{hatvalues()} just extracts $\mathrm{diag} H$) and hence HC2 and HC3 in \code{meatCL()}
are just available for \code{lm} and \code{glm} objects.


\subsubsection{Two-way and multi-way clustered covariances}
Certainly, there can be more than one cluster dimension, as for example observations that are characterized by housholds within states or companies within industries. It can therefore sometimes be helpful that one-way clustered covariances can be extended to so-called multi-way clustering as shown by \cite{hac:Miglioretti+Heagerty:2007}, \cite{hac:Thompson:2011} and \cite{hac:Cameron+Gelbach+Miller:2011}.

Multi-way clustered covariances comprise clustering on $2^{D} - 1$ dimensional combinations.
Clustering in two dimensions, for example in $id$ and $time$, gives $D = 2$, such that the clustered covariance matrix is composed of $2^2 - 1 = 3$ one-way clustered covariance matrices that have to be added up or substracted off, respectively.
For two-way clustered covariances with cluster dimensions $id$ and $time$, the one-way clustered covariance matrices on $id$ and on $time$ are added up, and the two-way clustered covariance matrix with clusters formed by the intersection of $id$ and $time$ is substacted off
%\fixme{in the equation below we would need M not S, right? For consistency, the subscript should follow the matrix name directly -- %rather than after the brackets.}
\begin{equation} \label{eq:twoway}
\hat M_{\mathrm{CL}(id,time)} \quad = \quad \hat M_{\mathrm{CL}(id)} + \hat M_{\mathrm{CL}(time)} - \hat M_{\mathrm{CL}(id \cap time)}.
\end{equation}
The same idea is used for obtaining clustered covariances with more than two clustering dimensions:
Meat parts with an odd number of cluster dimensions are added up, whereas those with an even number
are substracted. 

\cite{hac:Petersen:2009}, \cite{hac:Thompson:2011} and \cite{hac:Ma:2014}
suggest to substract the standard sandwich estimator  in case that the clusters
formed by the intersection of $id$ and $time$ do only contain a single
observation.
% \footnote{If one wants to cluster by dimensions \emph{id} and \emph{year} in a sample
% of \emph{id-month} observations, each cluster is composed of 12 observations \citep{hac:Ma:2014}.
% In such a setting, the last substracted matrix should be a one-way clustered covariance matrix
% with clusters formed by the intersections of \emph{id} and \emph{year}.}.
As \cite{hac:Ma:2014} argues, the standard sandwich estimator does not take into
account any bias adjustment, such that the cluster bias correction
$\frac{G}{G-1}$ is larger than one. Thus, covariances are underestimated if
always the clustered covariance matrix with cluster bias correction
$\frac{G}{G-1}$ is substracted as the last substracted matrix instead of
standard sandwich covariances.

% Various flavours of clustered covariances have been newly implemented in the
% \pkg{sandwich} package, relying on the same building blocks as in previous
% package versions. Again, clustered covariances rely on the same structure as
% seen in Equations~\ref{eq:sandwich}--\ref{eq:meat}. But instead of calling one
% out of the already implemented meat functions in
% Equations~\ref{eq:meat-op}--\ref{eq:meat-hc}, \code{meatCL()} is an
% implementation of the meat as in Equation~\ref{eq:meatCL} and is called via the
% function \code{vcovCL()} to calculate clustered covariances. The meat in
% Equation~\ref{eq:meatCL} is conceptually the same as in
% Equation~\ref{eq:meat-op}, but with the difference that the score is aggregated
% at the cluster level as in Equation~\ref{eq:estfun-cl} instead of the
% unaggregated scores in Equation~\ref{eq:estfun}. More details about both
% functions \code{meatCL()} and \code{vcovCL()} are given in the next section.

\subsection{Clustered covariances for panel data}

The information of panel data sets is often overstated, as cross-sectional as well as temporal dependencies may occur \citep{hac:Hoechle:2007}.
\citet[p.~702]{hac:Cameron+Trivedi:2005} noticed that ``$NT$ correlated observations have less information than $NT$ independent observations''. 
For panel data, the source of dependence in the data is crucial to find out what kind of covariance is optimal \citep{hac:Petersen:2009}.
In the following, panel Newey-West standard errors as well as Driscoll and Kraay standard errors are examined
\citep[see also][for a unifying view]{hac:Millo:2014}.

To reflect that the data are now panel data with time ordering within each cluster/group/id we change our notation
to an index $(i, t)$ for $i = 1, \dots, n_t$ observations at time $t = 1, \dots, T$ (with $n = n_1 + \dots + n_T$).
Note that compared to the notation from the clustered case above,
the variable $i$ now denotes the group/id (e.g., firm) which was previously denoted by $g$.

\subsubsection{Panel Newey-West}
\cite{hac:Newey+West:1987} proposed a heteroscedasticity and autocorrelation consistent standard error estimator that is traditionally used for time-series data, but can be modified for use in panel data \citep[see for example][]{hac:Petersen:2009}.
A panel Newey-West estimator can be obtained by setting the cross-sectional as well as the cross-serial correlation to zero \citep{hac:Millo:2014}. The meat is composed of
%\fixme{Can the new Omega symbol be avoided? Ideally by using the same notation as in Equation~7. Also, we don't %really need the lag L explicitly.
%This can be put implicitly into the weights, just as in Equation~7.
%Finally, for notational consistency, if a comma in the subscript is needed here, it should also be used above in the %clustered case.}
\begin{equation} \label{eq:newey}
  \hat M_\mathrm{PL}^{NW} \quad = \quad \frac{1}{n} \sum_{i, j = 1}^n w_{|i-j|} \, \psi(y_{i}, x_{i}, \hat \theta) \psi(y_{j}, x_{j}, \hat \theta)^\top.
\end{equation}
\cite{hac:Newey+West:1987} employ a Bartlett kernel for obtaining the weights as  
$w_{|i-j|} = 1 - \frac{|i-j|}{L + 1}$ at lag $\ell = |i-j|$ up to lag $L$.
As \cite{hac:Petersen:2009} noticed, the maximal lag length $L$ in a panel data set is $n_t - 1$,
i.e., the maximum number of \emph{time} periods per \emph{id} minus one.

\subsubsection{Driscoll and Kraay}

\cite{hac:Driscoll+Kraay:1998} have adapted the Newey-West approach by using
the aggregated estimating functions at each time point. This can be shown to be
robust to spatial and temporal dependence of general form, but with the caveat
that a long enough time dimension must be available.

Thus, the idea is again
to replace Equation~\ref{eq:estfun} by Equation~\ref{eq:estfun-cl} before computing
$\hat M_\mathrm{HAC}$ from Equation~\ref{eq:meat-hac}. Note, however, that the aggregation
is now done across cluster/id within each time period $t$. This yields a panel
sandwich estimator where the meat is computed as
\begin{equation} \label{eq:driscoll}
  \hat M_\mathrm{PL} \quad = \quad \frac{1}{n} \sum_{t = 1}^T \sum_{i, j = 1}^{n_t} w_{|i-j|} \,
    \psi(y_{it}, x_{it}, \hat \theta) \psi(y_{jt}, x_{jt}, \hat \theta)^\top,
\end{equation}
The weights $w_{|i-j|}$ are usually again the Bartlett weights up to lag $L$.
Note that for $L = 0$, $\hat M_\mathrm{PL}$ reduced to $\hat M_{\mathrm{CL}(\mathit{time})}$,
i.e., the one-way covariance clustered by time. Also, for the special case that there is just
one observation at each time point $t$, this panel covariance by \cite{hac:Driscoll+Kraay:1998}
simply yields the panel Newey-West covariance.

The new function \code{meatPL()} in the \pkg{sandwich} package implements this
approach analogously to \code{meatCL()}. For the computation of the weights $w_\ell$
the same function is employed that \code{meatHAC()} uses.


\subsection{Panel-corrected standard errors}

\cite{hac:Beck+Katz:1995} proposed another form or panel-corrected covariances -- typically
referred to as panel-corrected standard errors (PCSE). They are intended for panel data
(also called time-series-cross-section data in this literature) with moderate dimensions of
time and cross-section \citep{hac:Millo:2014}. They are robust against panel heteroscedasticity
and contemporaneously correlation, with the crucial assumption that contemporaneous correlation
accross cluster follows a fixed pattern \citep{hac:Millo:2014, hac:Johnson:2004}. Autocorrelation
within a cluster is assumed to be absent.

\cite{hac:Hoechle:2007} argues that for the PCSE estimator the finite sample properties are rather
poor if the cross-sectional dimension is large compared to the time dimension. This is in contrast
to the panel covariance by \cite{hac:Driscoll+Kraay:1998} which relies on large-$t$ asymptotics and
is robust to quite general forms of cross-sectional and temporal dependence and is consistent
independently of the cross-sectional dimension.

To emphasize that now both cross section \emph{and} and time ordering are considered,
index $(t,g)$ is employed for the observation from cluster/group $g = 1, \dots, G$ at time
$t = 1, \dots, n_g$. In the balanced case (that we focus on below) $n_g = T$ for all groups~$g$
so that there are $n = G \cdot T$ observations overall.

The basic idea for PCSE is to employ the outer product of (working) residuals within each cluster~$g$.
Thus, the working residuals are split into vectors for each cluster $g$:
$r(y_1, x_1^\top \hat \theta), \dots, r(y_G, x_G^\top \hat \theta)$.
For balanced data these can be arranged in a $T \times G$ matrix,
 \begin{equation} \label{eq:workres}
  R \quad = \quad [r(y_1, x_1^\top \hat \theta) \quad r(y_2, x_2^\top \hat \theta) \quad \ldots \quad r(y_G, x_G^\top \hat \theta)],
 \end{equation}
and the meat of the panel-corrected covariance matrix can be computed using the Kronecker product as
\begin{equation} \label{eq:pcse}
 \hat M_\mathrm{PC} \quad = \quad \frac{1}{n} X^\top \bigg[ \frac{(R^\top R)}{T} \otimes \vI_T \bigg] X.
\end{equation}
The details for the unbalanced case are omitted here for brevity but are discussed in detail in
\cite{hac:Bailey+Katz:2011}.

The new function \code{meatPC()} in the \pkg{sandwich} package implements both the balanced and
unbalanced case. As for \code{meatHC()} it is necessary to have a \code{model.matrix()} extractor
in addition to the \code{estfun()} extractor for splitting up the empirical estimating functions
into residuals and regressor matrix.


\section{Software} \label{sec:software}

As conveyed already in Section~\ref{sec:methods}, the \pkg{sandwich} package has been extended
along the same lines it was originally established in \citep{hac:Zeileis:2006}. The new clustered
and panel covariances require a new \code{meat*()} function that ideally only extracts the
\code{estfun()} from a fitted model object. For the full sandwich covariance an accompanying
\code{vcov*()} function is provided that couples the \code{meat*()} with the \code{bread()} estimate
extracted from the model object.

The new sandwich covariances \code{vcovCL()} for clustered data and \code{vcovPL()} for panel data,
as well as \code{vcovPC()} for panel-corrected covariances all follow this structure and are
introduced in more detail below.

Model classes which provide the necessary building blocks include \code{betareg},
\code{clm}, \code{coxph}, \code{crch}, \code{glm}, \code{hurdle}, \code{lm}, \code{mlm}, \code{mlogit},
\code{nls}, \code{polr}, \code{rlm}, \code{survreg}, or \code{zeroinfl} from packages 
\pkg{stats} \citep{hac:R:2017}, 
\pkg{betareg} \citep{hac:Cribari-Neto+Zeileis:2010,hac:Gruen+Kosmidis+Zeileis:2012},
\pkg{crch} \citep{hac:Messner+Mayr+Zeileis:2016},
\pkg{MASS} \citep{hac:Venables+Ripley:2002},
\pkg{mlogit} \citep{hac:Croissant:2013}, 
\pkg{ordinal} \citep{hac:Christensen:2015}, and
\pkg{survival} \citep{hac:Therneau:2015}.
For all of these an \code{estfun} method is available along with a \code{bread()} method
(or the default method works). In case the models are based on a single linear predictor only,
they also provide \code{model.matrix()} extractors so that the factorization from Equation~\ref{eq:fact}
into working residuals and regressor matrix can be easily computed.


\subsection{Clustered covariances}

One-, two-, and multi-way clustered covariances with HC0--HC3 bias correction
are implemented in
\begin{verbatim}
  vcovCL(x, cluster = NULL, type = NULL, sandwich = TRUE, fix = FALSE, ...)
\end{verbatim}
for a fitted-model-object \code{x} with the underlying meat estimator in
\begin{verbatim}
  meatCL(x, cluster = NULL, type = NULL, cadjust = TRUE, multi0 = FALSE, ...)
\end{verbatim}
The essential idea is to aggregate the empirical estimating functions within
each cluster and then compute a HC covariance analogous to \code{vcovHC()}.

The \code{cluster} argument allows to supply either one cluster vector or
a list (or data frame) of several cluster variables. If
no cluster variable is supplied, each observation is its own cluster per
default. Thus, by default, the clustered covariance estimator collapses to
the basic sandwich estimator.

The bias correction is composed of two parts that can be switched on and off
separately: First, the cluster bias correction from Equation~\ref{eq:biasadj0}
is controlled by \code{cadjust}. Second, the HC bias correction from
Equations~\ref{eq:biasadj1}--\ref{eq:biasadj4} is specified via \code{type}
with the default to use \code{"HC1"} for \code{lm} objects and
\code{"HC0"} otherwise. Moreover, \code{type = "HC2"} and \code{"HC3"}
are only available for \code{lm} and \code{glm} objects as they require
computation of full blocks of the hat matrix (rather than just the diagonal
elements as in \code{hatvalues()}). Hence, the hat matrices of (generalized)
linear models are provided directly in \code{meatCL()} and are not object-oriented
in the current implementation.

The \code{multi0} argument is relevant only for multi-way clustered
covariances with more than one clustering dimension. It specifies whether to
substract the basic cross-section HC0 covariance matrix as the last substracted
matrix in Equation~\ref{eq:twoway} instead of the covariance matrix formed by
the intersection of groups \citep{hac:Petersen:2009,hac:Thompson:2011,hac:Ma:2014}.

For consistency with \cite{hac:Zeileis:2004a}, the \code{sandwich} argument
specifies whether the full sandwich estimator is computed (default) or only
the meat.

Finally, the \code{fix} argument specifies whether the covariance matrix should
be fixed to be positive semi-definite in case it is not. This is achieved by
converting any negative eigenvalues from the eigendecomposition to zero.
\cite{hac:Cameron+Gelbach+Miller:2011} observe that this is most likely to
be necessary in applications with fixed effects, especially when clustering
is done over the same groups as the fixed effects.



\subsection{Clustered covariances for panel data}

For panel data, 
\begin{verbatim}
  vcovPL(x, cluster = NULL, order.by = NULL, kernel = "Bartlett",
   sandwich = TRUE, fix = FALSE, ...)
\end{verbatim}
based on
\begin{verbatim}
  meatPL(x, cluster = NULL, order.by = NULL, kernel = "Bartlett",
   lag = "NW1987", bw = NULL, adjust = TRUE, ...)
\end{verbatim}
computes sandwich covariances for panel data, specificially including
panel \citep{hac:Newey+West:1987} and \citep{hac:Driscoll+Kraay:1998}.
The essential idea is to aggregate the empirical estimating functions within
each time period and and then compute a HAC covariance analogous to \code{vcovHAC()}.

Again, \code{vcovPL()} returns the full sandwich if the argument
\code{sandwich = TRUE}, and \code{fix = TRUE} forces a positive semi-definite
result if necessary.

The \code{cluster} argument allows to specify a variable indicating the cluster/group/id variable
while \code{order.by} specifies the time variable. If only one of the two variables is
provided, then it is assumed that observations are ordered within the other variable.
And if neither is provided, only one cluster is used for all observations resulting
in the standard \citep{hac:Newey+West:1987} estimator. Finally, \code{cluster} can
also be a list with both variables: the cluster/group/id and the time/ordering variable,
respectively.

The weights in the panel sandwich covariance are set up by means of a \code{kernel} function
along with a bandwidth \code{bw} or the corresponding \code{lag}. All kernels described
in \cite{hac:Andrews:1991} and implemented in \code{vcovHAC()} by \cite{hac:Zeileis:2004}
are available, namely truncated, Bartlett, Parzen, Tykey-Hanning, and quadratic spectral. 
For the default case of the Bartlett kernel, the bandwidth \code{bw} corresponds to \code{lag + 1}
and only of the two arguments should be specified. The \code{lag} argument can either be an
integer or one of three character specifications: \code{"max"}, \code{"NW1987"}, or \code{"NW1994"}).
\code{"max"} (or equivalently, \code{"P2009"} for \citealp{hac:Petersen:2009}) indicates the maximum lag length $T - 1$, i.e., the number of time periods minus one.
\code{"NW1987"} corresponds to \cite{hac:Newey+West:1987}, who have shown that their estimator is consistent
if the number of lags increases with time periods $T$, but with speed less than $T^{1/4}$ \citep[see also][]{hac:Hoechle:2007}.
\code{"NW1994"} sets the lag length to $\mathrm{floor}[4 \cdot (\frac{T}{100})^{2/9}]$ \citep{hac:Newey+West:1994}.

The \code{adjust} argument allows to make a finite sample adjustment, which
amounts to multiplication with $n/(n - k)$, where $n$ is the number of
observations, and $k$ is the number of estimated parameters.


\subsection{Panel-corrected covariance}

Panel-corrected covariances and panel-corrected standard errors (PCSE) a la \cite{hac:Beck+Katz:1995} are implemented in
\begin{verbatim}
  vcovPC(x, cluster = NULL, order.by = NULL, subsample = FALSE,
   sandwich = TRUE, fix = FALSE, ...)
\end{verbatim}
based on
\begin{verbatim}
  meatPC(x,  cluster = NULL, order.by = NULL, subsample = FALSE,
   kronecker = FALSE, ...)
\end{verbatim}
They are usually used for panel data or time-series-cross-section (TSCS) data with a large enough time dimension. 
The arguments \code{sandwich}, \code{fix}, \code{cluster}, and \code{order.by}
have the same meaning as in \code{vcovCL()} and \code{vcovPL()}.

While estimation in balanced panels is straightforward, there are two alternatives to estimate
the meat for unbalanced panels \citep{hac:Bailey+Katz:2011}. For \code{subsample
= TRUE}, a balanced subset of the panel is employed, whereas for \code{subsample
= FALSE}, a pairwise balanced sample is used. For details, see
\cite{hac:Bailey+Katz:2011}.

The argument \code{kronecker} relates to estimation of the meat and determines
whether calculations are executed with the Kronecker product or elementwise.
The former is typically computationally faster in moderately large data sets
while the latter is less memory-intensive so that it can be applied to larger
numbers of observations.



\section{Illustrations} \label{sec:illu}

The main motivation for the new object-oriented implementation of clustered covariances
in \pkg{sandwich} was the applicability to models beyond \code{lm()} or \code{glm()}.
Specifically when working on \cite{hac:Berger+Stocker+Zeileis:2017} -- an extended
replication of \cite{hac:Aghion+VanReenen+Zingales:2013} -- clustered covariances
for negative binomial hurdle models were provided. In Section~\ref{ex-aghion} it is
illustrated how these can now be easily obtained.

To replicate further classic results, the benchmark data from \cite{hac:Petersen:2009}
is considered in Section~\ref{ex-petersen}. This focuses on the classical linear
regression case with model errors that are correlated within clusters. It is shown
how the results from various other \proglang{R} packages (\pkg{multiwayvcov},
\pkg{plm}, \pkg{geepack}, \pkg{pcse}) can be replicated using the new \pkg{sandwich}
package. One- and two-way clustered standard errors from \code{vcovCL()} from \pkg{sandwich} are compared to those obtained from \code{cluster.vcov()} from \pkg{multiwayvcov}. Furthermore, one-way clustered standard errors can also be obtained from \pkg{plm} and \pkg{geepack}, and are additionally benchmarked against the results from \pkg{sandwich}. Function \code{vcovSCC()} from \pkg{plm} gives Driscoll and Kraay type standard errors, its results are compared to the outcome of \code{vcovPL()} from \pkg{sandwich}. Also, panel-corrected standard errors can be estimated by function \code{vcovPC()} from \pkg{pcse} and are benchmarked against \code{vcovPC()} from \pkg{sandwich}.
 
\subsection[Aghion et al. (2013) and Berger et al. (2017)]{\cite{hac:Aghion+VanReenen+Zingales:2013} and \cite{hac:Berger+Stocker+Zeileis:2017}} \label{ex-aghion}
In this section, in a further example we will make use of the object-orientation of \code{vcovCL()}, and estimate clustered standard errors for a count data hurdle model.

\cite{hac:Aghion+VanReenen+Zingales:2013} investigate the effect of institutional owners (these are, for example, pension funds, insurance companies, etc.) on innovation.
The authors use firm-level panel data on innovation and institutional ownership from 1991 to 1999 over 803 firms, with the data clustered at company as well as industry level. 
To capture the differing value of patents, citation-weighted patent counts are used as a proxy for innovation, whereby the authors weight the patents by the number of future citations.
This motivates the use of count data models.

\cite{hac:Aghion+VanReenen+Zingales:2013} mostly employ Poisson and negative binomial models in a quasi-maximum likelihood approach and cluster standard errors by either companies or industries.
Still, one limitation of standard count data models is that the zeros and the nonzeros (positives) are assumed to come from the same data-generating process.

From an economic perspective, there is a difference in determinants of ``first innovation'' and ``continuing innovation''. The rationale behind this is the notion of nonlinearities in the innovation process.
In case that the first innovation is especially hard to obtain in comparison to succeeding innovations, hurdle models offer a useful way that allows for a distinction to be made between these two processes \citep{hac:Berger+Stocker+Zeileis:2017}.
Therefore, \cite{hac:Berger+Stocker+Zeileis:2017} employ two-part hurdle models with a binary part that models the decision to innovate at all, and a count part that models ongoing innovation, respectively.
The \cite{hac:Aghion+VanReenen+Zingales:2013} data are available in the \pkg{sandwich} package.
<<innovation-data>>= 
data("InstInnovation", package = "sandwich")
@ 
Hurdle models are fitted with the \code{hurdle} function from the \pkg{pscl} package \citep{hac:Zeileis+Kleiber+Jackman:2008}. Here, the count model family chosen is a negative binomial, and the zero hurdle model family is a binomial with logit link.
<<innovation-model>>=
library("pscl")
h.innov <- hurdle(
  cites ~ institutions + log(capital/employment) + log(sales),
  data = InstInnovation, dist = "negbin")
@
Below, a comparison of ``standard'' standard errors, basic sandwich standard errors and clustered standard errors for an exemplary hurdle model is shown. Standard errors are clustered by companies, with a total of 803 clusters.
<<innovation-se>>= 
library("sandwich")
vc <- list(
  "standard" = vcov(h.innov),
  "basic" = sandwich(h.innov),
  "CL-1" = vcovCL(h.innov, cluster = InstInnovation$company)
)
sapply(vc, function(x) sqrt(diag(x)))
@
What can be observed is that when the data are clustered, basic standard errors can greatly overstate estimator precision.
Then, for the exemplary hurdle model, clustered standard errors are scaled up by factors between $1.48$ and $1.86$ even compared to standard sandwich standard errors.



\subsection[Petersen (2009)]{\cite{hac:Petersen:2009}} \label{ex-petersen}
Benchmark data for testing the clustered standard error estimates in the linear model is a simulated data set\footnote{\url{http://www.kellogg.northwestern.edu/faculty/petersen/htm/papers/se/test_data.txt}} provided by \cite{hac:Petersen:2009}, containing 500 firms over 10 years. The data include 4 variables, \code{firm}, \code{year}, dependent variable \code{y} and explanatory variable \code{x} \citep[see also][]{hac:Graham+Arai+Hagstroemer:2016}.

The data are available in \pkg{sandwich} as well as in \pkg{multiwayvcov}.
<<petersen-data>>= 
data("PetersenCL", package = "sandwich")
@ 
A linear model is fitted with \code{lm()}, 
<<petersen-model>>=
p_lm <- lm(y ~ x, data = PetersenCL)
@ 
and testing the estimated coefficients with \code{coeftest()} from package \pkg{lmtest} gives
<<petersen-coeftest>>=
library("lmtest")
coeftest(p_lm)
@ 
However, the data are clustered by firms (\code{firm}), with a total of 500 clusters. But ignoring dependency by using ``standard'' standard errors results in an overstate of estimator precision.

\subsubsection{One-way clustered standard errors}
Thus, one-way clustered standard errors clustered by \code{firm} are employed.
It can be observed that clustered standard errors are larger than ``standard'' standard errors.
<<petersen-onecl-firm>>=
library("sandwich")
coeftest(p_lm, vcov = vcovCL(p_lm, cluster = PetersenCL$firm))
@ 
Next, it is shown how the results from various other \proglang{R} packages (\pkg{multiwayvcov},
\pkg{plm}, \pkg{geepack}, \pkg{pcse}) can be replicated using the new \pkg{sandwich} package.

First, \code{vcovCL()} from \pkg{sandwich} and \code{cluster.vcov()} from \pkg{multiwayvcov} are compared. 
<<petersen-packages1>>=
library("multiwayvcov")
@ 
Both functions are closely related, as \code{vcovCL()} is partly based on \code{cluster.vcov()}.
<<petersen-comparison1>>=
vc <- list(
"sandwich" = vcovCL(p_lm, cluster = PetersenCL$firm),
"multiwayvcov" = cluster.vcov(p_lm, cluster = PetersenCL$firm)
)
sapply(vc, function(x) sqrt(diag(x)))
@ 
The bias correction in \code{vcovCL()} is set to \code{cadjust = TRUE} and \code{type = "HC1"} by default. The same type of bias correction is achieved for \code{cluster.vcov()} by default.

Second, \code{vcovCL()} is compared to packages \pkg{plm} and \pkg{geepack}, who are also able to calculate clustered covariances.
<<petersen-packages2>>=
library("plm")
library("geepack")
@ 
A pooling model is estimated with \code{plm()}, \code{geeglm()} fits a generalized estimating equation (GEE) with independence correlation structure.
<<petersen-models>>=
p_plm <- plm(y ~ x, data = PetersenCL, model = "pooling",
 indexes = c("firmid", "year"))
p_gee <- geeglm(y ~ x, data = PetersenCL, id = PetersenCL$firm,
 corstr = "independence", family = gaussian)
@ 
As there is no \code{vcov()}-method specified for \code{geeglm()}, the convenience function \code{vcov.geeglm()} is provided to make this contribution.
<<petersen-convenience>>=
vcov.geeglm <- function(object) {
vc <- object$geese$vbeta
rownames(vc) <- colnames(vc) <- names(coef(object))
return(vc)
}
@ 
Nevertheless, in order to accomplish exactly the same value for clustered standard errors using \code{plm} and \code{geepack}, the bias correction $\frac{G}{G - 1}\frac{n - 1}{n - k}$ has to be omitted from the clustered covariance matrix estimated by \code{vcovCL()}. This is achieved by setting the function arguments \code{cadjust = FALSE} and \code{type = "HC0"}.
<<petersen-comparison2>>=
vc <- list(
"sandwich" = vcovCL(p_lm, cluster = PetersenCL$firm,
 cadjust = FALSE, type = "HC0"),
"plm" = vcovHC(p_plm, type = "HC0", cluster = "group"),
"geepack" = vcov(p_gee)
)
sapply(vc, function(x) sqrt(diag(x)))
@
All packages examined produce the same clustered standard errors for \code{lm} model objects. For packages \pkg{plm} and \pkg{geepack}, one has to omit the HC bias correction factor as well as the cluster bias correction. In general, differences in clustered standard errors often coincide with different types of bias corrections.

\subsubsection{Two-way clustered standard errors}
Two-way clustered standard errors with cluster dimensions \code{firm} as well as \code{year} are, at least for the explanatory variable \code{x}, a bit larger than one-way clustered standard errors clustered by \code{firm}. 

It can be observed that \code{vcovCL()} from \pkg{sandwich} and \code{cluster.vcov()} from \pkg{multiwayvcov} deliver equivalent results.
<<petersen-twocl>>=
cluster <- cbind(PetersenCL$firm, PetersenCL$year)
vc <- list(
"sandwich" = vcovCL(p_lm, cluster = cluster),
"multiwayvcov" = cluster.vcov(p_lm, cluster = cluster)
)
sapply(vc, function(x) sqrt(diag(x)))
@ 
@ 
However, as cluster dimension \code{year} has a total of only 10 cluster, the results should be regarded with caution, as it is required by theory that each cluster dimension has many clusters \citep{hac:Petersen:2009,hac:Cameron+Gelbach+Miller:2011,hac:Cameron+Miller:2015}.

\subsubsection{Driscoll and Kraay standard errors}
For Driscoll and Kraay standard errors, \code{vcovPL()} and \code{vcovSCC} from \pkg{plm} deliver equivalent results, given the same lag length and bias-correction. Here, the maximum lag length is chosen (which is equal to \code{length(PetersenCL$year) - 1}) with a \code{HC1} bias correction.
<<petersen-comparison3>>=
vc <- list(
"sandwich" = vcovPL(p_lm, cluster = PetersenCL$firm,
 cadjust = TRUE, lag = "max"),
"plm" = plm::vcovSCC(p_plm, maxlag = 9, inner = "cluster",
 type = "HC1")
)
sapply(vc, function(x) sqrt(diag(x)))
@
\subsubsection{Panel-corrected standard errors}
Panel-corrected standard errors can as well be calculated from \pkg{sandwich} in function \code{vcovPC()}, which gives the same results as \code{vcovPC} from \pkg{pcse}.
<<petersen-comparison4>>=
library("pcse")
vc <- list(
"sandwich" = sandwich::vcovPC(p_lm, cluster = PetersenCL$firm,
 order.by = PetersenCL$year),
"pcse" = pcse::vcovPC(p_lm, groupN = PetersenCL$firm,
 groupT = PetersenCL$year)
)
sapply(vc, function(x) sqrt(diag(x)))
@

\section{Simulation} \label{sec:simulation}

Next, we run a Monte Carlo simulation to assess the methods' performance in a simulation study.
The aim is to test clustered standard errors beyond linear and generalized linear models. For the linear model, there are a couple of simulation studies in the literature \citep{hac:Cameron+Gelbach+Miller:2008,hac:Arceneaux+Nickerson:2009,hac:Petersen:2009,hac:Cameron+Gelbach+Miller:2011,hac:Harden:2011,hac:Thompson:2011,hac:Cameron+Miller:2015,hac:Jin:2015}, far less for generalized linear models \citep[see for example][]{hac:Miglioretti+Heagerty:2007} and, up to our knowledge, none for models beyond \code{lm()} and \code{glm()}.

\subsection{Simulation design}
The simulations are each composed of $10,000$ replications.
We systematically vary parameters $\rho$ and $G$. $\rho$ determines the strength of cluster correlation and varies from $0$ to $0.9$. The number of clusters $G$ ranges from $10$ to $50, 100, 150, 200$ to $250$. Numerous studies \citep{hac:Green+Vavreck:2008,hac:Arceneaux+Nickerson:2009,hac:Harden:2011} confirmed that the higher the number of clusters, the lower the bias of the standard errors (in linear regression models). Furthermore, we only have a look at balanced cluster, observations per cluster are fixed to $5$.

\subsubsection{Linear predictor}
The linear predictor is
\begin{eqnarray} \label{eq:predictor}
h(\mu_{ig}) = \beta_{0} + \beta_{1} \cdot x_{1,ig} + \beta_{2} \cdot x_{2,g} + \beta_{3} \cdot x_{3,ig}
\end{eqnarray}
with a link function $h$ and expected value $\mu_{ig}$. 
In order to generate within cluster error correlation, there are two possible options. The first option is to take the marginal model Equation~\ref{eq:predictor} and introduce correlation via a copula. The second option is to add a random effect in Equation~\ref{eq:predictor}. 

It has to be mentioned that for the linear model with an identity link function $h(\cdot)$, both options amount to the same thing. Though for models other than \code{lm()}, the two options are different. In the simulation exercises, the copula option is favoured, because for models other than \code{lm()}, including a random effect simultaneously includes a bias, too.

We analyze three regressor variables
\begin{eqnarray} \label{eq:regressors}
x_{1,ig} & \sim & \rho_{x} \cdot \mathcal{N}_{g}(0, 1) + (1 - \rho_{x}) \cdot \mathcal{N}_{ig}(0, 1) \label{x1} \\ 
x_{2,g} & \sim & \mathcal{N}_{g}(0, 1) \label{x2} \\
x_{3,ig} & \sim & \mathcal{N}_{ig}(0, 1) \label{x3}
\end{eqnarray}
Regressor $x_{1,ig}$ in Equation~\ref{x1} is composed of a linear combination of a random draw at cluster level ($\mathcal{N}_{g}$) and a random draw at individual level ($\mathcal{N}_{ig}$), regressor $x_{2,g}$ in Equation~\ref{x2} is composed of a random draw at cluster level, and regressor $x_{3,ig}$ in Equation~\ref{x3} consists of a random draw at individual level. In most of the simulations, only a single regressor $x_{1,ig}$ is used.

In line with \cite{hac:Harden:2011}, this is done to produce variation at cluster level, individual level and at a combination of both levels. The cluster correlation of $x_{1,ig}$ is controlled by parameter $\rho_{x}$, and is set to 0.25 per default. This implies at least some within cluster correlation of regressor $x_{1,ig}$. If $\rho_{x} = 1$, regressor $x_{1,ig}$ is equivalent to regressor $x_{2,g}$. Furthermore, if $\rho_{x} = 0$, regressor $x_{1,ig}$ corresponds to regressor $x_{3,ig}$.

The vector of coefficients is fixed to
\begin{eqnarray} \label{eq:coefs}
\beta_{1} & = & (0, 0.85, 0.5, 0.7)^\top \label{beta1} \\ 
\beta_{2} & = & (0, 0.85, 0, 0)^\top \label{beta2}
\end{eqnarray}
even though these values can be interchanged without influencing the results\footnote{Values are equivalent to \cite{hac:Harden:2011}.}.

Response distributions examined include Gaussian (\code{gaussian}), binomial with a logit link (\code{binomial(logit)}), Poisson (\code{poisson}), zero-truncated Poisson (\code{zerotrunc}), Beta (\code{beta}) and zero-inflated Poisson (\code{zip}). \code{vcovCL()} allows estimation of clustered standard errors for all the abovementioned responses (and many more).

\subsubsection{Sandwich covariances}
Covariances being compared to each other include ``standard''  standard covariances without heteroscedasticity and without autocorrelation (\code{standard}), basic sandwich standard errors (\code{basic}), Driscoll and Kraay standard errors (\code{PL}), panel-corrected standard errors (\code{PC}) a la \cite{hac:Beck+Katz:1995} and clustered standard errors with HC0 to HC3 adjustment (\code{CL-0}, \code{CL-1}, \code{CL-2}, \code{CL-3}). In addition, standard errors from a random effects model (\code{random}) and from a GEE with exchangeable correlation structure (\code{gee}) are compared.

\subsubsection{Outcome measure}
In order to assess the validity of statistical inference, the coverage rate is the outcome measure of interest.
If standard errors are estimated accurately, the coverage rate of the $95$\% confidence interval should be close to 0.95. 
Values less than 0.925 will be considered to have underestimated standard errors towards a Type I error, while values greater than 0.975 will be considered to have overestimated standard errors towards a Type II error.

\subsubsection{Simulation code}
The \proglang{R} script \code{sim-CL.R} comprises the simulation code that simulates the data generating process that has been described above and includes three functions: \code{dgp()}, \code{fit()} and \code{sim()}.
While \code{dgp()} specifies the data generating process and generates a data frame with (at most) three regressors \code{x1}, \code{x2}, \code{x3} as well as cluster dimensions \code{id} and \code{time}.
\code{fit()} is responsible for the model fitting, the covariances as well as further outcomes (bias, mad, power and coverage). \code{sim()} conducts all simulations and provides for parallelization support.

%\begin{table}[t!]
%  \centering
%\begin{tabular}{| l | p{6cm} | p{7cm} |}
%  \hline
%  model & formula & model fit \\ \hline
%  \code{m} & \code{formula <- response ~ x1 + x2 + x3} & \code{lm(formula, data = data)} \\ \hline
%  \code{m_fe} & \code{formula_fe <- update(formula, . ~ . + id)} &\code{lm(formula_fe, data = data)} \\ \hline
%  \code{m_re} & \code{formula_re <- update(formula, . ~ . + (1 | id))} & \code{lme4::lmer(formula_re, data = %data, REML = FALSE)} \\ \hline
%  \code{m_gee} & \code{formula <- response ~ x1 + x2 + x3} & \code{geepack::geeglm(formula, data = data, id = %id, corstr = "exchangeable", family = gaussian)} \\ \hline
%\end{tabular}
%\caption{\code{sim-CL.R}: model fitting for a Gaussian response}
%  \label{tab:models}
%\end{table}

Table~\ref{tab:vcov} shows how the different types of covariances are calculated for a Gaussian response.
Four models are fitted: A pooled model (\code{m}),
% a fixed effects model (\code{m_fe}),
a random effects model (\code{m_re}) and a GEE with an exchangeable correlation structure (\code{m_gee}). 
%The fixed effects model includes fixed \code{id} effects,
The random effects model is specified as a model with random intercepts. Model fittig for random effects models and GEEs requires the packages \pkg{lme4} and \pkg{geepack}, respectively.
It has to be noticed, however, that it is not possible to fit all model types for all mentioned response distributions. There are some limitations for random effects models as well as for GEEs.

Furthermore, to calculate clustered standard errors, the square root of the diagonal elements of the covariance matrices in Table~\ref{tab:vcov} is calculated.

\begin{table}[t!]
  \centering
\begin{tabular}{llll}
  \hline
  Label & Model & Object & Variance-covariance matrix \\ \hline
  CL-0 & \code{lm} & \code{m} & \code{vcovCL(m, cluster = id, type = "HC0")} \\ 
  CL-1 & \code{lm} & \code{m} & \code{vcovCL(m, cluster = id, type = "HC1")} \\
  CL-2 & \code{lm} & \code{m} & \code{vcovCL(m, cluster = id, type = "HC2")} \\ 
  CL-3 & \code{lm} & \code{m} & \code{vcovCL(m, cluster = id, type = "HC3")} \\ 
  PL & \code{lm} & \code{m} & \code{vcovPL(m, cluster = id, adjust = FALSE)} \\ 
  PC & \code{lm} & \code{m} & \code{vcovPC(m, cluster = id, order.by = round)} \\ \hline
  standard &\code{lm} & \code{m} & \code{vcov(m)} \\ 
  basic & \code{lm} & \code{m} & \code{sandwich(m)} \\ \hline
 % fixed & \code{lm} & \code{m_fe} & \code{vcov(m_fe)[1:3, 1:3]} \\ 
  random & \code{lmer} & \code{m_re} & \code{vcov(m_re)} \\
  gee & \code{geeglm} & \code{m_gee}  & \code{m_gee\$geese\$vbeta} \\ \hline
\end{tabular}
\caption{Covariance matrices for a Gaussian response in \code{sim-CL.R}.
\label{tab:vcov}}
\end{table}

\subsection{Results}
The questions of interest in our simulation study can be summarized in the following four experiments:
\begin{itemize}
   \item Experiment I: How do the different model/covariance estimators perform for different types of regressors in the linear regression model?
   \item Experiment II: How do clustered covariances perform for glm type of responses?
   \item Experiment III: How do clustered covariances perform for beyond glm responses?
   \item Experiment IV: How do clustered covariances with HC0--HC3 bias correction perform for different type of responses?
\end{itemize}
Figure~\ref{fig:sim-01} shows the results from Experiment I and plots the coverage probabilities for the coefficients \code{x1} in Equation~\ref{x1}, \code{x2} in Equation~\ref{x2} and \code{x3} in Equation~\ref{x3} on the y-axis. Regressor \code{x1} is composed of a combination of a random draw at cluster level and a random draw at individual level, with a cluster correlation of 0.25. Regressor \code{x2} consists of a random draw at cluster level, and regressor \code{x3} is composed of a random draw at individual level. 

On the x-axis the within cluster correlation $\rho$ is shown, running from 0 (no correlation) to 0.9 (high correlation).
Depending on the regressors, the implications of running the cluster correlation $\rho$ from 0 to 0.9 on different covariance estimators are substantial.

Starting with regressor \code{x1}, at $\rho = 0$, all standard errors except the Driscoll and Kraay standard errors perform well. It can be observed that when the number of observations within each cluster is small with only $N_g = 5$, Driscoll and Kraay standard errors are substantially underestimated, even for a $\rho$ of 0.
Panel-corrected standard errors for regressor \code{x1} underestimate the ``true'' standard errors, that applies all the more the higher $\rho$. To a smaller extent, the same holds true for ``standard'' standard errors as well as basic sandwich standard errors.
As noted by \cite{hac:Hoechle:2007}, PCSE can be quite imprecise if the crosss-sectional dimension is large compared to the time dimension.

It can be observed that for regressor \code{x2}, Driscoll and Kraay standard errors, PCSE, and to a smaller extent also ``standard'' standard errors and basic sandwich standard errors are getting worse the larger the within cluster correlation. ``Standard'' standard errors are biased downwards because of the wrong assumption of independent observations \citep[see also][]{hac:Harden:2011}.

For regressor \code{x3}, independent of the value of $\rho$, all methods perform well (except again the Driscoll and Kraay estimator).

In summary, it holds for the linear model that if a regressor exhibits within cluster correlation even to a small extent, ``standard'' standard errors and basic sandwich standard errors that do not take the cluster dimension into account, deteriorate with an increasing $\rho$. In addition, PCSE and Driscoll and Kraay standard errors do not perform sufficiently well because of a too small $N_g$.

As the effects of regressor \code{x1} are in between the effects of regressors \code{x2} (varies at cluster level) and \code{x3} (varies at individual level), we limit ourselves to a single regressor \code{x1} in the following simulation experiments.

\setkeys{Gin}{width=\textwidth} 
\begin{figure}[t!] 
<<sim-01-figure,echo=FALSE,fig=TRUE,height=5,width=10>>=
my.settings <- canonical.theme(color = TRUE)
my.settings[["strip.background"]]$col <- "gray"
my.settings[["strip.border"]]$col <- "black"
my.settings[["superpose.line"]]$lwd <- 1
levels(s01$vcov)[c(5,2,1,7,4,6,3)] <- c("CL-0", "PL", "PC", "standard", "basic", "random", "gee")
s01$vcov <- factor(s01$vcov, levels(s01$vcov)[c(5,2,1,7,4,6,3)])
my.settings[["superpose.line"]]$col <- c("#377eb8", "green","#006400", "#ff7f00", "#f781bf", "#984ea3", "#e41a1c")
my.settings[["superpose.symbol"]]$col <- c("#377eb8", "green","#006400", "#ff7f00", "#f781bf", "#984ea3", "#e41a1c")
my.settings[["superpose.symbol"]]$pch <- c(rep(1,3), rep(2,2), rep(3,2))
xyplot(coverage ~ rho | par, groups = ~ factor(vcov),
  data = s01, subset = par != "(Intercept)",
  ylim = c(0, 1),
  type = "b", xlab = expression(rho), ylab = "coverage",
  auto.key = list(columns = 3),
  par.strip.text = list(col = "black"), par.settings = my.settings,
  panel = panel.xyref)
@ 
\caption{Experiment I}
\footnotesize{Experiment I explores how various types of (clustered) covariances perform for the linear regression model with Gaussian response and data composed of $G = 100$ (balanced) clusters each with $N_{g} = 5$ observations. The simulations of Experiment I are composed of $10,000$ replications. The regressors are included in the model, \code{x1} is composed of a linear combination of random draw at cluster level and a random draw at individual level with cluster correlation $\rho_x = 0.25$. \code{x2} consists of a random draw at cluster level, \code{x3} of a random draw at individual level. The coverage is plotted on the y-axis, the cluster correlation $\rho$ is plotted on the x-axis.
In a nutshell, if a regressor exhibits within cluster correlation even to a small extent, basic sandwich standard errors and ``standard'' standard errors that do not take the cluster dimension into account deteriorate with an increasing $\rho$. In addition, PCSE and Driscoll and Kraay standard errors do not perform sufficiently well because of a too small $N_g$. Nevertheless, clustered standard errors as well as standard errors from a random effects model and from a GEE with exchangeable correlation structure perform well.}
\label{fig:sim-01}
\end{figure}

Figure~\ref{fig:sim-02} illustrates the results from Experiment II. The y-axis represents again the coverage, and $\rho$ is plotted on the x-axis. Three response distributions are compared: Gaussian, binomial (with a logit link) and Poisson. Unlike in Experiment I, only a single regressor \code{x1} is included, with a cluster correlation $\rho_x$ of 0.25.
The left panel depicts the coverage for the Gaussian response, and thus corresponds to the left panel in Figure~\ref{fig:sim-01}. 

Results are qualitatively similar for the binomial and Poisson responses. The only exception are the standard errors from a random effects model, where the coverage is moving faster downwards than for the other standard errors. 
One can thus conclude that clustered standard errors will also work for glm's.

\begin{figure}[t!]
<<sim-02-figure,echo=FALSE,fig=TRUE,height=5,width=10>>=
my.settings <- canonical.theme(color = TRUE)
my.settings[["strip.background"]]$col <- "gray"
my.settings[["strip.border"]]$col <- "black"
my.settings[["superpose.line"]]$lwd <- 1
levels(s02$dist) <- c("gaussian", "binomial(logit)", "poisson")
levels(s02$vcov)[c(5,2,1,7,4,6,3)] <- c("CL-0", "PL", "PC", "standard", "basic", "random", "gee")
s02$vcov <- factor(s02$vcov, levels(s02$vcov)[c(5,2,1,7,4,6,3)])
my.settings[["superpose.line"]]$col <- c("#377eb8", "green","#006400", "#ff7f00", "#f781bf", "#984ea3", "#e41a1c")
my.settings[["superpose.symbol"]]$col <- c("#377eb8", "green","#006400", "#ff7f00", "#f781bf", "#984ea3", "#e41a1c")
my.settings[["superpose.symbol"]]$pch <- c(rep(1,3), rep(2,2), rep(3,2))
xyplot(coverage ~ rho | dist, groups = ~ factor(vcov),
  data = s02, subset = par != "(Intercept)",
  ylim = c(0.5, 1),
  type = "b", xlab = expression(rho), ylab = "coverage",
  auto.key = list(columns = 3),
  par.strip.text = list(col = "black"), par.settings = my.settings,
  panel = panel.xyref)
@
\caption{Experiment II}
\footnotesize{Experiment II explores how various types of (clustered) covariances perform for Gaussian, binomial and Poisson responses, where the data are composed of $G = 100$ (balanced) clusters each with $N_{g} = 5$ observations. The simulations of Experiment II are composed of $10,000$ replications. There is only a single regressor included in the model, \code{x1} is composed of a linear combination of random draw at cluster level and a random draw at individual level with cluster correlation $\rho_x = 0.25$. The coverage is plotted on the y-axis, the cluster correlation $\rho$ is plotted on the x-axis. Compared to the Gaussian response, results are qualitatively similar for the binomial and Poisson responses. The only exception are the standard errors from a random effects model, where the coverage is moving faster downwards than for the other standard errors. One can thus conclude that clustered standard errors will also work for glm's.}
\label{fig:sim-02}  
\end{figure}

Figure~\ref{fig:sim-03} shows the outcome of Experiment III and reveals the strength of the new function \code{vcovCL()} that comes with the feature that allows to estimate clustered covariances for models beyond glms. The y-axis represents again the coverage, and $\rho$ is plotted on the x-axis. 
Comparisons are made between beta regression, zero-truncated Poisson and zero-inflated Poisson (ZIP). For these response distributions, clustered standard errors perform well and remain nearly constant even for an increasing $\rho$. Standard errors that do not take into account the cluster correlation, like ``standard'' standard errors and basic sandwich standard errors underestimate the true standard errors the more the larger $\rho$.

\begin{figure}[t!] 
<<sim-03-figure,echo=FALSE,fig=TRUE,height=5,width=10>>=
s03 <- na.omit(s03)
my.settings <- canonical.theme(color = TRUE)
my.settings[["strip.background"]]$col <- "gray"
my.settings[["strip.border"]]$col <- "black"
my.settings[["superpose.line"]]$lwd <- 1
levels(s03$vcov)[c(2,3,1)]  <- c("CL-0", "standard", "basic")
levels(s03$dist)[c(1,2,3)] <- c("betareg", "zerotrunc", "zeroinfl")
s03$vcov <- factor(s03$vcov, levels(s03$vcov)[c(2,3,1)])
my.settings[["superpose.line"]]$col <- c("#377eb8", "#ff7f00", "#f781bf")
my.settings[["superpose.symbol"]]$col <- c("#377eb8", "#ff7f00", "#f781bf")
my.settings[["superpose.symbol"]]$pch <- c(1, rep(2,2))
xyplot(coverage ~ rho | dist, groups = ~ factor(vcov),
  data = s03, subset = par != "(Intercept)",
  ylim = c(0.8, 1),
  type = "b", xlab = expression(rho), ylab = "coverage",
  auto.key = list(columns = 2),
  par.strip.text = list(col = "black"), par.settings = my.settings,
  panel = panel.xyref)
@ 
\caption{Experiment III}
\footnotesize{Experiment III explores how various types of (clustered) covariances perform for Beta, zero-truncated Poisson and zero-inflated Poisson (ZIP) responses, where the data are composed of $G = 100$ (balanced) clusters each with $N_{g} = 5$ observations. The simulations of Experiment III are composed of $10,000$ replications. There is only a single regressor included in the model, \code{x1} is composed of a linear combination of random draw at cluster level and a random draw at individual level with cluster correlation $\rho_x = 0.25$. The coverage is plotted on the y-axis, the cluster correlation $\rho$ is plotted on the x-axis.
Comparisons are made between beta regression, zero-truncated Poisson and zero-inflated Poisson. For these response distributions, clustered standard errors perform well and remain nearly constant even for an increasing $\rho$. Standard errors that do not take into account the cluster correlation, like ``standard'' standard errors and basic sandwich standard errors underestimate the true standard errors the more the larger $\rho$.}
\label{fig:sim-03}
\end{figure}

Figure~\ref{fig:sim-04} depicts the findings of Experiment IV. The y-axis represents again the coverage, but in contrast to the other simulation experiments, the number of clusters $G$ is plotted on the x-axis, ranging from 10 to 50 clusters, and in further steps of 50 up to 250 clusters. Gaussian, binomial and Poisson responses are compared with each other, with the focus on clustered standard errors with HC0--HC3 types of bias correction.

In most cases, all of the standard errors are underestimated for $G = 10$ clusters (except clustered standard errors with HC3 bias correction for the binomial response). The larger the number of clusters $G$, the better the coverage and the less standard errors are underestimated. It can be observed that about 50 clusters is often enough for accurate inference. This result is well known in the literature \citep[][among others]{hac:Arceneaux+Nickerson:2009,hac:Petersen:2009,hac:Harden:2011,hac:Cameron+Miller:2015}.

Additionally, it can be observed that the higher the number of clusters, the less the different types of HC bias correction make the difference. However, for a small number of clusters, the HC3 correction works best, followed by HC2, HC1 and HC0. For the linear model, \cite{hac:Long+Ervin:2000} suggest to use HC3 for small samples (with less than 250 observations).

\begin{figure}[t!] 
<<sim-04-figure,echo=FALSE,fig=TRUE,height=5,width=10>>=
my.settings <- canonical.theme(color = TRUE)
my.settings[["strip.background"]]$col <- "gray"
my.settings[["strip.border"]]$col <- "black"
my.settings[["superpose.line"]]$lwd <- 1
levels(s04$vcov)[c(1:4)] <- c("CL-0", "CL-1", "CL-2", "CL-3")
s04$vcov <- factor(s04$vcov, levels(s04$vcov)[c(1:4)])
levels(s04$dist)[c(1:3)] <- c("gaussian", "binomial(logit)", "poisson")
my.settings[["superpose.line"]]$col <- c("#377eb8", "#00E5EE", "#e41a1c", "#4daf4a")
my.settings[["superpose.symbol"]]$col <- c("#377eb8", "#00E5EE","#e41a1c", "#4daf4a")
xyplot(coverage ~ nid | dist, groups = ~ factor(vcov),
  data = na.omit(s04), subset = par != "(Intercept)",
  type = "b", xlab = "G", ylab = "coverage",
  auto.key = list(columns = 2),
  par.strip.text = list(col = "black"), par.settings = my.settings,
  panel = panel.xyref)
@
\caption{Experiment IV}
\footnotesize{Experiment IV explores how clustered covariances with HC0-HC3 correction perform for Gaussian, binomial and Poisson responses, the cluster correlation is fixed at $\rho = 0.25$. The data are composed of $G = 10, 50, 100, \ldots, 250$ (balanced) clusters each with $N_{g} = 5$ observations. The simulations of Experiment IV are composed of $10,000$ replications. There is only a single regressor included in the model, \code{x1} is composed of a linear combination of random draw at cluster level and a random draw at individual level with cluster correlation $\rho_x = 0.25$. The coverage is plotted on the y-axis, the number of clusters $G$ on the x-axis.}
\label{fig:sim-04}
\end{figure}

\section*{Computation details}

The packages \pkg{sandwich}, \pkg{countreg}, \pkg{geepack}, \pkg{lattice}, \pkg{lme4}, \pkg{lmtest},\pkg{multiwayvcov}, \pkg{plm} and \pkg{pscl} are
required for the applications in this paper.
For replication of the simulation exercises, the \proglang{R} script \code{sim-CL.R} is required.

\proglang{R} version \Sexpr{paste(R.Version()[6:7], collapse = ".")} has been used for computations.
Package versions that have been employed are \pkg{sandwich}
\Sexpr{gsub("-", "--", packageDescription("sandwich")$Version)},
\pkg{countreg} \Sexpr{gsub("-", "--", packageDescription("countreg")$Version)},
\pkg{geepack} \Sexpr{gsub("-", "--", packageDescription("geepack")$Version)},
\pkg{lattice} \Sexpr{gsub("-", "--", packageDescription("lattice")$Version)},
\pkg{lme4} \Sexpr{gsub("-", "--", packageDescription("lme4")$Version)},
\pkg{lmtest} \Sexpr{gsub("-", "--", packageDescription("lmtest")$Version)},
\pkg{multiwayvcov} \Sexpr{gsub("-", "--", packageDescription("multiwayvcov")$Version)},
\pkg{plm} \Sexpr{gsub("-", "--", packageDescription("plm")$Version)},
and \pkg{pscl} \Sexpr{gsub("-", "--", packageDescription("pscl")$Version)}
have been used.

\proglang{R} itself and all packages (except \pkg{countreg}) used are available from
CRAN at \url{https://CRAN.R-project.org/}. 
\pkg{countreg} is accessible from \url{https://R-Forge.R-project.org/projects/countreg/}.


\bibliography{hac}

\newpage

\begin{appendix}

%% for "plain pretty" printing
\DefineVerbatimEnvironment{Sinput}{Verbatim}{}
<<echo=FALSE>>=
options(prompt = "  ")
@

\section{Simulation of correlated data with a random cluster effect} \label{sec:rceffect}

Introducing within cluster error correlation via a random cluster effect instead of a copula comes along with introducing a bias, which also affects the coverage.
The linear predictor in Equation~\ref{eq:predictor} with a random cluster effect $v_g$ is
\begin{eqnarray} \label{eq:predictorv}
h(\mu_{ig}) = \beta_{0} + \beta_{1} \cdot x_{1,ig} + \beta_{2} \cdot x_{2,g} + \beta_{3} \cdot x_{3,ig} + v_g,
\end{eqnarray}
where $v_g$ is a random draw at cluster level $v_{g} \sim \mathcal{N}_{g}(0, \frac{\rho}{1 - \rho})$.
$\rho$ ranges from 0 to 0.9 and determines again the importance of the random cluster effect.

Contrasting coverage and bias plots for models except the linear model depicts the problem caused by this approach.

\begin{figure}[t!] 
<<sim-05-figure,echo=FALSE,fig=TRUE,height=5,width=10>>=
my.settings <- canonical.theme(color = TRUE)
my.settings[["strip.background"]]$col <- "gray"
my.settings[["strip.border"]]$col <- "black"
my.settings[["superpose.line"]]$lwd <- 1
levels(s05$vcov) <- c("CL-0", "standard", "basic")
levels(s05$vcov)[c(2,3,1)]  <- c("CL-0", "standard", "basic")
s05$vcov <- factor(s05$vcov, levels(s05$vcov)[c(2,3,1)])
levels(s05$dist) <- c("gaussian", "binomial(logit)", "poisson")
my.settings[["superpose.line"]]$col <- c("#377eb8", "#ff7f00", "#f781bf")
my.settings[["superpose.symbol"]]$col <- c("#377eb8", "#ff7f00", "#f781bf")
my.settings[["superpose.symbol"]]$pch <- c(1, rep(2,2))
xyplot(coverage ~ rho | dist, groups = ~ factor(vcov),
  data = s05, subset = par != "(Intercept)",
  ylim = c(0, 1),
  type = "b", xlab = expression(rho), ylab = "coverage",
  auto.key = list(columns = 2),
  par.strip.text = list(col = "black"), par.settings = my.settings,
  panel = panel.xyref)
@  
<<sim-05-bias-figure,echo=FALSE,fig=TRUE,height=5,width=10>>=
panel.xyref <- function(x, y, ...) {
  panel.abline(h = 0, col = 2)
  panel.xyplot(x, y, ...)  
}
my.settings <- canonical.theme(color = TRUE)
my.settings[["strip.background"]]$col <- "gray"
my.settings[["strip.border"]]$col <- "black"
my.settings[["superpose.line"]]$lwd <- 1
levels(s05$vcov)[c(1:3)] <- c("basic", "CL-0", "standard")
levels(s05$dist) <- c("gaussian", "binomial(logit)", "poisson")
my.settings[["superpose.line"]]$col <- c("slategray", "slategray", "slategray")
my.settings[["superpose.symbol"]]$col <- c("slategray", "slategray", "slategray")
xyplot(bias ~ rho | dist, groups = ~ factor(vcov),
  data = s05, subset = par != "(Intercept)",
  ylim = c(-0.5, 0.1),
  type = "b", xlab = expression(rho), ylab = "bias",
  par.strip.text = list(col = "black"), par.settings = my.settings,
  panel = panel.xyref)
@ 
\caption{Introducing correlation via a random cluster effect}
\footnotesize{Explores how various types of (clustered) covariances perform for Gaussian, binomial and Poisson responses, where the data are composed of $G = 100$ (balanced) clusters each with $N_{g} = 5$ observations. The simulation is composed of $10,000$ replications. There is only a single regressor included in the model, \code{x1} is composed of a linear combination of random draw at cluster level and a random draw at individual level with cluster correlation $\rho_x = 0.25$. The coverage is plotted on the y-axis in the upper panel, the bias in the lower panel. The cluster correlation $\rho$ is plotted on the x-axis in both panels.}
\label{fig:sim-05-01}  
\end{figure}

Figure~\ref{fig:sim-05-01} constrasts coverage (in the upper panel) and bias (in the lower panel) for Gaussian, binomial (with a logit link) and Poisson response distributions with a single regressor \code{x1}. Regressor \code{x1} exhibits a cluster correlation $\rho_x$ of 0.25. Each of the 10,000 simulated datasets consists of 100 clusters with 5 observations per cluster.

For the Gaussian response, the random cluster effect does not introduce a bias, but for the binomial and Poisson responses, the random cluster effect comes along with a negative bias. It can be observed that the bias is rising with the random cluster effect $\rho$.

\begin{figure}[t!] 
<<sim-02-1figure,echo=FALSE,fig=TRUE,height=5,width=10>>=
panel.xyref <- function(x, y, ...) {
  panel.abline(h = 0.95, col = 2)
  panel.xyplot(x, y, ...)  
}
my.settings <- canonical.theme(color = TRUE)
my.settings[["strip.background"]]$col <- "gray"
my.settings[["strip.border"]]$col <- "black"
my.settings[["superpose.line"]]$lwd <- 1
levels(s02$dist) <- c("gaussian", "binomial(logit)", "poisson")
levels(s02$vcov)[c(5,2,1,7,4,6,3)] <- c("CL-0", "PL", "PC", "standard", "basic", "random", "gee")
s02$vcov <- factor(s02$vcov, levels(s02$vcov)[c(5,2,1,7,4,6,3)])
my.settings[["superpose.line"]]$col <- c("#377eb8", "#ff7f00", "#f781bf")
my.settings[["superpose.symbol"]]$col <- c("#377eb8", "#ff7f00", "#f781bf")
my.settings[["superpose.symbol"]]$pch <- c(1, rep(2,2))
xyplot(coverage ~ rho | dist, groups = ~ factor(vcov),
  data = s02[which(s02$vcov %in% c("standard", "basic", "CL-0")), ], subset = par != "(Intercept)",
  ylim = c(0, 1),
  type = "b", xlab = expression(rho), ylab = "coverage",
  auto.key = list(columns = 2),
  par.strip.text = list(col = "black"), par.settings = my.settings,
  panel = panel.xyref)
@ 
<<sim-02-1-bias-figure,echo=FALSE,fig=TRUE,height=5,width=10>>=
panel.xyref <- function(x, y, ...) {
  panel.abline(h = 0, col = 2)
  panel.xyplot(x, y, ...)  
}
my.settings <- canonical.theme(color = TRUE)
my.settings[["strip.background"]]$col <- "gray"
my.settings[["strip.border"]]$col <- "black"
my.settings[["superpose.line"]]$lwd <- 1
levels(s02$dist) <- c("gaussian", "binomial(logit)", "poisson")
levels(s02$vcov)[c(5,2,1,7,4,6,3)] <- c("CL-0", "PL", "PC", "standard", "basic", "random", "gee")
s02$vcov <- factor(s02$vcov, levels(s02$vcov)[c(5,2,1,7,4,6,3)])
my.settings[["superpose.line"]]$col <- c("slategray", "slategray", "slategray")
my.settings[["superpose.symbol"]]$col <- c("slategray", "slategray", "slategray")
xyplot(bias ~ rho | dist, groups = ~ factor(vcov),
  data = s02[which(s02$vcov %in% c("standard", "basic", "CL-0")), ], subset = par != "(Intercept)",
  ylim = c(-0.5, 0.1),
  type = "b", xlab = expression(rho), ylab = "bias",
  par.strip.text = list(col = "black"), par.settings = my.settings,
  panel = panel.xyref)
@ 
\caption{Introducing correlation via a normal copula}
\footnotesize{Explores how various types of (clustered) covariances perform for Gaussian, binomial and Poisson responses, where the data are composed of $G = 100$ (balanced) clusters each with $N_{g} = 5$ observations. The simulation is composed of $10,000$ replications. There is only a single regressor included in the model, \code{x1} is composed of a linear combination of random draw at cluster level and a random draw at individual level with cluster correlation $\rho_x = 0.25$. The coverage is plotted on the y-axis in the upper panel, the bias in the lower panel. The cluster correlation $\rho$ is plotted on the x-axis in both panels.}
\label{fig:sim-02-1}
\end{figure}

Introducing within cluster correlation via a copula as is done for all simulation exercises in Section~\ref{sec:simulation} does not introduce a bias. This can be observed in Figure~\ref{fig:sim-02-1}, where estimators are unbiased for the response distributions and thus no further deteriorate the coverage.

\end{appendix}

\end{document}
