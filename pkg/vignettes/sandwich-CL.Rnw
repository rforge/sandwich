\documentclass[nojss]{jss}
\usepackage{thumbpdf}
\usepackage{amsmath,amssymb,bm}
\usepackage{multicol}
%% need no \usepackage{Sweave}

\newcommand{\newoperator}[3]{\newcommand*{#1}{\mathop{#2}#3}}
\newcommand{\renewoperator}[3]{\renewcommand*{#1}{\mathop{#2}#3}}
\newcommand{\vI}{\bm I}
\newcommand{\vH}{\bm H}

\author{Susanne Berger\\University of Innsbruck \And 
        Nathaniel Graham\\Trinity University Texas \And
        Achim Zeileis\\University of Innsbruck}
\title{Various Versatile Variances: An Object-Oriented Implementation of Clustered Covariances in \proglang{R}}

\Plainauthor{Susanne Berger, Nathaniel Graham, Achim Zeileis}
\Plaintitle{Various Versatile Variances: An Object-Oriented Implementation of Clustered Covariances in R}
\Shorttitle{Various Versatile Variances}

\Keywords{clustered data, clustered covariance matrix estimators, object-orientation, simulation, \proglang{R}}
\Plainkeywords{clustered data, clustered covariance matrix estimators, object-orientation, simulation, R}

\Abstract{
Clustered covariances or clustered standard errors are very widely used to account for correlated or clustered data, especially in economics, political sciences, or other social sciences. They are employed to adjust the inference following estimation of a standard least-squares regression or generalized linear model estimated by maximum likelihood. Although many publications just refer to ``the'' clustered standard errors, there is a surprisingly wide variation in clustered covariances, particularly due to different flavors of bias corrections. Furthermore, while the linear regression model is certainly the most important application case, the same strategies can be employed in more general models (e.g. for zero-inflated, censored, or limited responses).

In \proglang{R}, the sandwich package \citep{hac:Zeileis:2004a,hac:Zeileis:2006} provides an object-oriented approach to ``robust'' covariance matrix estimation based on methods for two generic functions (\code{estfun()} and \code{bread()}). Using this infrastructure, sandwich covariances for cross-section or time series data have been available for models beyond \code{lm()} or \code{glm()}, e.g., for packages \pkg{MASS}, \pkg{pscl}, \pkg{countreg}, \pkg{betareg}, among many others. However, corresponding functions for clustered or panel data have been somewhat scattered or available only for certain modeling functions. This shortcoming has been corrected in the development version of sandwich on R-Forge. Here, we introduce this new object-oriented implementation of clustered and panel covariances and assess the methods' performance in a simulation study.
}

\Address{
  Susanne Berger\\
  Department of Statistics\\
  Faculty of Economics and Statistics\\
  Universit\"at Innsbruck\\
  Universit\"atsstr.~15\\
  6020 Innsbruck, Austria\\
  Telephone: +43/512/507-7113\\
  E-mail: \email{susanne.berger@uibk.ac.at}
}

\begin{document}

\SweaveOpts{engine=R,eps=FALSE}
%\VignetteIndexEntry{Various Versatile Variances: An Object-Oriented Implementation of Clustered Covariances in R}
%\VignetteDepends{sandwich,pscl,lattice}
%\VignetteKeywords{clustered data, clustered covariance matrix estimators, object-orientation, simulation, R}
%\VignettePackage{sandwich}

<<preliminaries,echo=FALSE,results=hide>>=
library("sandwich")
library("pscl")
library("lattice")
library("countreg")
library("lme4")
source("sim-CL.R")
panel.xyref <- function(x, y, ...) {
  panel.abline(h = 0.95, col = 2)
  panel.xyplot(x, y, ...)  
}
options(prompt = "R> ", continue = "+   ")
@ 

% simulations

<<sim-01,echo=FALSE,results=hide>>=
if(file.exists("sim-01.rda")) load("sim-01.rda") else {
set.seed(1)
s01 <- sim(nrep = 10000, nid = 100, nround = 5,
           dist = "gaussian", rho = seq(0, 0.9, by = 0.1), xrho = 0.25,
           coef = c(0, 0.85, 0.5, 0.7), formula = response ~ x1 + x2 + x3,
           vcov = c("without", "HC0", "HC0-id", "random", "gee", "bk"),
           type = "copula")
save(s01, file = "sim-01.rda")
}
@

<<sim-02,echo=FALSE,results=hide>>=
if(file.exists("sim-02.rda")) load("sim-02.rda") else {
set.seed(2)
s02 <- sim(nrep = 10000, nid = 100, nround = 5,
           dist = c("gaussian", "logit", "poisson"), rho = seq(0, 0.9, by = 0.1), xrho = 0.25,
           coef = c(0, 0.85, 0, 0), formula = response ~ x1,
           vcov = c("without", "HC0", "HC0-id", "random", "gee", "bk"),
           type = "copula")
save(s02, file = "sim-02.rda")
}
@

<<sim-03,echo=FALSE,results=hide>>=
if(file.exists("sim-03.rda")) load("sim-03.rda") else {
set.seed(3)
s03 <- sim(nrep = 10000, nid = 100, nround = 5,
           dist = c("zerotrunc", "zip", "beta"), rho = seq(0, 0.9, by = 0.1), xrho = 0.25,
           coef = c(0, 0.85, 0, 0), formula = response ~ x1,
           vcov = c("without", "HC0", "HC0-id"),
           type = "copula")
save(s03, file = "sim-03.rda")
}
@

<<sim-04,echo=FALSE,results=hide>>=
if(file.exists("sim-04.rda")) load("sim-04.rda") else {
set.seed(4)
s04 <- sim(nrep = 10000, nid = c(10, seq(50, 250, by = 50)), nround = 5,
           dist = c("gaussian","poisson", "logit"), rho = 0.25, xrho = 0.25,
           coef = c(0, 0.85, 0, 0), formula = response ~ x1,
           vcov = c("HC0-id","HC1-id","HC2-id","HC3-id"),
           type = "copula")
save(s04, file = "sim-04.rda")
}
@

<<sim-05,echo=FALSE,results=hide>>=
if(file.exists("sim-05.rda")) load("sim-05.rda") else {
set.seed(5)
s05 <- sim(nrep = 10000, nid = 100, nround = 5,
           dist = c("gaussian", "logit", "poisson"), rho = seq(0, 0.9, by = 0.1), xrho = 0.25,
           coef = c(0, 0.85, 0, 0), formula = response ~ x1,
           vcov = c("without", "HC0", "HC0-id"),
           type = "ranef")
save(s05, file = "sim-05.rda")
}
@

\section{Introduction} \label{sec:intro}

Observations that contain correlation between objects of the same group/cluster are often referred to as ``cluster-correlated'' observations. Each cluster comprises multiple objects, such that the data structure is either nested or hierarchical \citep{hac:Galbraith:Daniel:Vissel:2010}.
For model errors that are correlated within, but not across clusters, the assumption of independent and identically distributed errors cannot plausibly sustain.
But ignoring dependency of model errors within clusters by using ``classical'' standard errors (that is, iid standard errors without heteroscedasticity and without autocorrelation) can result in an overstate of estimator precision and can thus lead to serious mistakes in statistical inference \citep{hac:Moulton:1986, hac:Moulton:1990}.

For valid statistical inference, the correlation between objects of the same group can no longer be ignored.
Thus, clustered covariances are widely used to account for clustered errors.
Clustered errors can emerge in cross-section as well as in panel data. For cross-section data, the cluster dimension can for instance be the industry in which companies operate or the classroom of pupils.
In panel data, clustered errors can arise from correlation of different time periods for a given individual, while there is no correlation across individuals \citep{hac:Cameron+Miller:2015}.

This paper contributes to the literature particularly in two respects:

First, we discuss a set of computational tools for the \proglang{R} system for statistical computing \citep{hac:R:2008}, providing for an object-oriented implementation of clustered covariances/standard errors in the \proglang{R} package \pkg{sandwich}.
Using this infrastructure, sandwich covariances for cross-section or time series data have been available for models beyond \code{lm()} or \code{glm()}, e.g., for packages \pkg{MASS}, \pkg{pscl}, \pkg{countreg}, \pkg{betareg}, among many others. However, corresponding functions for clustered or panel data have been somewhat scattered or available only for certain modeling functions. This shortcoming has been corrected in the development version of sandwich on \proglang{R}-Forge.

Second, we perform a Monte Carlo simulation study for various response distributions with the aim to test the performance of clustered standard errors beyond \code{lm()} and \code{glm()}. Up to our knowledge, there does not yet exist such a simulation exercise in the literature.

The rest of the paper is structured as follows: Section~\ref{sec:idea} discusses the idea of clustered covariances and reviews existing \proglang{R} packages for sandwich as well as clustered covariances.
Section~\ref{sec:methods} deals with the theory behind sandwich covariances, especially with respect to clustered covariances for cross-sectional and longitudinal data, clustered data as well as panel data.
Section~\ref{sec:software} then takes a look behind the scenes of a new object-oriented \proglang{R} implementation for clustered covariances, Section~\ref{sec:illu} gives an empirical illustration based on data provided from \cite{hac:Petersen:2009} and \cite{hac:Aghion+VanReenen+Zingales:2013}. The simulation setup and results are discussed in Section~\ref{sec:simulation}. Section~\ref{sec:summary} concludes.

\section{Idea} \label{sec:idea}

As data described by econometric models often contain heteroscedasticity and/or autocorrelation of unknown form, it is essential to use covariance matrix estimators for inference that can consistently estimate the covariance of the model parameters.
The idea of a quasi-maximum likelihood approach is to estimate a model by assuming independent data, but use sandwich covariances to adjust for heteroscedasticity and/or autocorrelation.

Independent but heteroscedastic model errors typically occur within a context of cross-section data. Then, heteroscedasticity consistent (HC) covariance estimators can be used to account for the fact that the diagonal elements of the covariance matrix are nonconstant.

For time series data, the independency assumption of the errors often does not hold because of autocorrelation. For such cases, heteroscedasticity and autocorrelation consistent (HAC) estimators are a suitable option.

For clustered or panel data, with errors independent across but correlated within clusters, it is quite common to use clustered sandwich covariances.
Various kinds of sandwich covariances have already been implemented in a couple of \proglang{R} packages.

\subsection[R packages for sandwich covariances]{\proglang{R} packages for sandwich covariances}

The standard \proglang{R} package to model sandwich covariance estimators is the \pkg{sandwich} package \citep{hac:Zeileis:2004a,hac:Zeileis:2006}, which provides for an object-oriented implementation for the building blocks of the sandwich that rely only on a small set of extractor functions (\code{estfun()} and \code{bread()}) for fitted model objects.
The function \code{sandwich()} computes a sandwich estimate from a fitted model object, default is the Eicker-Huber-White outer product estimate \citep{hac:White:1980}.
\code{vcovHC()} is a wrapper calling \code{sandwich()} and \code{bread()}, and can compute arbitrary HC covariance matrix estimates, ranging from HC0 to HC5.
\code{vcovHAC()} can compute HAC covariance matrix estimates, with the convenience functions \code{kernHAC()} for Andrews' kernel HAC \citep{hac:Andrews:1991} and \code{NeweyWest()} for Newey-West-style HAC \citep{hac:Newey+West:1987,hac:Newey+West:1994}. Thus, for cross-section and time-series data, object-oriented implementations for HC and HAC estimators are already implemented in \pkg{sandwich}.

Another \proglang{R} package that includes heteroscedasticity consistent covariance estimators for models produced by \code{lm()} is the \pkg{car} package \citep{hac:Fox+Weisberg:2011}. Its function \pkg{hccm} calculates HC0-HC4 type covariance matrices.

However, there is as yet no systematic implementation of clustered/panel covariances beyond (generalized) linear models. Though, activities were carried out to implement clustered covariances in a couple of \proglang{R} packages.

\subsection[R packages for clustered covariances]{\proglang{R} packages for clustered covariances}

Clustered covariances are availlable in several \proglang{R} packages, including \pkg{multiwayvcov} \citep{hac:Graham+Arai+Hagstroemer:2016}, \pkg{plm} \citep{hac:Croissant+Millo:2008}, \pkg{geepack} \citep{hac:Halekoh+Hojsgaard+Yan:2002}, \pkg{lfe} \citep{hac:Gaure:2016}, \pkg{clubSandwich} \citep{hac:Pustejovsky:2016} and \pkg{clusterSEs} \citep{hac:Esarey:2017}, among others.

Nevertheless, usage is restricted to \code{lm} and \code{glm}-like object in \pkg{multiwayvcov}, to \code{plm} objects in \pkg{plm},
to \code{geeglm} objects in \pkg{geepack}, to \code{felm} objects in \pkg{lfe}, and to ordinary and weighted least squares linear regression models in \pkg{clubSandwich}.
While \pkg{multiwayvcov} and \pkg{clubSandwich} are specialized on the estimation of clustered covariances, the other packages mentioned are partly focused on different issues. \pkg{clusterSEs} calculates cluster-robust p-values and confidence intervalls, while \pkg{plm} places the focus on the estimation of linear panel models. \pkg{geepack} concentrates on a generalized estimation equations approach, and allows for clustered covariances via an independence correlation structure, and \pkg{lfe} is mainly used to estimate linear models with multiple group fixed effects, but allows for clustered covariances, too.

In a nutshell, there is until now no object-oriented implementation for clustered covariances in \proglang{R}. There are some packages that allow for clustered covariances, but connected with the disadvantage that these functions are only applicable to a limited range of modeling functions. While the linear regression model is of course an important application case, the same strategies can be employed in more general models, including zero-inflated, censored, or limited responses. 

In the next section, some theoretical background of the underlying methods for clustered as well as panel data is introduced. As a starting point, the structure of sandwich covariances is established, because all types of covariances considered here are based on sandwich covariances.

\section{Methods} \label{sec:methods}

Let $(y_{i},x_{i})$ for $i = 1, \ldots, n$ be data with some distribution controlled by a parameter vector $\theta$ with k dimensions.
To compute sandwich covariances, take a bread matrix $B(\theta)$ and a meat matrix $M(\theta)$ and multiply them to a sandwich with meat between two slices of bread \citep{hac:Zeileis:2006}
\begin{eqnarray} \label{eq:sandwich}
  S(\theta) & = & B(\theta) \cdot M(\theta) \cdot B(\theta) \\  \label{eq:bread}
  B(\theta) & = & \left( \E[ - \psi'(y, x, \theta) ] \right)^{-1} \\  \label{obj}
  M(\theta) & = & \VAR[ \psi(y, x, \theta) ].
\end{eqnarray}
The building blocks for the calculation of the sandwich are provided by the \pkg{sandwich} package.
A natural idea for an object-oriented implementation of such estimators is to provide for various functions that compute different estimators for the meat $M(\theta)$ based on an \code{estfun()} extractor function that extracts the empirical estimating functions $\psi(y, x, \theta)$ from a fitted model object.
An estimating function
\begin{eqnarray}
  \psi(y, x, \theta) \quad = \quad \frac{\partial \Psi(y, x, \theta)}{\partial \theta},
\end{eqnarray}
is defined as the derivative of an objective function $\Psi(y, x, \theta)$, typically the log-likelihood, with respect to a parameter vector $\theta$.
Thus, an empirical estimating (or score) function evaluates an estimating function at the observed data and the estimated parameters \citep{hac:Zeileis:2006}.
\code{estfun()} returns an $n \times k$ matrix of empirical estimating functions from a fitted model object \citep{hac:Zeileis:2006}:
 \[ \left( \begin{array}{c} \psi(y_1, x_1, \hat \theta) \\ \vdots \\ \psi(y_n, x_n, \hat \theta)
           \end{array} \right). \]

\code{bread()} extracts the empirical version of the inverse Hessian
\begin{equation} \label{eq:Bhat}
  \hat B \quad = \quad \left( \frac{1}{n} \sum_{i = 1}^n - \psi'(y_i, x_i, \hat \theta) \right)^{-1}.
\end{equation}
\code{meat()}, \code{meatHAC()} and \code{meatHC()} compute outer product, HAC and HC estimators for the meat, respectively, given the existence of an \code{estfun()} method:
\begin{eqnarray} \label{eq:meat-op}
  \hat M & = & \frac{1}{n} \sum_{i = 1}^n\psi(y_i, x_i, \hat \theta) \psi(y_i, x_i, \hat \theta)^\top \\ \label{eq:meat-hac}
  \hat M_\mathrm{HAC} & = & \frac{1}{n} \sum_{i, j = 1}^n w_{|i-j|} \, \psi(y_i, x_i, \hat \theta) \psi(y_j, x_j, \hat \theta)^\top \\ \label{eq:meat-hc}
  \hat M_\mathrm{HC} & = & \frac{1}{n} X^\top \left( \begin{array}{ccc} 
  \omega(r(y_1, x_1^\top \theta)) & \cdots & 0 \\ 
  \vdots & \ddots & \vdots \\
  0 & \cdots & \omega(r(y_n, x_n^\top \theta))
  \end{array} \right) X.
\end{eqnarray}
The outer product estimator in (\ref{eq:meat-op}) corresponds to the White estimator \citep{hac:Eicker:1963,hac:Huber:1967,hac:White:1980}.
$w_{|i-j|}$ in (\ref{eq:meat-hac}) is a vector of weights \citep{hac:Zeileis:2004}
In (\ref{eq:meat-hc}), functions $\omega(\cdot)$ derive estimates of the variance of the observed working residuals $r(y_1, x_1^\top \theta), \ldots, r(y_n, x_n^\top \theta)$ \citep{hac:Zeileis:2006}.

\subsection{Clustered covariances} 

A simple and natural estimator for the meat matrix is the outer product of the empirical estimating functions $\hat M$.
For (one-way) clustered covariances, the level of aggregation changes from individuals to clusters/groups, which leads to changes in the meat matrix. 
The empirical estimating functions are first summed groupwise (e.g., for $g = 1, \ldots, G$ cluster, with $n_{g}$ observations per cluster), returning a $G \times k$ matrix of empirical estimating functions
 \[ \left( \begin{array}{c} \psi(y_1, x_1, \hat \theta) \\ \vdots \\ \psi(y_G, x_G, \hat \theta)
           \end{array} \right), \]
before the outer product is calculated
\begin{equation} \label{eq:meatCL}
  \hat M_\mathrm{CL} \quad = \quad \frac{1}{n} \sum_{g = 1}^G\sum_{i = 1}^{n_{g}}\psi(y_{ig}, x_{ig}, \hat \theta) \psi(y_{ig}, x_{ig}, \hat \theta)^\top.
\end{equation}
The clustered covariance estimator generalizes White's famous covariance estimator \citep{hac:White:1980}.
If it is the case that each observation is its own cluster, clustered covariances correspond to the White's estimator.

\subsubsection{Types of bias correction}
The clustered covariance estimator controls for both heteroscedasticity across as well as within cluster, but this comes at the cost that the number of clusters approaches infinity, not the number of observations \citep{hac:Cameron+Gelbach+Miller:2008,hac:Cameron+Miller:2015}.
Although many publications just refer to ``the'' clustered standard errors, there is a surprisingly wide variation in clustered covariances, particularly due to different flavors of bias corrections.
The bias correction factor can be split in two parts, a ``cluster bias correction'' and an ``HC bias correction''.
The cluster bias correction is defined as 
\begin{equation}
 \frac{G}{G - 1},
\end{equation} 
where $G$ is again the number of clusters.
The HC bias correction can additionally to the cluster bias correction be applied to the clustered covariance estimator. This is an easy task for HC0 and HC1 types of bias correction, but more demanding for cluster generalizations of HC2 and HC3, which have been developed by \cite{hac:Kauermann+Carroll:2001} and \cite{hac:Bell+McCaffrey:2002}.
HC0 to HC3 bias corrections are defined as
\begin{eqnarray} \label{eq:biasadj}
  \mathrm{HC0:} & = & 1 \\
  \mathrm{HC1:} & = & \frac{n}{n - k} \\
  \mathrm{HC2:} & = & [\vI_{n_{g}} - \vH_{gg}]^{-0.5} \\
  \mathrm{HC3:} & = & [\vI_{n_{g}} - \vH_{gg}]^{-1}, 
\end{eqnarray}  
where $n$ is the number of observations and $k$ is the number of estimated parameters.
$\vI_{n_{g}}$ is an identity matrix of size $n_{g}$, and $\vH_{gg}$ is a block-diagonal hat matrix.

\subsubsection{Two- and multi-way clustered covariances}
Certainly, there can be more than one cluster dimension, as for example observations that are characterized by housholds within states or companies within industries. It can therefore sometimes be helpful that one-way clustered covariances can be extended to so-called multi-way clustering as shown by \cite{hac:Miglioretti+Heagerty:2007}, \cite{hac:Thompson:2011} and \cite{hac:Cameron+Gelbach+Miller:2011}.

Multi-way clustered covariances comprise clustering on $2^{D} - 1$ dimensional combinations.
Clustering in two dimensions, for example in $id$ and $time$, gives $D = 2$, such that the clustered covariance matrix is composed of $2^2 - 1 = 3$ one-way clustered covariance matrices that have to be added up or substracted off, respectively.
For two-way clustered covariances with cluster dimensions $id$ and $time$, the one-way clustered covariance matrices on $id$ and on $time$ are added up, and the two-way clustered covariance matrix with clusters formed by the intersection of $id$ and $time$ is substacted off
\begin{equation} \label{eq:twoway}
S(\theta)_{id,time} \quad = \quad S(\theta)_{id} + S(\theta)_{time} - S(\theta)_{id \cap time}.
\end{equation}
Clustered covariance matrices like $S(\theta)_{id}$ and $S(\theta)_{time}$ with an odd number of cluster dimensions are added up, whereas those with an even number of cluster dimensions like $S(\theta)_{id \cap time}$ are substracted off to get the two-way clustered covariance estimator. The same heuristic applies to clustered covariances with more than two dimensions of clusters.

\cite{hac:Petersen:2009} and \cite{hac:Ma:2014} suggest to substract the White covariance matrix as the last substracted matrix in case that the clusters formed by the intersection of $id$ and $time$ do only contain a single observation\footnote{If one wants to cluster by dimensions $id$ and $year$ in a sample of $id-month$ observations, each cluster is composed of 12 observations \citep{hac:Ma:2014}. In such a setting, the last substracted matrix should be a one-way clustered covariance matrix with clusters formed by the intersections of $id$ and $year$.}.

\subsection{Clustered covariances for panel data}
The information of panel data sets is often overstated, as cross-sectional as well as temporal dependencies may occur \citep{hac:Hoechle:2007}. \cite[][p.~702]{hac:Cameron+Trivedi:2005} noticed that ``NT correlated observations have less information than NT independent observations''. 
For panel data, the source of dependence in the data is crucial to find out what kind of covariance is optimal \citep{hac:Petersen:2009}.
In the following, panel Newey-West standard errors as well as Driscoll and Kraay standard errors are examined \citep[see also][]{hac:Millo:2014}.

\subsubsection{Panel Newey-West}
\cite{hac:Newey+West:1987} proposed a heteroscedasticity and autocorrelation consistent standard error estimator that is traditionally used for time-series data, but can be modified for use in panel data by estimating correlations between lagged residuals in the same cluster \citep{hac:Petersen:2009}.

A panel Newey-West estimator can be obtained by setting the cross-sectional as well as the cross-serial correlation to zero \citep{hac:Millo:2014}. The meat is composed of
\begin{equation} \label{eq:newey}
  \hat M_\mathrm{PL}^ {NW} \quad = \quad \hat M + \sum_{l = 1}^L w_l[\hat \Omega_l + \hat \Omega_l^\top],
\end{equation}  
with
\begin{equation}
  \hat \Omega_l = \sum_{t = 1}^T\sum_{i = 1}^n \psi(y_{i,t}, x_{i,t}, \hat \theta) \psi(y_{i,t-l}, x_{i,t-l}, \hat \theta)^\top,
\end{equation}
where $w_l$ is a distance-decreasing kernel function, and $L$ the maximal lag length. As \cite{hac:Petersen:2009} noticed, the maximal lag length in a panel data set is the maximum number of years per firm minus one.

Though, an underlying assumption is that although residuals are correlated within groups, they are uncorrelated between groups, which is often artificial and does often not reflect the nature of panel data \citep{hac:Hoechle:2007}. 
To impose fewer restrictive assumptions, correlation within and between groups should be possible.

\subsubsection{Driscoll and Kraay}
\cite{hac:Driscoll+Kraay:1998} have adapted the Newey-West approach and introduce a technique yielding standard error estimates that are robust to spatial and temporal dependence of general form, but with the caveat that a long enough time dimension exists.

Driscoll and Kraay standard errors are a generalization of Newey-West standard errors, applied to the time series of cross-section averages of $\psi(y_{i,t}, x_{i,t}, \hat \theta)$ \citep{hac:Hoechle:2007}. Thus, independent of the cross-section dimension, Driscoll and Kraay standard errors are consistent.

They are obtained as the positive square root of the diagonal elements of the sandwich $S(\theta)$ with a meat
\begin{equation} \label{eq:driscoll}
  \hat M_\mathrm{PL}^ {DC} \quad = \quad \hat M_\mathrm{CL, t} + \sum_{l = 1}^{L} w_l[\hat \Omega_l + \hat \Omega_l^\top],
\end{equation}  
where $i$ denotes the cross section and $t$ the time units with 
\begin{equation}
  \hat \Omega_l = \sum_{t = 1}^T \psi(y_{t}, x_{t}, \hat \theta) \psi(y_{t-l}, x_{t-l}, \hat \theta)^\top \quad \mathrm{and} \quad \psi(y_{t}, x_{t}, \hat \theta) = \sum_{i = 1}^{n_{t}}\psi(y_{i,t}, x_{i,t}, \hat \theta)
\end{equation}
\citep{hac:Hoechle:2007}. $w_l = 1 - \frac{l}{L + 1}$ are the modified Barttlett weights, and $L$ is the maximal lag length used \citep{hac:Hoechle:2007}.  For lag $l = 0$, $\hat \Omega_0 = \hat M_{CL, t}$, where $\hat M_{CL, t}$ is the meat of one-way clustered covariances clustered by time. 

\subsection{Panel-corrected standard errors}

For time-series-cross-section (hereinafter abbreviated to TSCS) data, panel-corrected standard errors (hereinafter abbreviated to PCSE) have been introduced by \cite{hac:Beck+Katz:1995} as an attempt to weaken the problems of the Parks-Kmenta method\footnote{}. 
According to \cite{hac:Beck+Katz:1995}, TSCS data are characterized by having repeated observations on fixed units (for example states), where the number of units normally is between 10 and 100, and the number of time periods ranges between 20 and 50 years. 

Thus, TSCS data do often allow not only for heterocedasticity, but for spatial and temporal correlation as well.
Furthermore, \cite{hac:Johnson:2004} points out that a crucial assumption of PCSE is that the contemporaneous correlation accross cluster follows a fixed pattern.
The authors suggest to keep the OLS parameter estimates, but use PCSE instead of ordinary OLS standard errors.

To calculate PCSE \citep{hac:Johnson:2004}, the empirical working residuals are separated by cluster dimension $g$, such that they are of form $r(y_1, x_1^\top \hat \theta), r(y_2, x_2^\top \hat \theta), \ldots, r(y_G, x_G^\top \hat \theta)$.

The working residuals can be recovered from the empirical estimating function divided by $x_i$, as $\psi(y_i, x_i, \hat \theta) = r(y_i, x_i^\top \hat \theta) \cdot x_i$ \citep{hac:Zeileis:2006}.
 Each empirical working residual is a vector with $t$ elements. Grouped together into a $t \times G$ matrix,
 \begin{equation} \label{eq:workres}
  E \quad = \quad [r(y_1, x_1^\top \hat \theta) \quad r(y_2, x_2^\top \hat \theta) \quad \ldots \quad r(y_G, x_G^\top \hat \theta)],
 \end{equation}  
the meat of the panel-corrected covariance matrix is defined as \citep{hac:Beck+Katz:1995}
\begin{equation} \label{eq:pcse}
 \hat M_\mathrm{PC} \quad = \quad \frac{1}{n} X^\top \bigg[ \frac{(E^\top E)}{t} \otimes \vI_t \bigg] X.
\end{equation}
Compared to White's estimator, where squared residuals are along the diagonal of the estimator, the covariance across clusters is estimated by the cross product of their residuals \citep{hac:Johnson:2004}. While Whites approach does not account for clustering and handles each observations separately, panel-corrected covariances utilize the clustering structure.

With regard to the finite-sample propertiers of the PCSE estimator, \cite{hac:Hoechle:2007} argues that they are rather poor if the cross-sectional dimension $g$ is large compared to the time dimension $t$. The PCSE estimator will be quite imprecise for a small $t/g$ ration, as the whole $g \times g$ cross-sectional covariance matrix is estimated.

\section{Software} \label{sec:software}

\subsection{Clustered standard errors}

In the event of clustered data, 
\begin{verbatim}
vcovCL(x, cluster = NULL, type = NULL, sandwich = TRUE, fix = FALSE, ...)
\end{verbatim}
implemented in the \pkg{sandwich} package can be used for the estimation of one-, two-, and multi-way clustered covariances with different types of bias adjustments.
Compared to classical sandwich estimators, the meat changes for clustered covariances. The function
\begin{verbatim}
meatCL(x, cluster = NULL, type = NULL, cadjust = TRUE, white = FALSE, ...)
\end{verbatim}
calculates the meat of clustered covariances.

The \code{cluster} argument allows to supply one or more cluster variables. If no cluster variable is supplied, each observation is its own cluster.

Different types of bias adjustments are offered, and can be separated into two part, the cluster bias adjustment $G/(G-1)$ (where $G$ is the number of clusters within a cluster dimension) and the HC bias adjustment. Both parts can be separately switched on and off.
With the \code{cadjust} argument, the cluster bias adjustment can be switched on and off, whereas the \code{type} argument allows to specify the HC type of bias correction.
HC0-HC3 are offered, HC2 and HC3 are geared towards the linear model, but are also applicable for GLMs \citep{hac:Bell+McCaffrey:2002, hac:Kauermann+Carroll:2001}. A precondition for HC2 and HC3 types of bias adjustment is the existence of a hat matrix or a weighted version of the hat matrix for GLMs, respectively.
Hence, this is the only part of \code{vcovCL()} that is not fully object-oriented, but just works for \code{lm()} and \code{glm()} model objects.
However, beyond the glm case it's often not clear what the hat matrix should be.

The \code{white} argument specifies whether to substract HC0-type (that is, White's) covariance matrix as the last substracted matrix or the covariance matrix formed by the intersection of groups \citep{hac:Ma:2014}. If, for instance, the intersection of $id$ and $time$ in (\ref{eq:twoway}) consists of multiple observations, it is suggested \citep{hac:Petersen:2009,hac:Ma:2014} to replace White's covariance matrix as the last substracted matrix by the covariance matrix with clusters formed by intersections of $id$ and $time$.

The \code{sandwich} argument returns either the full sandwich or the meat \code{meatCL}, the \code{fix} argument specifies the eigendecomposition of the estimated covariance matrix and convert any negative eigenvalues to zero.
In some applications with fixed effects, covariance matrix of the estimator may not be positive-definite, which is the why this argument is sometimes required. Additionally, \cite{hac:Cameron+Gelbach+Miller:2011} observe that this is most likely to arise when clustering is done over the same groups as the fixed effects.

\subsection{Standard errors for panel data}

For panel data, 
\begin{verbatim}
vcovPL(x, cluster = NULL, order.by = NULL, kernel = "Bartlett",
 sandwich = TRUE, fix = FALSE, ...)
\end{verbatim}
as well as 
\begin{verbatim}
meatPL(x, cluster = NULL, order.by = NULL, kernel = "Bartlett",
 lag = "max", bw = NULL, adjust = TRUE, ...)
\end{verbatim}
allow for estimation of Driscoll and Kraay type covariances \citep{hac:Driscoll+Kraay:1998} as well as Newey-West type covariances \citep{hac:Newey+West:1987}.

\cite{hac:Driscoll+Kraay:1998} apply a Newey-West type correction to the sequence of cross-sectional averages of the moment conditions \citep{hac:Hoechle:2007}.
For large length of the time dimesion (and regardless of the length of the cross-sectional dimension), the \cite{hac:Driscoll+Kraay:1998} standard errors are robust to general forms of cross-sectional and serial correlation \citep{hac:Hoechle:2007}.
The \cite{hac:Newey+West:1987} covariance matrix restricts the \cite{hac:Driscoll+Kraay:1998} covariance matrix to non cross-sectional correlation \par
The \code{cluster} argument allows to specify a variable indicating the clustering of observations, \code{order.by} allows to specify a variable indicating the aggregation within time periods.
The \code{kernel} argument is a character specifying the kernel used. All kernels described in \cite{hac:Andrews:1991} are supported.
\code{lag} indicates the lag length used, \code{bw} specifies the bandwidth of the kernel, which by default corresponds to $\textrm{lag} + 1$.
The \code{adjust} argument allows to make a finite sample adjustment, which amounts to multiplication with $n/(n - k)$, where $n$ is the number of observations, and $k$ is the number of estimated parameters.

\subsection{Panel-corrected standard errors}

Panel-corrected standard errors (PCSE) a la \cite{hac:Beck+Katz:1995} are implemented in
\begin{verbatim}
vcovPC(x, cluster = NULL, order.by = NULL, subsample = TRUE,
 sandwich = TRUE, fix = FALSE, \ldots)
\end{verbatim}
and are usually used for time-series-cross-section (TSCS) data with a large enough time dimension. While an easy task for balanced panel, there are two alternatives to estimate the meat for unbalanced panel \citep{hac:Bailey+Katz:2011}. If \code{subsample = TRUE}, a balanced subset of the panel is employed, whereas for \code{subsample = FALSE}, a pairwise balaced sample is used.

\section{Illustrations} \label{sec:illu}
The classical area of application for clustered standard errors is the linear model, with model errors that are correlated within clusters. In this section, replication and comparison of clustered standard errors for different \proglang{R} packages is shown on the basis of benchmark \cite{hac:Petersen:2009} data (Example \ref{ex-petersen}). Due to the nice feature of object-orientation, clustered covariances in \code{vcovCL()} are now available for models beyond \code{lm()} or \code{glm()}. Example \ref{ex-aghion} applies \code{vcovCL()} to a hurdle model, using data provided by \cite{hac:Aghion+VanReenen+Zingales:2013}.

\subsection[Petersen (2009)]{\cite{hac:Petersen:2009}} \label{ex-petersen}
Benchmark data for testing the clustered standard error estimates in the linear model is a simulated data set\footnote{\url{http://www.kellogg.northwestern.edu/faculty/petersen/htm/papers/se/test_data.txt}} provided by \cite{hac:Petersen:2009}, containing 500 firms over 10 years. The data include 4 variables, \code{firmid}, \code{year}, dependent variable \code{y} and explanatory variable \code{x} \citep[see also][]{hac:Graham+Arai+Hagstroemer:2016}.

The data are availlable in \pkg{sandwich} as well as in \pkg{multiwayvcov}.
<<petersen-data>>= 
data("PetersenCL", package = "sandwich")
formula <- y ~ x
@ 
A linear model is fitted with \code{lm()}, 
<<petersen-model>>=
lm.petersen <- lm(formula, data = PetersenCL)
@ 
and testing the estimated coefficients with \code{coeftest()} from package \pkg{lmtest} gives
<<petersen-coeftest>>=
library("lmtest")
coeftest(lm.petersen)
@ 
However, the data are clustered by firms (\code{firmid}), with a total of 500 clusters. But ignoring dependency by using ``classical'' standard errors results in an overstate of estimator precision.

\subsubsection{One-way clustered standard errors}
Thus, one-way clustered standard errors clustered by \code{firmid} are employed.
It can be observed that clustered standard errors are larger than ``classical'' standard errors.
<<petersen-onecl-firm>>=
library("sandwich")
coeftest(lm.petersen, vcov = vcovCL(lm.petersen, cluster = PetersenCL$firmid))
@ 
Next, a comparison of clustered standard errors resulting from packages \pkg{sandwich}, \pkg{plm}, \pkg{geepack} and \pkg{multiwayvcov} is shown. 
<<petersen-packages>>=
library("plm")
library("geepack")
library("multiwayvcov")
@ 
A pooling model is estimated with \code{plm()}, \code{geeglm()} fits a generalized estimating equation (GEE) with independence correlation structure.
<<petersen-models>>=
plm.petersen <- plm(formula, data = PetersenCL, model = "pooling")
gee.petersen <- geeglm(formula, data = PetersenCL, id = PetersenCL$firmid,
corstr = "independence", family = gaussian)
@ 
The bias correction in \code{vcovCL()} is set to \code{cadjust = TRUE} and \code{type = "HC1"} by default. For \code{cluster.vcov()} from \pkg{multiwayvcov}, the same type of bias correction is achieved by default.
Nevertheless, in order to accomplish exactly the same value for clustered standard errors using \code{plm} and \code{geepack}, the bias correction $\frac{G}{G - 1}\frac{n - 1}{n - k}$ has to be multiplied to the covariance matrices.
<<petersen-df>>=
n <- nrow(PetersenCL)
G <- length(unique(PetersenCL$firmid))
k <- 2
bc <- G / (G - 1) * (n - 1) / (n - k)
@ 
$G$ is the number of clusters, $n$ is the total number of observations, and $k$ is the number of parameters estimated.

All packages examined produce the same clustered standard errors for \code{lm()}. For packages \pkg{plm} and \pkg{geepack}, one manually has to adjust the bias correction factor. In general, differences in clustered standard errors often coincide with different types of bias corrections.
<<petersen-comparison>>=
vc <- list(
  "sandwich" = vcovCL(lm.petersen, cluster = PetersenCL$firmid),
  "multiwayvcov" = cluster.vcov(lm.petersen, cluster = PetersenCL$firmid),
  "plm" = vcovHC(plm.petersen, type = "HC0", cluster = "group") * bc,
  "geepack" = gee.petersen$geese$vbeta * bc
)
sapply(vc, function(x) sqrt(diag(x)))
@ 

\subsubsection{Two-way clustered standard errors}
Two-way clustered standard errors with cluster dimensions \code{firmid} as well as \code{year} are, at least for the explanatory variable \code{x}, a bit larger than one-way clustered standard errors clustered by \code{firmid}.
<<petersen-twocl>>=
coeftest(lm.petersen, vcov. = vcovCL(lm.petersen, 
cluster = cbind(PetersenCL$firmid, PetersenCL$year)))
@ 
 However, as cluster dimension \code{year} has a total of only 10 cluster, the results should be regarded with caution, as it is required by theory that each cluster dimension has many clusters \citep{hac:Petersen:2009,hac:Cameron+Gelbach+Miller:2011,hac:Cameron+Miller:2015}
 
\subsection[Aghion et al. (2013)]{\cite{hac:Aghion+VanReenen+Zingales:2013}} \label{ex-aghion}
In this section, a further example will make use of the object-orientation of \code{vcovCL()}, and estimate clustered standard errors for a count data hurdle model.

\cite{hac:Aghion+VanReenen+Zingales:2013} investigate the effect of institutional owners (these are, for example, pension funds, insurance companies, etc.) on innovation.
The authors use firm-level panel data on innovation and institutional ownership from 1991 to 1999 over 803 firms, with the data clustered at company as well as industry level. 
To capture the differing value of patents, citation-weighted patent counts are used as a proxy for innovation, whereby the authors weight the patents by the number of future citations.
This motivates the use of count data models.

\cite{hac:Aghion+VanReenen+Zingales:2013} mostly employ Poisson and negative binomial models in a quasi-maximum likelihood approach and cluster standard errors by either companies or industries.
Still, one limitation of standard count data models is that the zeros and the nonzeros (positives) are assumed to come from the same data-generating process.

From an economic perspective, there is a difference in determinants of ``first innovation'' and ``continuing innovation''. The rationale behind this is the notion of nonlinearities in the innovation process.
In case that the first innovation is especially hard to obtain in comparison to succeeding innovations, hurdle models offer a useful way that allows for a distinction to be made between these two processes \citep{hac:Berger+Stocker+Zeileis:2016}.

\subsubsection[Berger et al. (2016)]{Reanalysis by \cite{hac:Berger+Stocker+Zeileis:2016}}

\cite{hac:Berger+Stocker+Zeileis:2016} employ two-part hurdle models with a binary part that models the decision to innovate at all, and a count part that models ongoing innovation, respectively.
The \cite{hac:Aghion+VanReenen+Zingales:2013} data are availlable in the \pkg{sandwich} package.
<<innovation-data>>= 
data("InstInnovation", package = "sandwich")
@ 
Hurdle models are fitted with the \code{hurdle} function from the \pkg{pscl} package \citep{hac:Zeileis+Kleiber+Jackman:2008}. Here, the count model family chosen is a negative binomial, and the zero hurdle model family is a binomial with logit link.
<<innovation-model>>=
library("pscl")
h.innov <- hurdle(
  cites ~ institutions + log(capital/employment) + log(sales),
  data = InstInnovation, dist = "negbin")
@
Below, a comparison of "classical", HC0 and clustered standard errors for an exemplary hurdle model is shown. Standard errors are clustered by companies, with a total of 803 clusters.
<<innovation-se>>= 
library("sandwich")
vc <- list(
  "classical" = vcov(h.innov),
  "HC0" = sandwich(h.innov),
  "cluster" = vcovCL(h.innov, cluster = InstInnovation$company)
)
sapply(vc, function(x) sqrt(diag(x)))
@
What can be observed is that when the data are clustered, ``classical'' standard errors can greatly overstate estimator precision.
Then, for the exemplary hurdle model, clustered standard errors are scaled up by factors between $1.48$ and $1.86$ compared to HC0  standard errors.

\section{Simulation} \label{sec:simulation}

We run a Monte Carlo simulation to assess the methods' performance in a simulation study.
The aim is to test clustered standard errors beyond \code{lm()} and \code{glm()}. For the linear model, there are a couple of simulation studies in the literature \citep{hac:Cameron+Gelbach+Miller:2008,hac:Arceneaux+Nickerson:2009,hac:Petersen:2009,hac:Cameron+Gelbach+Miller:2011,hac:Harden:2011,hac:Thompson:2011,hac:Cameron+Miller:2015,hac:Jin:2015}, far less for \code{glm()} \citep{hac:Miglioretti+Heagerty:2007} and, up to our knowledge, none for models beyond \code{lm()} and \code{glm()}.

\subsection{Simulation design}
The simulations are each composed of $10,000$ replications.
We systematically vary parameters $\rho$ and $G$. $\rho$ determines the strength of cluster correlation and varies from $0$ to $0.9$. The number of clusters $G$ ranges from $10$ to $50, 100, 150, 200$ to $250$. Numerous studies \citep{hac:Green+Vavreck:2008,hac:Arceneaux+Nickerson:2009,hac:Harden:2011} confirmed that the higher the number of clusters, the lower the bias of the standard errors (in linear regression models). Furthermore, we only have a look at balanced cluster, observations per cluster are fixed to $5$.

\subsubsection{Linear predictor}
The linear predictor is
\begin{eqnarray} \label{eq:predictor}
h(\mu_{ig}) = \beta_{0} + \beta_{1} \cdot x_{1,ig} + \beta_{2} \cdot x_{2,g} + \beta_{3} \cdot x_{3,ig}
\end{eqnarray}
with a link function $h$ and expected value $\mu_{ig}$. 
In order to generate within cluster error correlation, there are two possible options. The first option is to take the marginal model equation (\ref{eq:predictor}) and introduce correlation via a copula. The second option is to add a random effect in equation (\ref{eq:predictor}). 

It has to be mentioned that for the linear model with an identity link function $h(\cdot)$, both options amount to the same thing. Though for models other than \code{lm()}, the two options are different. In the simulations, the copula option is favoured, because for models other than \code{lm()}, including a random effect simultaneously includes a bias, too.

We analyze three regressor variables
\begin{eqnarray} \label{eq:regressors}
x_{1,ig} & \sim & \rho_{x} \cdot \mathcal{N}_{g}(0, 1) + (1 - \rho_{x}) \cdot \mathcal{N}_{ig}(0, 1) \label{x1} \\ 
x_{2,g} & \sim & \mathcal{N}_{g}(0, 1) \label{x2} \\
x_{3,ig} & \sim & \mathcal{N}_{ig}(0, 1) \label{x3}
\end{eqnarray}
Regressor $x_{1,ig}$ (\ref{x1}) is composed of a linear combination of a random draw at cluster level ($\mathcal{N}_{g}$) and a random draw at individual level ($\mathcal{N}_{ig}$), regressor $x_{2,g}$ (\ref{x2}) is composed of a random draw at cluster level, and regressor $x_{3,ig}$ (\ref{x3}) consists of a random draw at individual level. In most of the simulations, only a single regressor  $x_{1,ig}$ (\ref{x1}) is used.

In line with \cite{hac:Harden:2011}, this is done to produce variation at cluster level, individual level and at a combination of both levels. The cluster correlation of $x_{1,ig}$ is controlled by parameter $\rho_{x}$, and is set to 0.25 per default. This implies at least some within cluster correlation of regressor $x_{1,ig}$. If $\rho_{x} = 1$, regressor (\ref{x1}) is equivalent to regressor (\ref{x2}). Furthermore, if $\rho_{x} = 0$, regressor (\ref{x1}) corresponds to regressor (\ref{x3}).

The vector of coefficients is fixed to
\begin{eqnarray} \label{eq:coefs}
\beta_{1} & = & (0, 0.85, 0.5, 0.7)^\top \label{beta1} \\ 
\beta_{2} & = & (0, 0.85, 0, 0)^\top \label{beta2}
\end{eqnarray}
even though these values can be interchanged without influencing the results\footnote{Values are equivalent to \cite{hac:Harden:2011}}.

Response distributions examined include Gaussian, Bernoulli (with logit link), Poisson, zero-truncated Poisson, Beta and zero-inflated Poisson (ZIP). \code{vcovCL()} allows estimation of clustered standard errors for all the abovementioned responses (and many more).

\subsubsection{Sandwich covariances}
Standard errors being compared to each other include iid standard errors without heteroscedasticity and without autocorrelation (classical), HC0 standard errors \citep{hac:White:1980}, panel-corrected (PC) standard errors a la \cite{hac:Beck+Katz:1995} and clustered standard errors with HC0 to HC3 adjustment (HC0-cluster, HC1-cluster, HC2-cluster, HC3-cluster). In addition, standard errors from a random effects model (random) and from a GEE with exchangeable correlation structure (gee) are compared.

\subsubsection{Outcome measures}
The outcome measures we are most interested in are coverage and bias. In order to assess the validity of statistical inference, the coverage rate is used.
If standard errors are estimated accurately, the coverage rate of the $95$\% confidence intervall should be close to 0.95. 
Values less than 0.925 will be considered to have underestimated standard errors while values greater than 0.975 will be considered to have overestimated standard errors.


\subsection{Results}
\setkeys{Gin}{width=.85\textwidth} 
\begin{figure}[thb] \label{fig:sim-01}
  \begin{center}
<<sim-01-figure,echo=FALSE,fig=TRUE,height=5,width=10>>=
my.settings <- canonical.theme(color = TRUE)
my.settings[["strip.background"]]$col <- "gray"
my.settings[["strip.border"]]$col <- "black"
my.settings[["superpose.line"]]$lwd <- 1
levels(s01$vcov) <- c("PC", "gee", "HC0", "HC0-cluster", "random", "classical")
my.settings[["superpose.line"]]$col <- c("#006400", "#e41a1c", "#f781bf", "#377eb8", "#984ea3", "#ff7f00")
my.settings[["superpose.symbol"]]$col <- c("#006400", "#e41a1c", "#f781bf", "#377eb8", "#984ea3", "#ff7f00")
xyplot(coverage ~ rho | par, groups = ~ factor(vcov),
  data = s01, subset = par != "(Intercept)",
  ylim = c(0, 1),
  type = "b", xlab = expression(rho), ylab = "coverage",
  auto.key = list(columns = 2),
  par.strip.text = list(col = "black"), par.settings = my.settings,
  panel = panel.xyref)
@ 
\caption{Fixed: $N_{g} = 5$, $G = 100$, $\rho_{x} = 0.25$, $F = \mathcal{N}$. Copula.}
  \end{center}
\end{figure}

\begin{figure}[thb] \label{fig:sim-02}
  \begin{center}
<<sim-02-figure,echo=FALSE,fig=TRUE,height=5,width=10>>=
my.settings <- canonical.theme(color = TRUE)
my.settings[["strip.background"]]$col <- "gray"
my.settings[["strip.border"]]$col <- "black"
my.settings[["superpose.line"]]$lwd <- 1
levels(s02$vcov) <- c("PC", "gee", "HC0", "HC0-cluster", "random", "classical")
levels(s02$dist) <- c("gaussian", "binomial(logit)", "poisson")
my.settings[["superpose.line"]]$col <- c("#006400", "#e41a1c", "#f781bf", "#377eb8", "#984ea3", "#ff7f00")
my.settings[["superpose.symbol"]]$col <- c("#006400", "#e41a1c", "#f781bf", "#377eb8", "#984ea3", "#ff7f00")
xyplot(coverage ~ rho | dist, groups = ~ factor(vcov),
  data = s02, subset = par != "(Intercept)",
  ylim = c(0.8, 1),
  type = "b", xlab = expression(rho), ylab = "coverage",
  auto.key = list(columns = 2),
  par.strip.text = list(col = "black"), par.settings = my.settings,
  panel = panel.xyref)
@ 
\caption{Fixed: $N_{g} = 5$, $G = 100$, $\rho_{x} = 0.25$, $F = \mathcal{N}$, Binomial, Poisson. Copula.}
  \end{center}
\end{figure}

\begin{figure}[thb] \label{fig:sim-03}
  \begin{center}
<<sim-03-figure,echo=FALSE,fig=TRUE,height=5,width=10>>=
s03 <- na.omit(s03)
my.settings <- canonical.theme(color = TRUE)
my.settings[["strip.background"]]$col <- "gray"
my.settings[["strip.border"]]$col <- "black"
my.settings[["superpose.line"]]$lwd <- 1
levels(s03$vcov) <- c("HC0", "HC0-cluster", "classical")
levels(s03$dist) <- c("betareg", "zerotrunc", "zeroinfl")
my.settings[["superpose.line"]]$col <- c("#f781bf", "#377eb8", "#ff7f00")
my.settings[["superpose.symbol"]]$col <- c("#f781bf", "#377eb8", "#ff7f00")
xyplot(coverage ~ rho | dist, groups = ~ factor(vcov),
  data = s03, subset = par != "(Intercept)",
  ylim = c(0.8, 1),
  type = "b", xlab = expression(rho), ylab = "coverage",
  auto.key = list(columns = 2),
  par.strip.text = list(col = "black"), par.settings = my.settings,
  panel = panel.xyref)
@ 
\caption{Fixed: $N_{g} = 5$, $G = 100$, $\rho_{x} = 0.25$, $F =$ Beta, zerotrunc. Poisson, ZIP. Copula.}
  \end{center}
\end{figure}

\begin{figure}[thb] \label{fig:sim-04}
 \begin{center}
<<sim-04-figure,echo=FALSE,fig=TRUE,height=5,width=10>>=
my.settings <- canonical.theme(color = TRUE)
my.settings[["strip.background"]]$col <- "gray"
my.settings[["strip.border"]]$col <- "black"
my.settings[["superpose.line"]]$lwd <- 1
levels(s04$vcov) <- c("HC0-cluster", "HC1-cluster", "HC2-cluster", "HC3-cluster")
levels(s04$dist) <- c("gaussian", "binomial(logit)", "poisson")
my.settings[["superpose.line"]]$col <- c("#377eb8", "cyan4", "#e41a1c", "#4daf4a")
my.settings[["superpose.symbol"]]$col <- c("#377eb8", "cyan4","#e41a1c", "#4daf4a")
xyplot(coverage ~ nid | dist, groups = ~ factor(vcov),
  data = na.omit(s04), subset = par != "(Intercept)",
  type = "b", xlab = "G", ylab = "coverage",
  auto.key = list(columns = 2),
  par.strip.text = list(col = "black"), par.settings = my.settings,
  panel = panel.xyref)
@ 
\caption{Fixed: $N_{g} = 5$, $\rho = 0.25$, $\rho_{x} = 0.25$, $F = \mathcal{N}$, Binomial, Poisson. Copula.}
 \end{center}
\end{figure}

\section{Summary} \label{sec:summary}

\clearpage

\bibliography{hac}

\clearpage

\begin{appendix}

%% for "plain pretty" printing
\DefineVerbatimEnvironment{Sinput}{Verbatim}{}
<<echo=FALSE>>=
options(prompt = "  ")
@

\section[R Code]{\proglang{R} code} \label{sec:code}

The packages \pkg{sandwich}, \pkg{pscl} and \pkg{lattice} are
required for the applications in this paper.

\section{Simulation of correlated data with a random cluster effect} \label{sec:rceffect}

\begin{figure}[thb] \label{fig:sim-05}
  \begin{center}
<<sim-05-figure,echo=FALSE,fig=TRUE,height=5,width=10>>=
my.settings <- canonical.theme(color = TRUE)
my.settings[["strip.background"]]$col <- "gray"
my.settings[["strip.border"]]$col <- "black"
my.settings[["superpose.line"]]$lwd <- 1
levels(s05$vcov) <- c("HC0", "HC0-cluster", "classical")
levels(s05$dist) <- c("gaussian", "binomial(logit)", "poisson")
my.settings[["superpose.line"]]$col <- c("#f781bf", "#377eb8", "#ff7f00")
my.settings[["superpose.symbol"]]$col <- c("#f781bf", "#377eb8", "#ff7f00")
xyplot(coverage ~ rho | dist, groups = ~ factor(vcov),
  data = s05, subset = par != "(Intercept)",
  ylim = c(0, 1),
  type = "b", xlab = expression(rho), ylab = "coverage",
  auto.key = list(columns = 2),
  par.strip.text = list(col = "black"), par.settings = my.settings,
  panel = panel.xyref)
@  
<<sim-05-bias-figure,echo=FALSE,fig=TRUE,height=5,width=10>>=
panel.xyref <- function(x, y, ...) {
  panel.abline(h = 0, col = 2)
  panel.xyplot(x, y, ...)  
}
my.settings <- canonical.theme(color = TRUE)
my.settings[["strip.background"]]$col <- "gray"
my.settings[["strip.border"]]$col <- "black"
my.settings[["superpose.line"]]$lwd <- 1
levels(s05$vcov) <- c("HC0", "HC0-cluster", "classical")
levels(s05$dist) <- c("gaussian", "binomial(logit)", "poisson")
my.settings[["superpose.line"]]$col <- c("slategray", "slategray", "slategray")
my.settings[["superpose.symbol"]]$col <- c("slategray", "slategray", "slategray")
xyplot(bias ~ rho | dist, groups = ~ factor(vcov),
  data = s05, subset = par != "(Intercept)",
  ylim = c(-0.5, 0.1),
  type = "b", xlab = expression(rho), ylab = "bias",
  par.strip.text = list(col = "black"), par.settings = my.settings,
  panel = panel.xyref)
@ 
\caption{Fixed: $N_{g} = 5$, $G = 100$, $\rho_{x} = 0.25$, $F = \mathcal{N}$, Binomial, Poisson. Random effect.}
    \end{center}
\end{figure}

\begin{figure}[thb!] \label{fig:sim-02-1}
  \begin{center}
<<sim-02-1figure,echo=FALSE,fig=TRUE,height=5,width=10>>=
panel.xyref <- function(x, y, ...) {
  panel.abline(h = 0.95, col = 2)
  panel.xyplot(x, y, ...)  
}
my.settings <- canonical.theme(color = TRUE)
my.settings[["strip.background"]]$col <- "gray"
my.settings[["strip.border"]]$col <- "black"
my.settings[["superpose.line"]]$lwd <- 1
levels(s02$vcov) <- c("PC", "gee", "HC0", "HC0-cluster", "random", "classical")
levels(s02$dist) <- c("gaussian", "binomial(logit)", "poisson")
my.settings[["superpose.line"]]$col <- c("#f781bf", "#377eb8", "#ff7f00")
my.settings[["superpose.symbol"]]$col <- c("#f781bf", "#377eb8", "#ff7f00")
xyplot(coverage ~ rho | dist, groups = ~ factor(vcov),
  data = s02[which(s02$vcov %in% c("classical", "HC0", "HC0-cluster")), ], subset = par != "(Intercept)",
  ylim = c(0, 1),
  type = "b", xlab = expression(rho), ylab = "coverage",
  auto.key = list(columns = 2),
  par.strip.text = list(col = "black"), par.settings = my.settings,
  panel = panel.xyref)
@ 
<<sim-02-1-bias-figure,echo=FALSE,fig=TRUE,height=5,width=10>>=
panel.xyref <- function(x, y, ...) {
  panel.abline(h = 0, col = 2)
  panel.xyplot(x, y, ...)  
}
my.settings <- canonical.theme(color = TRUE)
my.settings[["strip.background"]]$col <- "gray"
my.settings[["strip.border"]]$col <- "black"
my.settings[["superpose.line"]]$lwd <- 1
levels(s02$vcov) <- c("PC", "gee", "HC0", "HC0-cluster", "random", "classical")
levels(s02$dist) <- c("gaussian", "binomial(logit)", "poisson")
my.settings[["superpose.line"]]$col <- c("slategray", "slategray", "slategray")
my.settings[["superpose.symbol"]]$col <- c("slategray", "slategray", "slategray")
xyplot(bias ~ rho | dist, groups = ~ factor(vcov),
  data = s02[which(s02$vcov %in% c("classical", "HC0", "HC0-cluster")), ], subset = par != "(Intercept)",
  ylim = c(-0.5, 0.1),
  type = "b", xlab = expression(rho), ylab = "bias",
  par.strip.text = list(col = "black"), par.settings = my.settings,
  panel = panel.xyref)
@ 
\caption{Fixed: $N_{g} = 5$, $G = 100$, $\rho_{x} = 0.25$, $F = \mathcal{N}$, Binomial, Poisson. Copula.}
  \end{center}
\end{figure}

\end{appendix}

\end{document}
